[
  {
    "objectID": "04-frequency_distribution.html",
    "href": "04-frequency_distribution.html",
    "title": "Week 3: Examining Data I",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nUnderstand frequency distributions\nHow to categorise continuous measurements in classes/categorise using the cut()\nHow to compute a frequency table\nHow to provide an interpretation to such outputs\nHow to translate a frequency distribution table into a histogram and cumulative frequency plot\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore we do anything - make sure to have downloaded the dataset for this computer session by clicking [HERE]. It contains the file Barcelona_Air_Pollution_data.csv - this comma separated values (CSV) file contains the data needed to follow today’s tutorial.\nInstructions In your computer, do the following:\n\nGo to the folder named GEOG0186 - this should have been created in Week 2\nNext, create a new sub-folder within GEOG0186 and rename it as Week 3.\nFrom the downloaded folder Dataset for Week 3, make sure to unzip and transfer ALL the datasets directly to the Week 3 folder.\n\n\n\n\n\n\nJump straight to the coding exercise by clicking HERE if you want to skip this section!\n\n\nDefinition 1: Statistics is a branch in the mathematical sciences that pertains to the collection, analysis, interpretation, and graphical presentation of data. The best thing about statistics is that it’s a highly applied branch of science which is applicable to many areas such as social science, politics, health (e.g., epidemiology), business & finance, environmental sciences and geography.\nStatistics is broadly split into two main areas:\n\nDescriptive statistics, which focuses on describing the visible characteristics about a dataset\nInferential statistics is more research-based, which focuses on making predictions (rather than stating facts) and testing hypothesis about a phenomenon.\n\nDefinition 2: A variable is any characteristics, numbered value, or quantity that can be measured or counted. A variable can also be referred to a data Item. A variable can be broadly classified as discrete, continuous or categorical variable.\nWe have provided two videos: the first which broadly explains why statistics as a subject is important; and the second explains in much details what statistics is as a subject, and what are the various data types.\n[Theory] Why is statistics important? (Length: 13:21 minutes)\n\n\n\n\n\n\n\n\n\nWatch on YouTube [LINK]\n[Theory] What is statistics, and the types of variables? (Length: 31:17 minutes)\n\n\n\n\n\n\n\n\n\nWatch on YouTube [LINK]\n\n\n\n\nWe will focus on descriptive statistics as an introduction introducing everyone to the absolute basics. Descriptive statistics is all about knowing the data types and finding the distribution, central tendency and variability in such data set. These four key words may sound intimidating – but trust me – it is very easy! Let us learn how to perform the following aspect of finding the distribution in RStudio using the air pollution data for Barcelona.\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nLet us import for following dataset Barcelona_Air_Pollution_data.csv into RStudio, and call this object air_ quality_data.\nRemember - always make sure that your work directory is linked to your folder containing your data.\nFor Windows:\n\nsetwd(\"C:/Users/accountName/Desktop/GEOG0186/Week 3\")\n\nFor Macs:\n\nsetwd(\"/Users/accountName/Desktop/GEOG0186/Week 3\")\n\nNow, import you the data set as follows:\n\nair_quality_data &lt;- read.csv(\"Barcelona_Air_Pollution_data.csv\")\n\nYou use the command View() see the full data viewer, or head() to see the first five rows of the dataset.\n\n# see imported dataset\nView(air_quality_data)\n\n\nhead(air_quality_data)\n\nYou will notice that the data contains six variables with the following information:\n\n\n\n\n\n\n\n\nVariable name\nVariable Type\nInformation\n\n\n\n\nLocation\nString/Text only\nName of location Eixample, Barcelona\n\n\nReadingDate\nDate\nData collection date for air quality measures\n\n\nNO2_est\nContinuous\nMeasurements for Nitrogen dioxide (NO\\(_2\\)) (ppb)\n\n\nNO2_category\nCategorical\nHealth impacts (negligible/low/moderate/high)\n\n\nPM10_est\nContinuous\nMeasurements for Particulate matter (PM\\(_10\\))\n\n\nPM10_category\nCategorical\nHealth impacts (negligible/low/moderate)\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe NO2_est, for example, contains measurable items i.e., 718 observations for concentrations of ambient NO\\(_2\\) in Eixample area of Barcelona, and hence its a continuous variable. These estimates have been categorised in accordance with their health dangers i.e., negligible (\\(&lt;\\) 10 ppb); low (11-50 ppb);\n\n\nLet us begin to analyse NO2_est and NO2_category with Frequency Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nWe use frequency distribution to analyse a set of continuous measurements. In data handling in this context, there are two outputs generated:\n\nFrequency, which tells us how often a particular result was obtained. From this we can calculate a percentage value which is referred to as Relative Frequency.\n\nCumulative Frequency, this is a cumulative sum of the frequencies, which indicates how often a result was obtained that is less than a stated value in our collection of data. Again, from this we can also calculate a cumulative percentage value which is referred to as Cumulative Relative Frequency.\n\nSuppose, we want to assess the 718 observations for air pollutant Nitrogen Dioxide (NO\\(_2\\)).\nLet’s list the observations for Nitrogen Dioxide (NO\\(_2\\)) in Barcelona:\n\nair_quality_data$NO2_est\n\n  [1]  61  59  29  75  23  49  43  35  83  75  71  56  54  44  41  54  62  56\n [19]  26  42  71  86  85  52  56  45  68  86  69  71   4  82  43  51 114  43\n [37]  18  58  24  53  98  53 100  53  49  49  46  82  77  67  76  52  61  80\n [55]  77  70  56  49  42  73  64  33  71  72  13  37  26  46  84  72  65  76\n [73]  90  46  61  81  64  62  58  78  83  37 130  43  20  40 102  68  48  74\n [91]  52  43  80  71  42  84  44 121  41  66  44  50  38  75  41  45  48  63\n[109]  53  63  63  46  34  87  75  74  36  69  46  15  80  75  83  95   5  65\n[127]  21  84  68  32  45  73  53  31  85  91  73  46  25  75  70  84  68  65\n[145]  58 113  62  60  55  69  82 100 105  47  60 103  53  34  39  22  21  71\n[163]  85  56  73  61  24  44  47  49 100  64  91  79  42  32  33  84  43  61\n[181]  63  49  80  46  58  45  37  66  60  75  35  75  48  43  57  67  54  38\n[199]  22  51  69  51  64  32  20  52  42  65  69  47  40  34  34  51  57  43\n[217]  52  86  53  43  54  75  56  62  41  84  41  22  83  76  51  31  50  65\n[235]  76  77  61  50  75  49  47  65  78  39  51  49  75  45  50  69  86  75\n[253]  89  68  84  90  90  56 106  63  90  57  38  86  22  39  19  61  44  63\n[271]  52  42  46  56  40  69  62  42  54  17  49  84  34  89  65  53  78  67\n[289]  55  61  39  82  58  15  63  76  55  80  56  79  72  58  74  27  93  40\n[307]  40  58  79  81 123  84  37  87  38  49  91  50  59  69  57  68  53  38\n[325]  51  78  71  72  55  70  56  63  85  78  64  23  84  43  46  33  59  58\n[343]  47  64  68  89  76  86 116  52  34  63  40  41  72  87  37  62  38  68\n[361]  88  39  59  77  75 112  44  40  90  37  66  61  65  50  79  79  36  36\n[379]  12  86  40  62  63  71  53  30  44  76  41  62  77  80  62  86  37  48\n[397]  80  55  56  49  84  48  49  84  60  76  28  77  41  57  55  51  54  54\n[415]  11  43  38  86  30  23  78  29  80  16  48  90  44  42  50  54  45  42\n[433]  70  49  67  73  60  42  99  97  77  46  52  24  75  30  70  81  53  17\n[451]  63  59  44  41  67  56  58 111  43  47  49  58  36  72  36 103  63  77\n[469]  65  42  42  79  41  24  59  50  46  55  77  91  54  70  73  53  80  53\n[487]  72  67  95  57  87  39  73  56  34  56  75  74  72  42 119  55  43  69\n[505]  55  52  77  63 108  43  61  47 117  80  61  67  78  49  42  35  58  54\n[523]  36  84  56  72  70  40  59  71  56  49  66  52  48  60  54  73  66  67\n[541]  70  93  65  60  13  83  49  42  62  63  50  46  54  94  73  54  74  54\n[559]  10  71  41  17  75  55  54  54  83  47  49  90  76  89  83  43  76  67\n[577]  75  88  59  60  34  36  63  42  59  71  73  73  40  74  53  56  99  46\n[595]  46  64  37  20  84  86  47  57  54  56  78  73  65  72  37  57  38  46\n[613]  43  57  98  32  98  53  86  59  63  42  60  60  51  58  59  83  67  42\n[631]  74  62  84  67  49  76   2  44  51  69  69  87  49  18  73  66  81  78\n[649]  82  69  50  36  71  60   7  50  49  60  55  25  58  76  69  61  88  45\n[667]  59  59  91  61  81  81  83  71 108  99  46  69  38  54  59  16  75  81\n[685]  35  63  65  71  57  53  58  37  39  43  64  76  56  72  87  74  74  77\n[703]  46  87  53  60  47  10  61  35  36  60  71  45  47  79  37 123\n\n\nIn a list format it is quite difficult to make head or tail on what observations appear frequently and its distribution. To summarise this - it will be helpful to classify the information into Classes and then obtain the Frequency and Cumulative Frequency in a table. We call this table a Frequency Table.\nThe minimum value for NO\\(_2\\) is 2 and the maximum is 130. We can group the 718 observations into 13 classes using an interval of 10s e.g., 1-10, 11-20, 21-30, 31-40, 41-50, 51-60, 61-70, 71-80, 81-90, 91-100, 101-110, 111-120 and 121-130\n\n\n\n\n\n\nImportant\n\n\n\nThe way and manner you specify the classes and interval are up to you really. Here, 10 is being used for convenience.\n\n\nThe interval width is 10, we can generate sequence of number from 0 to 130, inclusively, to create the classes which in turn be used to group the 718 observations into 13 classes using the seq() and cut().\nFor example:\n\n# using starting value as 0\n# using highest value as 130\n# using interval as 10\n\n#  specify in this order the lower, highest, interval value in seq() function\nclasses &lt;- seq(0, 130, 10)\nclasses\n\n [1]   0  10  20  30  40  50  60  70  80  90 100 110 120 130\n\n\nThe sequence of values are stored in the object called classes. Now, let us apply the cut() function to group the NO\\(_2\\) data accordingly. We can do this by generating a new variable called Groups.\n\n# tell the cut() function to group NO2_est using the classes object\nair_quality_data$Groups &lt;- cut(air_quality_data$NO2_est, breaks=classes)\nair_quality_data$Groups\n\n  [1] (60,70]   (50,60]   (20,30]   (70,80]   (20,30]   (40,50]   (40,50]  \n  [8] (30,40]   (80,90]   (70,80]   (70,80]   (50,60]   (50,60]   (40,50]  \n [15] (40,50]   (50,60]   (60,70]   (50,60]   (20,30]   (40,50]   (70,80]  \n [22] (80,90]   (80,90]   (50,60]   (50,60]   (40,50]   (60,70]   (80,90]  \n [29] (60,70]   (70,80]   (0,10]    (80,90]   (40,50]   (50,60]   (110,120]\n [36] (40,50]   (10,20]   (50,60]   (20,30]   (50,60]   (90,100]  (50,60]  \n [43] (90,100]  (50,60]   (40,50]   (40,50]   (40,50]   (80,90]   (70,80]  \n [50] (60,70]   (70,80]   (50,60]   (60,70]   (70,80]   (70,80]   (60,70]  \n [57] (50,60]   (40,50]   (40,50]   (70,80]   (60,70]   (30,40]   (70,80]  \n [64] (70,80]   (10,20]   (30,40]   (20,30]   (40,50]   (80,90]   (70,80]  \n [71] (60,70]   (70,80]   (80,90]   (40,50]   (60,70]   (80,90]   (60,70]  \n [78] (60,70]   (50,60]   (70,80]   (80,90]   (30,40]   (120,130] (40,50]  \n [85] (10,20]   (30,40]   (100,110] (60,70]   (40,50]   (70,80]   (50,60]  \n [92] (40,50]   (70,80]   (70,80]   (40,50]   (80,90]   (40,50]   (120,130]\n [99] (40,50]   (60,70]   (40,50]   (40,50]   (30,40]   (70,80]   (40,50]  \n[106] (40,50]   (40,50]   (60,70]   (50,60]   (60,70]   (60,70]   (40,50]  \n[113] (30,40]   (80,90]   (70,80]   (70,80]   (30,40]   (60,70]   (40,50]  \n[120] (10,20]   (70,80]   (70,80]   (80,90]   (90,100]  (0,10]    (60,70]  \n[127] (20,30]   (80,90]   (60,70]   (30,40]   (40,50]   (70,80]   (50,60]  \n[134] (30,40]   (80,90]   (90,100]  (70,80]   (40,50]   (20,30]   (70,80]  \n[141] (60,70]   (80,90]   (60,70]   (60,70]   (50,60]   (110,120] (60,70]  \n[148] (50,60]   (50,60]   (60,70]   (80,90]   (90,100]  (100,110] (40,50]  \n[155] (50,60]   (100,110] (50,60]   (30,40]   (30,40]   (20,30]   (20,30]  \n[162] (70,80]   (80,90]   (50,60]   (70,80]   (60,70]   (20,30]   (40,50]  \n[169] (40,50]   (40,50]   (90,100]  (60,70]   (90,100]  (70,80]   (40,50]  \n[176] (30,40]   (30,40]   (80,90]   (40,50]   (60,70]   (60,70]   (40,50]  \n[183] (70,80]   (40,50]   (50,60]   (40,50]   (30,40]   (60,70]   (50,60]  \n[190] (70,80]   (30,40]   (70,80]   (40,50]   (40,50]   (50,60]   (60,70]  \n[197] (50,60]   (30,40]   (20,30]   (50,60]   (60,70]   (50,60]   (60,70]  \n[204] (30,40]   (10,20]   (50,60]   (40,50]   (60,70]   (60,70]   (40,50]  \n[211] (30,40]   (30,40]   (30,40]   (50,60]   (50,60]   (40,50]   (50,60]  \n[218] (80,90]   (50,60]   (40,50]   (50,60]   (70,80]   (50,60]   (60,70]  \n[225] (40,50]   (80,90]   (40,50]   (20,30]   (80,90]   (70,80]   (50,60]  \n[232] (30,40]   (40,50]   (60,70]   (70,80]   (70,80]   (60,70]   (40,50]  \n[239] (70,80]   (40,50]   (40,50]   (60,70]   (70,80]   (30,40]   (50,60]  \n[246] (40,50]   (70,80]   (40,50]   (40,50]   (60,70]   (80,90]   (70,80]  \n[253] (80,90]   (60,70]   (80,90]   (80,90]   (80,90]   (50,60]   (100,110]\n[260] (60,70]   (80,90]   (50,60]   (30,40]   (80,90]   (20,30]   (30,40]  \n[267] (10,20]   (60,70]   (40,50]   (60,70]   (50,60]   (40,50]   (40,50]  \n[274] (50,60]   (30,40]   (60,70]   (60,70]   (40,50]   (50,60]   (10,20]  \n[281] (40,50]   (80,90]   (30,40]   (80,90]   (60,70]   (50,60]   (70,80]  \n[288] (60,70]   (50,60]   (60,70]   (30,40]   (80,90]   (50,60]   (10,20]  \n[295] (60,70]   (70,80]   (50,60]   (70,80]   (50,60]   (70,80]   (70,80]  \n[302] (50,60]   (70,80]   (20,30]   (90,100]  (30,40]   (30,40]   (50,60]  \n[309] (70,80]   (80,90]   (120,130] (80,90]   (30,40]   (80,90]   (30,40]  \n[316] (40,50]   (90,100]  (40,50]   (50,60]   (60,70]   (50,60]   (60,70]  \n[323] (50,60]   (30,40]   (50,60]   (70,80]   (70,80]   (70,80]   (50,60]  \n[330] (60,70]   (50,60]   (60,70]   (80,90]   (70,80]   (60,70]   (20,30]  \n[337] (80,90]   (40,50]   (40,50]   (30,40]   (50,60]   (50,60]   (40,50]  \n[344] (60,70]   (60,70]   (80,90]   (70,80]   (80,90]   (110,120] (50,60]  \n[351] (30,40]   (60,70]   (30,40]   (40,50]   (70,80]   (80,90]   (30,40]  \n[358] (60,70]   (30,40]   (60,70]   (80,90]   (30,40]   (50,60]   (70,80]  \n[365] (70,80]   (110,120] (40,50]   (30,40]   (80,90]   (30,40]   (60,70]  \n[372] (60,70]   (60,70]   (40,50]   (70,80]   (70,80]   (30,40]   (30,40]  \n[379] (10,20]   (80,90]   (30,40]   (60,70]   (60,70]   (70,80]   (50,60]  \n[386] (20,30]   (40,50]   (70,80]   (40,50]   (60,70]   (70,80]   (70,80]  \n[393] (60,70]   (80,90]   (30,40]   (40,50]   (70,80]   (50,60]   (50,60]  \n[400] (40,50]   (80,90]   (40,50]   (40,50]   (80,90]   (50,60]   (70,80]  \n[407] (20,30]   (70,80]   (40,50]   (50,60]   (50,60]   (50,60]   (50,60]  \n[414] (50,60]   (10,20]   (40,50]   (30,40]   (80,90]   (20,30]   (20,30]  \n[421] (70,80]   (20,30]   (70,80]   (10,20]   (40,50]   (80,90]   (40,50]  \n[428] (40,50]   (40,50]   (50,60]   (40,50]   (40,50]   (60,70]   (40,50]  \n[435] (60,70]   (70,80]   (50,60]   (40,50]   (90,100]  (90,100]  (70,80]  \n[442] (40,50]   (50,60]   (20,30]   (70,80]   (20,30]   (60,70]   (80,90]  \n[449] (50,60]   (10,20]   (60,70]   (50,60]   (40,50]   (40,50]   (60,70]  \n[456] (50,60]   (50,60]   (110,120] (40,50]   (40,50]   (40,50]   (50,60]  \n[463] (30,40]   (70,80]   (30,40]   (100,110] (60,70]   (70,80]   (60,70]  \n[470] (40,50]   (40,50]   (70,80]   (40,50]   (20,30]   (50,60]   (40,50]  \n[477] (40,50]   (50,60]   (70,80]   (90,100]  (50,60]   (60,70]   (70,80]  \n[484] (50,60]   (70,80]   (50,60]   (70,80]   (60,70]   (90,100]  (50,60]  \n[491] (80,90]   (30,40]   (70,80]   (50,60]   (30,40]   (50,60]   (70,80]  \n[498] (70,80]   (70,80]   (40,50]   (110,120] (50,60]   (40,50]   (60,70]  \n[505] (50,60]   (50,60]   (70,80]   (60,70]   (100,110] (40,50]   (60,70]  \n[512] (40,50]   (110,120] (70,80]   (60,70]   (60,70]   (70,80]   (40,50]  \n[519] (40,50]   (30,40]   (50,60]   (50,60]   (30,40]   (80,90]   (50,60]  \n[526] (70,80]   (60,70]   (30,40]   (50,60]   (70,80]   (50,60]   (40,50]  \n[533] (60,70]   (50,60]   (40,50]   (50,60]   (50,60]   (70,80]   (60,70]  \n[540] (60,70]   (60,70]   (90,100]  (60,70]   (50,60]   (10,20]   (80,90]  \n[547] (40,50]   (40,50]   (60,70]   (60,70]   (40,50]   (40,50]   (50,60]  \n[554] (90,100]  (70,80]   (50,60]   (70,80]   (50,60]   (0,10]    (70,80]  \n[561] (40,50]   (10,20]   (70,80]   (50,60]   (50,60]   (50,60]   (80,90]  \n[568] (40,50]   (40,50]   (80,90]   (70,80]   (80,90]   (80,90]   (40,50]  \n[575] (70,80]   (60,70]   (70,80]   (80,90]   (50,60]   (50,60]   (30,40]  \n[582] (30,40]   (60,70]   (40,50]   (50,60]   (70,80]   (70,80]   (70,80]  \n[589] (30,40]   (70,80]   (50,60]   (50,60]   (90,100]  (40,50]   (40,50]  \n[596] (60,70]   (30,40]   (10,20]   (80,90]   (80,90]   (40,50]   (50,60]  \n[603] (50,60]   (50,60]   (70,80]   (70,80]   (60,70]   (70,80]   (30,40]  \n[610] (50,60]   (30,40]   (40,50]   (40,50]   (50,60]   (90,100]  (30,40]  \n[617] (90,100]  (50,60]   (80,90]   (50,60]   (60,70]   (40,50]   (50,60]  \n[624] (50,60]   (50,60]   (50,60]   (50,60]   (80,90]   (60,70]   (40,50]  \n[631] (70,80]   (60,70]   (80,90]   (60,70]   (40,50]   (70,80]   (0,10]   \n[638] (40,50]   (50,60]   (60,70]   (60,70]   (80,90]   (40,50]   (10,20]  \n[645] (70,80]   (60,70]   (80,90]   (70,80]   (80,90]   (60,70]   (40,50]  \n[652] (30,40]   (70,80]   (50,60]   (0,10]    (40,50]   (40,50]   (50,60]  \n[659] (50,60]   (20,30]   (50,60]   (70,80]   (60,70]   (60,70]   (80,90]  \n[666] (40,50]   (50,60]   (50,60]   (90,100]  (60,70]   (80,90]   (80,90]  \n[673] (80,90]   (70,80]   (100,110] (90,100]  (40,50]   (60,70]   (30,40]  \n[680] (50,60]   (50,60]   (10,20]   (70,80]   (80,90]   (30,40]   (60,70]  \n[687] (60,70]   (70,80]   (50,60]   (50,60]   (50,60]   (30,40]   (30,40]  \n[694] (40,50]   (60,70]   (70,80]   (50,60]   (70,80]   (80,90]   (70,80]  \n[701] (70,80]   (70,80]   (40,50]   (80,90]   (50,60]   (50,60]   (40,50]  \n[708] (0,10]    (60,70]   (30,40]   (30,40]   (50,60]   (70,80]   (40,50]  \n[715] (40,50]   (70,80]   (30,40]   (120,130]\n13 Levels: (0,10] (10,20] (20,30] (30,40] (40,50] (50,60] (60,70] ... (120,130]\n\n\nThe observations have now been grouped to the classes. You can see this explicitly in the data viewer:\n\nView(air_quality_data)\n\n\n\n\n\n\n\nImportant\n\n\n\nWhat have we done here? The first value under the NO2_est column is 61, this value falls between 61-70 and hence under the Group column is was classed into the (60,70] interval by the cut() function. The second value in NO2_est is 59, and hence it was classed into the (50,60] interval, and so on. Note that the interval (60, 70] is the equivalent of saying: “any numbers greater than 60 (i.e., from 60.01 onwards but excluding 60 from that interval), and at the same time being less than or equal to 70, inclusively”. The round bracket ( represents any number greater than 60, whilst the square bracket ] closes the interval translating to any number being less than or equal 70 at the same time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nWe can now generate our Frequency Table and hence determine Frequency and Cumulative Frequency of the ambient levels of NO\\(_2\\) in Eixample. We perform by using the table() function to tabulate the frequency of values that were grouped within an interval using the Group column.\n\ntable(air_quality_data$Groups)\n\n\n   (0,10]   (10,20]   (20,30]   (30,40]   (40,50]   (50,60]   (60,70]   (70,80] \n        6        17        24        68       131       137       109       116 \n  (80,90]  (90,100] (100,110] (110,120] (120,130] \n       72        20         7         7         4 \n\n\nUsing table() function only shows results in the Console - lets store the table results in a data frame object and call it frequency_results:\n\nfrequency_results &lt;- data.frame(table(air_quality_data$Groups))\nfrequency_results\n\n        Var1 Freq\n1     (0,10]    6\n2    (10,20]   17\n3    (20,30]   24\n4    (30,40]   68\n5    (40,50]  131\n6    (50,60]  137\n7    (60,70]  109\n8    (70,80]  116\n9    (80,90]   72\n10  (90,100]   20\n11 (100,110]    7\n12 (110,120]    7\n13 (120,130]    4\n\n\nYou can see column names Var1 and Freq. The Var1 is the original Groups columns which incidentally been renamed to Var1. The Freq column was generated from the table() function. We can rename the 1st and 2nd columns using colnames().\n\n# rename first column t9 \"Groups\"\n# rename second column to \"Frequency\"\n# print new variable names in console using names() function\n\ncolnames(frequency_results)[1] &lt;- \"Groups\"    \ncolnames(frequency_results)[2] &lt;- \"Frequency\" \nnames(frequency_results) \n\n[1] \"Groups\"    \"Frequency\"\n\nfrequency_results\n\n      Groups Frequency\n1     (0,10]         6\n2    (10,20]        17\n3    (20,30]        24\n4    (30,40]        68\n5    (40,50]       131\n6    (50,60]       137\n7    (60,70]       109\n8    (70,80]       116\n9    (80,90]        72\n10  (90,100]        20\n11 (100,110]         7\n12 (110,120]         7\n13 (120,130]         4\n\n\nFinally, we derive the Relative Frequency i.e., a percentage that is derived by dividing each frequency value from a group by the total number of observations (i.e., in this case: 718). We can add the relativeFreq column to the frequency_results table.\n\n# generate a new column\nfrequency_results$relativeFreq &lt;- frequency_results$Frequency/718\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation of frequency: The above table output show the frequency distribution of a set of concentrations for Nitrogen Dioxide measured in Eixample (in Barcelona). The group with the highest frequency value is 50-60ppb (i.e., 137) which accounts for 0.1908 (19.08%) of the data. These measurements typically fall under the category that’s considered to cause moderate harm to humans.\n\n\nLet’s add the Cumulative Frequency and Cumulative Relative Frequency i.e., percentage using this cumulative summation code i.e., cumsum function below:\n\n# add cumulativeFreq column to the data frame by adding Frequency using cumsum() function\nfrequency_results$cumulativeFreq &lt;- cumsum(frequency_results$Frequency)\n\n# add cumulativeRelFreq column to the data frame by adding Frequency using cumsum() function\nfrequency_results$cumulativeRelFreq &lt;- cumsum(frequency_results$relativeFreq)\n\n# print table results\nfrequency_results\n\n      Groups Frequency relativeFreq cumulativeFreq cumulativeRelFreq\n1     (0,10]         6  0.008356546              6       0.008356546\n2    (10,20]        17  0.023676880             23       0.032033426\n3    (20,30]        24  0.033426184             47       0.065459610\n4    (30,40]        68  0.094707521            115       0.160167131\n5    (40,50]       131  0.182451253            246       0.342618384\n6    (50,60]       137  0.190807799            383       0.533426184\n7    (60,70]       109  0.151810585            492       0.685236769\n8    (70,80]       116  0.161559889            608       0.846796657\n9    (80,90]        72  0.100278552            680       0.947075209\n10  (90,100]        20  0.027855153            700       0.974930362\n11 (100,110]         7  0.009749304            707       0.984679666\n12 (110,120]         7  0.009749304            714       0.994428969\n13 (120,130]         4  0.005571031            718       1.000000000\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThoughts to self: cumsum function… like WTF man?!\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation of cumulative frequency: The above table output show the cumulative frequency distribution ambient concentrations for Nitrogen Dioxide measured in Eixample (in Barcelona). We can see that there are 246 measurements or less with N0\\(_2\\) concentrations to be considered as negligible or low impact to health (&lt;50ppb). This corresponds to 0.3426 (34.26%) of the data.\nConversely, we can also say - we can see that there are 472 measurements with N0\\(_2\\) concentrations more than 50ppb which is considered to be moderate or high impact to human health. This corresponds to 0.6573 (65.73%) of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nThe frequency table for Frequencies and Cumulative Frequencies can be graphical represented in a form of Histogram and Cumulative Frequency Plot (or Ogive Plot) respectively. Now, the data we need must be in its original form (i.e., not grouped) to plot the histogram, and we will need to use the classes object which we created earlier on from the seq() function so as to be used as breaks in the hist() plot function:\n\nhist(air_quality_data$NO2_est, breaks = classes)\n\n\n\n\n\n\n\n\nThe above graph is not to the expected standards! It is missing key details such as the title and label for the x-axis. Let’s apply some cosmetics such as a main title and label for the x-axis\n\nhist(air_quality_data$NO2_est, breaks = classes, main = \"Histogram for NO2 in Barcelona\", xlab = \"NO2 estimates (ppb)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation of histogram: The above figure output describes the shape for ambient measures of NO\\(_2\\) in Barcelona which appears bell-shaped centered around 60ppb. Note that the frequency bars in this graph are essentially the same as the frequency values in the table.\n\n\nLastly, we then compute its cumulative frequency with the cumsum() function to support the interpretation. The coding needs a bit of hacking because we need to force a starting zero element for this graph to work.\n\ncumulfreq0 &lt;- c(0, cumsum(frequency_results$Frequency))\nplot(classes, cumulfreq0)\n\n\n\n\n\n\n\n\nApply the appropriate cosmetics to output by adding the following:\n\nMain title\nAxis titles for x- and y-axis\nConnecting the points to form a full cumulative frequency/ogive plot\n\n\ncumulfreq0 &lt;- c(0, cumsum(frequency_results$Frequency))\nplot(classes, cumulfreq0, main=\"Cumulative Frequency for N02 in Barcelona\", xlab=\"NO2 estimates (ppb)\", ylab=\"Cumulative Frequencies\")\nlines(classes, cumulfreq0) \n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate a Frequency Distribution Table for the PM\\(_10\\) variable in the air_quality_data data frame object. It should contain the following:\n\n\nFrequency\nRelative Frequency\nCumulative Frequency\nCumulative Relative Frequency\n\n\n\n\n\n\n\nNote\n\n\n\nHINTS to consider:\n\nUse the min() and max() functions to know the minimum and maximum values for PM\\(_10\\) in the dataset.\nUse the sequence function i.e., seq() to generate a sequence of numbers to be made classes for grouping the PM\\(_10\\) values. Consider using the interval of 5 in your generating the sequence of numbers.\nUse the cut accordingly to create the groups\nUse both the table() and data.frame() functions accordingly to generate the desired outputs\nFor the cumulative estimates - use the cumsum() function\n\n\n\n\nWhat would the appropriate interpretation be for the relative frequency for highest PM\\(_10\\) group measured in Barcelona?\nGenerate a histogram plot using hist() function that is fully annotated with title and x- and y-titles.\n\n\nHave a go at these questions before revealing the solution codes and output below\n\n\n\nClick here to see solution code:\n\n\n# Solutions for 1\n# Using PM10\nmin(air_quality_data$PM10_est)\n\n[1] 3\n\nmax(air_quality_data$PM10_est)\n\n[1] 60\n\n# create classes\nclasses_PM10 &lt;- seq(0, 60, 5)\nair_quality_data$Groups_PM10 &lt;- cut(air_quality_data$PM10_est, breaks = classes_PM10)\ntable(air_quality_data$Groups_PM10)\n\n\n  (0,5]  (5,10] (10,15] (15,20] (20,25] (25,30] (30,35] (35,40] (40,45] (45,50] \n      2       9      20      60      87     143     140     124      75      38 \n(50,55] (55,60] \n     18       2 \n\n# Generating Frequency Tables\nfrequency_table_PM10 &lt;- data.frame(table(air_quality_data$Groups_PM10))\n# renaming columns\ncolnames(frequency_table_PM10)[1] &lt;- \"Groups\"\ncolnames(frequency_table_PM10)[2] &lt;- \"Frequency\"\n# Calculation of relative frequency (proportion)\nfrequency_table_PM10$relativeFreq &lt;- frequency_table_PM10$Frequency/718\n# Calculation of cumulative frequency and cumulative relative frequency (proportion)\nfrequency_table_PM10$cumulativeFreq &lt;- cumsum(frequency_table_PM10$Frequency)\nfrequency_table_PM10$cumulativeRelFreq &lt;- cumsum(frequency_table_PM10$relativeFreq)\n# show table\nhead(frequency_table_PM10, 12)\n\n    Groups Frequency relativeFreq cumulativeFreq cumulativeRelFreq\n1    (0,5]         2  0.002785515              2       0.002785515\n2   (5,10]         9  0.012534819             11       0.015320334\n3  (10,15]        20  0.027855153             31       0.043175487\n4  (15,20]        60  0.083565460             91       0.126740947\n5  (20,25]        87  0.121169916            178       0.247910864\n6  (25,30]       143  0.199164345            321       0.447075209\n7  (30,35]       140  0.194986072            461       0.642061281\n8  (35,40]       124  0.172701950            585       0.814763231\n9  (40,45]        75  0.104456825            660       0.919220056\n10 (45,50]        38  0.052924791            698       0.972144847\n11 (50,55]        18  0.025069638            716       0.997214485\n12 (55,60]         2  0.002785515            718       1.000000000\n\n# Solutions for 2\n# For PM10, the group with the highest frequency value is `25-30μm` (i.e., 143) which accounts for 0.1991\n# (19.91%) of the data.\n\n# Solutions for 3\n# histogram plot\nhist(air_quality_data$PM10_est, \n    breaks = classes_PM10, main = \"Histogram: PM10 in Barcelona\", \n    xlab = \"PM10 estimates [μm]\")",
    "crumbs": [
      "Core Content",
      "Week 3: Examining Data I"
    ]
  },
  {
    "objectID": "04-frequency_distribution.html#learning-outcomes",
    "href": "04-frequency_distribution.html#learning-outcomes",
    "title": "Week 3: Examining Data I",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nUnderstand frequency distributions\nHow to categorise continuous measurements in classes/categorise using the cut()\nHow to compute a frequency table\nHow to provide an interpretation to such outputs\nHow to translate a frequency distribution table into a histogram and cumulative frequency plot\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore we do anything - make sure to have downloaded the dataset for this computer session by clicking [HERE]. It contains the file Barcelona_Air_Pollution_data.csv - this comma separated values (CSV) file contains the data needed to follow today’s tutorial.\nInstructions In your computer, do the following:\n\nGo to the folder named GEOG0186 - this should have been created in Week 2\nNext, create a new sub-folder within GEOG0186 and rename it as Week 3.\nFrom the downloaded folder Dataset for Week 3, make sure to unzip and transfer ALL the datasets directly to the Week 3 folder.",
    "crumbs": [
      "Core Content",
      "Week 3: Examining Data I"
    ]
  },
  {
    "objectID": "04-frequency_distribution.html#short-lecture-videos-optional",
    "href": "04-frequency_distribution.html#short-lecture-videos-optional",
    "title": "Week 3: Examining Data I",
    "section": "",
    "text": "Jump straight to the coding exercise by clicking HERE if you want to skip this section!\n\n\nDefinition 1: Statistics is a branch in the mathematical sciences that pertains to the collection, analysis, interpretation, and graphical presentation of data. The best thing about statistics is that it’s a highly applied branch of science which is applicable to many areas such as social science, politics, health (e.g., epidemiology), business & finance, environmental sciences and geography.\nStatistics is broadly split into two main areas:\n\nDescriptive statistics, which focuses on describing the visible characteristics about a dataset\nInferential statistics is more research-based, which focuses on making predictions (rather than stating facts) and testing hypothesis about a phenomenon.\n\nDefinition 2: A variable is any characteristics, numbered value, or quantity that can be measured or counted. A variable can also be referred to a data Item. A variable can be broadly classified as discrete, continuous or categorical variable.\nWe have provided two videos: the first which broadly explains why statistics as a subject is important; and the second explains in much details what statistics is as a subject, and what are the various data types.\n[Theory] Why is statistics important? (Length: 13:21 minutes)\n\n\n\n\n\n\n\n\n\nWatch on YouTube [LINK]\n[Theory] What is statistics, and the types of variables? (Length: 31:17 minutes)\n\n\n\n\n\n\n\n\n\nWatch on YouTube [LINK]",
    "crumbs": [
      "Core Content",
      "Week 3: Examining Data I"
    ]
  },
  {
    "objectID": "04-frequency_distribution.html#analysing-air-pollution-data-in-barcelona-part-i",
    "href": "04-frequency_distribution.html#analysing-air-pollution-data-in-barcelona-part-i",
    "title": "Week 3: Examining Data I",
    "section": "",
    "text": "We will focus on descriptive statistics as an introduction introducing everyone to the absolute basics. Descriptive statistics is all about knowing the data types and finding the distribution, central tendency and variability in such data set. These four key words may sound intimidating – but trust me – it is very easy! Let us learn how to perform the following aspect of finding the distribution in RStudio using the air pollution data for Barcelona.\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nLet us import for following dataset Barcelona_Air_Pollution_data.csv into RStudio, and call this object air_ quality_data.\nRemember - always make sure that your work directory is linked to your folder containing your data.\nFor Windows:\n\nsetwd(\"C:/Users/accountName/Desktop/GEOG0186/Week 3\")\n\nFor Macs:\n\nsetwd(\"/Users/accountName/Desktop/GEOG0186/Week 3\")\n\nNow, import you the data set as follows:\n\nair_quality_data &lt;- read.csv(\"Barcelona_Air_Pollution_data.csv\")\n\nYou use the command View() see the full data viewer, or head() to see the first five rows of the dataset.\n\n# see imported dataset\nView(air_quality_data)\n\n\nhead(air_quality_data)\n\nYou will notice that the data contains six variables with the following information:\n\n\n\n\n\n\n\n\nVariable name\nVariable Type\nInformation\n\n\n\n\nLocation\nString/Text only\nName of location Eixample, Barcelona\n\n\nReadingDate\nDate\nData collection date for air quality measures\n\n\nNO2_est\nContinuous\nMeasurements for Nitrogen dioxide (NO\\(_2\\)) (ppb)\n\n\nNO2_category\nCategorical\nHealth impacts (negligible/low/moderate/high)\n\n\nPM10_est\nContinuous\nMeasurements for Particulate matter (PM\\(_10\\))\n\n\nPM10_category\nCategorical\nHealth impacts (negligible/low/moderate)\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe NO2_est, for example, contains measurable items i.e., 718 observations for concentrations of ambient NO\\(_2\\) in Eixample area of Barcelona, and hence its a continuous variable. These estimates have been categorised in accordance with their health dangers i.e., negligible (\\(&lt;\\) 10 ppb); low (11-50 ppb);\n\n\nLet us begin to analyse NO2_est and NO2_category with Frequency Distributions",
    "crumbs": [
      "Core Content",
      "Week 3: Examining Data I"
    ]
  },
  {
    "objectID": "04-frequency_distribution.html#frequency-distributions",
    "href": "04-frequency_distribution.html#frequency-distributions",
    "title": "Week 3: Examining Data I",
    "section": "",
    "text": "[Watch on YouTube]\nWe use frequency distribution to analyse a set of continuous measurements. In data handling in this context, there are two outputs generated:\n\nFrequency, which tells us how often a particular result was obtained. From this we can calculate a percentage value which is referred to as Relative Frequency.\n\nCumulative Frequency, this is a cumulative sum of the frequencies, which indicates how often a result was obtained that is less than a stated value in our collection of data. Again, from this we can also calculate a cumulative percentage value which is referred to as Cumulative Relative Frequency.\n\nSuppose, we want to assess the 718 observations for air pollutant Nitrogen Dioxide (NO\\(_2\\)).\nLet’s list the observations for Nitrogen Dioxide (NO\\(_2\\)) in Barcelona:\n\nair_quality_data$NO2_est\n\n  [1]  61  59  29  75  23  49  43  35  83  75  71  56  54  44  41  54  62  56\n [19]  26  42  71  86  85  52  56  45  68  86  69  71   4  82  43  51 114  43\n [37]  18  58  24  53  98  53 100  53  49  49  46  82  77  67  76  52  61  80\n [55]  77  70  56  49  42  73  64  33  71  72  13  37  26  46  84  72  65  76\n [73]  90  46  61  81  64  62  58  78  83  37 130  43  20  40 102  68  48  74\n [91]  52  43  80  71  42  84  44 121  41  66  44  50  38  75  41  45  48  63\n[109]  53  63  63  46  34  87  75  74  36  69  46  15  80  75  83  95   5  65\n[127]  21  84  68  32  45  73  53  31  85  91  73  46  25  75  70  84  68  65\n[145]  58 113  62  60  55  69  82 100 105  47  60 103  53  34  39  22  21  71\n[163]  85  56  73  61  24  44  47  49 100  64  91  79  42  32  33  84  43  61\n[181]  63  49  80  46  58  45  37  66  60  75  35  75  48  43  57  67  54  38\n[199]  22  51  69  51  64  32  20  52  42  65  69  47  40  34  34  51  57  43\n[217]  52  86  53  43  54  75  56  62  41  84  41  22  83  76  51  31  50  65\n[235]  76  77  61  50  75  49  47  65  78  39  51  49  75  45  50  69  86  75\n[253]  89  68  84  90  90  56 106  63  90  57  38  86  22  39  19  61  44  63\n[271]  52  42  46  56  40  69  62  42  54  17  49  84  34  89  65  53  78  67\n[289]  55  61  39  82  58  15  63  76  55  80  56  79  72  58  74  27  93  40\n[307]  40  58  79  81 123  84  37  87  38  49  91  50  59  69  57  68  53  38\n[325]  51  78  71  72  55  70  56  63  85  78  64  23  84  43  46  33  59  58\n[343]  47  64  68  89  76  86 116  52  34  63  40  41  72  87  37  62  38  68\n[361]  88  39  59  77  75 112  44  40  90  37  66  61  65  50  79  79  36  36\n[379]  12  86  40  62  63  71  53  30  44  76  41  62  77  80  62  86  37  48\n[397]  80  55  56  49  84  48  49  84  60  76  28  77  41  57  55  51  54  54\n[415]  11  43  38  86  30  23  78  29  80  16  48  90  44  42  50  54  45  42\n[433]  70  49  67  73  60  42  99  97  77  46  52  24  75  30  70  81  53  17\n[451]  63  59  44  41  67  56  58 111  43  47  49  58  36  72  36 103  63  77\n[469]  65  42  42  79  41  24  59  50  46  55  77  91  54  70  73  53  80  53\n[487]  72  67  95  57  87  39  73  56  34  56  75  74  72  42 119  55  43  69\n[505]  55  52  77  63 108  43  61  47 117  80  61  67  78  49  42  35  58  54\n[523]  36  84  56  72  70  40  59  71  56  49  66  52  48  60  54  73  66  67\n[541]  70  93  65  60  13  83  49  42  62  63  50  46  54  94  73  54  74  54\n[559]  10  71  41  17  75  55  54  54  83  47  49  90  76  89  83  43  76  67\n[577]  75  88  59  60  34  36  63  42  59  71  73  73  40  74  53  56  99  46\n[595]  46  64  37  20  84  86  47  57  54  56  78  73  65  72  37  57  38  46\n[613]  43  57  98  32  98  53  86  59  63  42  60  60  51  58  59  83  67  42\n[631]  74  62  84  67  49  76   2  44  51  69  69  87  49  18  73  66  81  78\n[649]  82  69  50  36  71  60   7  50  49  60  55  25  58  76  69  61  88  45\n[667]  59  59  91  61  81  81  83  71 108  99  46  69  38  54  59  16  75  81\n[685]  35  63  65  71  57  53  58  37  39  43  64  76  56  72  87  74  74  77\n[703]  46  87  53  60  47  10  61  35  36  60  71  45  47  79  37 123\n\n\nIn a list format it is quite difficult to make head or tail on what observations appear frequently and its distribution. To summarise this - it will be helpful to classify the information into Classes and then obtain the Frequency and Cumulative Frequency in a table. We call this table a Frequency Table.\nThe minimum value for NO\\(_2\\) is 2 and the maximum is 130. We can group the 718 observations into 13 classes using an interval of 10s e.g., 1-10, 11-20, 21-30, 31-40, 41-50, 51-60, 61-70, 71-80, 81-90, 91-100, 101-110, 111-120 and 121-130\n\n\n\n\n\n\nImportant\n\n\n\nThe way and manner you specify the classes and interval are up to you really. Here, 10 is being used for convenience.\n\n\nThe interval width is 10, we can generate sequence of number from 0 to 130, inclusively, to create the classes which in turn be used to group the 718 observations into 13 classes using the seq() and cut().\nFor example:\n\n# using starting value as 0\n# using highest value as 130\n# using interval as 10\n\n#  specify in this order the lower, highest, interval value in seq() function\nclasses &lt;- seq(0, 130, 10)\nclasses\n\n [1]   0  10  20  30  40  50  60  70  80  90 100 110 120 130\n\n\nThe sequence of values are stored in the object called classes. Now, let us apply the cut() function to group the NO\\(_2\\) data accordingly. We can do this by generating a new variable called Groups.\n\n# tell the cut() function to group NO2_est using the classes object\nair_quality_data$Groups &lt;- cut(air_quality_data$NO2_est, breaks=classes)\nair_quality_data$Groups\n\n  [1] (60,70]   (50,60]   (20,30]   (70,80]   (20,30]   (40,50]   (40,50]  \n  [8] (30,40]   (80,90]   (70,80]   (70,80]   (50,60]   (50,60]   (40,50]  \n [15] (40,50]   (50,60]   (60,70]   (50,60]   (20,30]   (40,50]   (70,80]  \n [22] (80,90]   (80,90]   (50,60]   (50,60]   (40,50]   (60,70]   (80,90]  \n [29] (60,70]   (70,80]   (0,10]    (80,90]   (40,50]   (50,60]   (110,120]\n [36] (40,50]   (10,20]   (50,60]   (20,30]   (50,60]   (90,100]  (50,60]  \n [43] (90,100]  (50,60]   (40,50]   (40,50]   (40,50]   (80,90]   (70,80]  \n [50] (60,70]   (70,80]   (50,60]   (60,70]   (70,80]   (70,80]   (60,70]  \n [57] (50,60]   (40,50]   (40,50]   (70,80]   (60,70]   (30,40]   (70,80]  \n [64] (70,80]   (10,20]   (30,40]   (20,30]   (40,50]   (80,90]   (70,80]  \n [71] (60,70]   (70,80]   (80,90]   (40,50]   (60,70]   (80,90]   (60,70]  \n [78] (60,70]   (50,60]   (70,80]   (80,90]   (30,40]   (120,130] (40,50]  \n [85] (10,20]   (30,40]   (100,110] (60,70]   (40,50]   (70,80]   (50,60]  \n [92] (40,50]   (70,80]   (70,80]   (40,50]   (80,90]   (40,50]   (120,130]\n [99] (40,50]   (60,70]   (40,50]   (40,50]   (30,40]   (70,80]   (40,50]  \n[106] (40,50]   (40,50]   (60,70]   (50,60]   (60,70]   (60,70]   (40,50]  \n[113] (30,40]   (80,90]   (70,80]   (70,80]   (30,40]   (60,70]   (40,50]  \n[120] (10,20]   (70,80]   (70,80]   (80,90]   (90,100]  (0,10]    (60,70]  \n[127] (20,30]   (80,90]   (60,70]   (30,40]   (40,50]   (70,80]   (50,60]  \n[134] (30,40]   (80,90]   (90,100]  (70,80]   (40,50]   (20,30]   (70,80]  \n[141] (60,70]   (80,90]   (60,70]   (60,70]   (50,60]   (110,120] (60,70]  \n[148] (50,60]   (50,60]   (60,70]   (80,90]   (90,100]  (100,110] (40,50]  \n[155] (50,60]   (100,110] (50,60]   (30,40]   (30,40]   (20,30]   (20,30]  \n[162] (70,80]   (80,90]   (50,60]   (70,80]   (60,70]   (20,30]   (40,50]  \n[169] (40,50]   (40,50]   (90,100]  (60,70]   (90,100]  (70,80]   (40,50]  \n[176] (30,40]   (30,40]   (80,90]   (40,50]   (60,70]   (60,70]   (40,50]  \n[183] (70,80]   (40,50]   (50,60]   (40,50]   (30,40]   (60,70]   (50,60]  \n[190] (70,80]   (30,40]   (70,80]   (40,50]   (40,50]   (50,60]   (60,70]  \n[197] (50,60]   (30,40]   (20,30]   (50,60]   (60,70]   (50,60]   (60,70]  \n[204] (30,40]   (10,20]   (50,60]   (40,50]   (60,70]   (60,70]   (40,50]  \n[211] (30,40]   (30,40]   (30,40]   (50,60]   (50,60]   (40,50]   (50,60]  \n[218] (80,90]   (50,60]   (40,50]   (50,60]   (70,80]   (50,60]   (60,70]  \n[225] (40,50]   (80,90]   (40,50]   (20,30]   (80,90]   (70,80]   (50,60]  \n[232] (30,40]   (40,50]   (60,70]   (70,80]   (70,80]   (60,70]   (40,50]  \n[239] (70,80]   (40,50]   (40,50]   (60,70]   (70,80]   (30,40]   (50,60]  \n[246] (40,50]   (70,80]   (40,50]   (40,50]   (60,70]   (80,90]   (70,80]  \n[253] (80,90]   (60,70]   (80,90]   (80,90]   (80,90]   (50,60]   (100,110]\n[260] (60,70]   (80,90]   (50,60]   (30,40]   (80,90]   (20,30]   (30,40]  \n[267] (10,20]   (60,70]   (40,50]   (60,70]   (50,60]   (40,50]   (40,50]  \n[274] (50,60]   (30,40]   (60,70]   (60,70]   (40,50]   (50,60]   (10,20]  \n[281] (40,50]   (80,90]   (30,40]   (80,90]   (60,70]   (50,60]   (70,80]  \n[288] (60,70]   (50,60]   (60,70]   (30,40]   (80,90]   (50,60]   (10,20]  \n[295] (60,70]   (70,80]   (50,60]   (70,80]   (50,60]   (70,80]   (70,80]  \n[302] (50,60]   (70,80]   (20,30]   (90,100]  (30,40]   (30,40]   (50,60]  \n[309] (70,80]   (80,90]   (120,130] (80,90]   (30,40]   (80,90]   (30,40]  \n[316] (40,50]   (90,100]  (40,50]   (50,60]   (60,70]   (50,60]   (60,70]  \n[323] (50,60]   (30,40]   (50,60]   (70,80]   (70,80]   (70,80]   (50,60]  \n[330] (60,70]   (50,60]   (60,70]   (80,90]   (70,80]   (60,70]   (20,30]  \n[337] (80,90]   (40,50]   (40,50]   (30,40]   (50,60]   (50,60]   (40,50]  \n[344] (60,70]   (60,70]   (80,90]   (70,80]   (80,90]   (110,120] (50,60]  \n[351] (30,40]   (60,70]   (30,40]   (40,50]   (70,80]   (80,90]   (30,40]  \n[358] (60,70]   (30,40]   (60,70]   (80,90]   (30,40]   (50,60]   (70,80]  \n[365] (70,80]   (110,120] (40,50]   (30,40]   (80,90]   (30,40]   (60,70]  \n[372] (60,70]   (60,70]   (40,50]   (70,80]   (70,80]   (30,40]   (30,40]  \n[379] (10,20]   (80,90]   (30,40]   (60,70]   (60,70]   (70,80]   (50,60]  \n[386] (20,30]   (40,50]   (70,80]   (40,50]   (60,70]   (70,80]   (70,80]  \n[393] (60,70]   (80,90]   (30,40]   (40,50]   (70,80]   (50,60]   (50,60]  \n[400] (40,50]   (80,90]   (40,50]   (40,50]   (80,90]   (50,60]   (70,80]  \n[407] (20,30]   (70,80]   (40,50]   (50,60]   (50,60]   (50,60]   (50,60]  \n[414] (50,60]   (10,20]   (40,50]   (30,40]   (80,90]   (20,30]   (20,30]  \n[421] (70,80]   (20,30]   (70,80]   (10,20]   (40,50]   (80,90]   (40,50]  \n[428] (40,50]   (40,50]   (50,60]   (40,50]   (40,50]   (60,70]   (40,50]  \n[435] (60,70]   (70,80]   (50,60]   (40,50]   (90,100]  (90,100]  (70,80]  \n[442] (40,50]   (50,60]   (20,30]   (70,80]   (20,30]   (60,70]   (80,90]  \n[449] (50,60]   (10,20]   (60,70]   (50,60]   (40,50]   (40,50]   (60,70]  \n[456] (50,60]   (50,60]   (110,120] (40,50]   (40,50]   (40,50]   (50,60]  \n[463] (30,40]   (70,80]   (30,40]   (100,110] (60,70]   (70,80]   (60,70]  \n[470] (40,50]   (40,50]   (70,80]   (40,50]   (20,30]   (50,60]   (40,50]  \n[477] (40,50]   (50,60]   (70,80]   (90,100]  (50,60]   (60,70]   (70,80]  \n[484] (50,60]   (70,80]   (50,60]   (70,80]   (60,70]   (90,100]  (50,60]  \n[491] (80,90]   (30,40]   (70,80]   (50,60]   (30,40]   (50,60]   (70,80]  \n[498] (70,80]   (70,80]   (40,50]   (110,120] (50,60]   (40,50]   (60,70]  \n[505] (50,60]   (50,60]   (70,80]   (60,70]   (100,110] (40,50]   (60,70]  \n[512] (40,50]   (110,120] (70,80]   (60,70]   (60,70]   (70,80]   (40,50]  \n[519] (40,50]   (30,40]   (50,60]   (50,60]   (30,40]   (80,90]   (50,60]  \n[526] (70,80]   (60,70]   (30,40]   (50,60]   (70,80]   (50,60]   (40,50]  \n[533] (60,70]   (50,60]   (40,50]   (50,60]   (50,60]   (70,80]   (60,70]  \n[540] (60,70]   (60,70]   (90,100]  (60,70]   (50,60]   (10,20]   (80,90]  \n[547] (40,50]   (40,50]   (60,70]   (60,70]   (40,50]   (40,50]   (50,60]  \n[554] (90,100]  (70,80]   (50,60]   (70,80]   (50,60]   (0,10]    (70,80]  \n[561] (40,50]   (10,20]   (70,80]   (50,60]   (50,60]   (50,60]   (80,90]  \n[568] (40,50]   (40,50]   (80,90]   (70,80]   (80,90]   (80,90]   (40,50]  \n[575] (70,80]   (60,70]   (70,80]   (80,90]   (50,60]   (50,60]   (30,40]  \n[582] (30,40]   (60,70]   (40,50]   (50,60]   (70,80]   (70,80]   (70,80]  \n[589] (30,40]   (70,80]   (50,60]   (50,60]   (90,100]  (40,50]   (40,50]  \n[596] (60,70]   (30,40]   (10,20]   (80,90]   (80,90]   (40,50]   (50,60]  \n[603] (50,60]   (50,60]   (70,80]   (70,80]   (60,70]   (70,80]   (30,40]  \n[610] (50,60]   (30,40]   (40,50]   (40,50]   (50,60]   (90,100]  (30,40]  \n[617] (90,100]  (50,60]   (80,90]   (50,60]   (60,70]   (40,50]   (50,60]  \n[624] (50,60]   (50,60]   (50,60]   (50,60]   (80,90]   (60,70]   (40,50]  \n[631] (70,80]   (60,70]   (80,90]   (60,70]   (40,50]   (70,80]   (0,10]   \n[638] (40,50]   (50,60]   (60,70]   (60,70]   (80,90]   (40,50]   (10,20]  \n[645] (70,80]   (60,70]   (80,90]   (70,80]   (80,90]   (60,70]   (40,50]  \n[652] (30,40]   (70,80]   (50,60]   (0,10]    (40,50]   (40,50]   (50,60]  \n[659] (50,60]   (20,30]   (50,60]   (70,80]   (60,70]   (60,70]   (80,90]  \n[666] (40,50]   (50,60]   (50,60]   (90,100]  (60,70]   (80,90]   (80,90]  \n[673] (80,90]   (70,80]   (100,110] (90,100]  (40,50]   (60,70]   (30,40]  \n[680] (50,60]   (50,60]   (10,20]   (70,80]   (80,90]   (30,40]   (60,70]  \n[687] (60,70]   (70,80]   (50,60]   (50,60]   (50,60]   (30,40]   (30,40]  \n[694] (40,50]   (60,70]   (70,80]   (50,60]   (70,80]   (80,90]   (70,80]  \n[701] (70,80]   (70,80]   (40,50]   (80,90]   (50,60]   (50,60]   (40,50]  \n[708] (0,10]    (60,70]   (30,40]   (30,40]   (50,60]   (70,80]   (40,50]  \n[715] (40,50]   (70,80]   (30,40]   (120,130]\n13 Levels: (0,10] (10,20] (20,30] (30,40] (40,50] (50,60] (60,70] ... (120,130]\n\n\nThe observations have now been grouped to the classes. You can see this explicitly in the data viewer:\n\nView(air_quality_data)\n\n\n\n\n\n\n\nImportant\n\n\n\nWhat have we done here? The first value under the NO2_est column is 61, this value falls between 61-70 and hence under the Group column is was classed into the (60,70] interval by the cut() function. The second value in NO2_est is 59, and hence it was classed into the (50,60] interval, and so on. Note that the interval (60, 70] is the equivalent of saying: “any numbers greater than 60 (i.e., from 60.01 onwards but excluding 60 from that interval), and at the same time being less than or equal to 70, inclusively”. The round bracket ( represents any number greater than 60, whilst the square bracket ] closes the interval translating to any number being less than or equal 70 at the same time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nWe can now generate our Frequency Table and hence determine Frequency and Cumulative Frequency of the ambient levels of NO\\(_2\\) in Eixample. We perform by using the table() function to tabulate the frequency of values that were grouped within an interval using the Group column.\n\ntable(air_quality_data$Groups)\n\n\n   (0,10]   (10,20]   (20,30]   (30,40]   (40,50]   (50,60]   (60,70]   (70,80] \n        6        17        24        68       131       137       109       116 \n  (80,90]  (90,100] (100,110] (110,120] (120,130] \n       72        20         7         7         4 \n\n\nUsing table() function only shows results in the Console - lets store the table results in a data frame object and call it frequency_results:\n\nfrequency_results &lt;- data.frame(table(air_quality_data$Groups))\nfrequency_results\n\n        Var1 Freq\n1     (0,10]    6\n2    (10,20]   17\n3    (20,30]   24\n4    (30,40]   68\n5    (40,50]  131\n6    (50,60]  137\n7    (60,70]  109\n8    (70,80]  116\n9    (80,90]   72\n10  (90,100]   20\n11 (100,110]    7\n12 (110,120]    7\n13 (120,130]    4\n\n\nYou can see column names Var1 and Freq. The Var1 is the original Groups columns which incidentally been renamed to Var1. The Freq column was generated from the table() function. We can rename the 1st and 2nd columns using colnames().\n\n# rename first column t9 \"Groups\"\n# rename second column to \"Frequency\"\n# print new variable names in console using names() function\n\ncolnames(frequency_results)[1] &lt;- \"Groups\"    \ncolnames(frequency_results)[2] &lt;- \"Frequency\" \nnames(frequency_results) \n\n[1] \"Groups\"    \"Frequency\"\n\nfrequency_results\n\n      Groups Frequency\n1     (0,10]         6\n2    (10,20]        17\n3    (20,30]        24\n4    (30,40]        68\n5    (40,50]       131\n6    (50,60]       137\n7    (60,70]       109\n8    (70,80]       116\n9    (80,90]        72\n10  (90,100]        20\n11 (100,110]         7\n12 (110,120]         7\n13 (120,130]         4\n\n\nFinally, we derive the Relative Frequency i.e., a percentage that is derived by dividing each frequency value from a group by the total number of observations (i.e., in this case: 718). We can add the relativeFreq column to the frequency_results table.\n\n# generate a new column\nfrequency_results$relativeFreq &lt;- frequency_results$Frequency/718\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation of frequency: The above table output show the frequency distribution of a set of concentrations for Nitrogen Dioxide measured in Eixample (in Barcelona). The group with the highest frequency value is 50-60ppb (i.e., 137) which accounts for 0.1908 (19.08%) of the data. These measurements typically fall under the category that’s considered to cause moderate harm to humans.\n\n\nLet’s add the Cumulative Frequency and Cumulative Relative Frequency i.e., percentage using this cumulative summation code i.e., cumsum function below:\n\n# add cumulativeFreq column to the data frame by adding Frequency using cumsum() function\nfrequency_results$cumulativeFreq &lt;- cumsum(frequency_results$Frequency)\n\n# add cumulativeRelFreq column to the data frame by adding Frequency using cumsum() function\nfrequency_results$cumulativeRelFreq &lt;- cumsum(frequency_results$relativeFreq)\n\n# print table results\nfrequency_results\n\n      Groups Frequency relativeFreq cumulativeFreq cumulativeRelFreq\n1     (0,10]         6  0.008356546              6       0.008356546\n2    (10,20]        17  0.023676880             23       0.032033426\n3    (20,30]        24  0.033426184             47       0.065459610\n4    (30,40]        68  0.094707521            115       0.160167131\n5    (40,50]       131  0.182451253            246       0.342618384\n6    (50,60]       137  0.190807799            383       0.533426184\n7    (60,70]       109  0.151810585            492       0.685236769\n8    (70,80]       116  0.161559889            608       0.846796657\n9    (80,90]        72  0.100278552            680       0.947075209\n10  (90,100]        20  0.027855153            700       0.974930362\n11 (100,110]         7  0.009749304            707       0.984679666\n12 (110,120]         7  0.009749304            714       0.994428969\n13 (120,130]         4  0.005571031            718       1.000000000\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThoughts to self: cumsum function… like WTF man?!\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation of cumulative frequency: The above table output show the cumulative frequency distribution ambient concentrations for Nitrogen Dioxide measured in Eixample (in Barcelona). We can see that there are 246 measurements or less with N0\\(_2\\) concentrations to be considered as negligible or low impact to health (&lt;50ppb). This corresponds to 0.3426 (34.26%) of the data.\nConversely, we can also say - we can see that there are 472 measurements with N0\\(_2\\) concentrations more than 50ppb which is considered to be moderate or high impact to human health. This corresponds to 0.6573 (65.73%) of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nThe frequency table for Frequencies and Cumulative Frequencies can be graphical represented in a form of Histogram and Cumulative Frequency Plot (or Ogive Plot) respectively. Now, the data we need must be in its original form (i.e., not grouped) to plot the histogram, and we will need to use the classes object which we created earlier on from the seq() function so as to be used as breaks in the hist() plot function:\n\nhist(air_quality_data$NO2_est, breaks = classes)\n\n\n\n\n\n\n\n\nThe above graph is not to the expected standards! It is missing key details such as the title and label for the x-axis. Let’s apply some cosmetics such as a main title and label for the x-axis\n\nhist(air_quality_data$NO2_est, breaks = classes, main = \"Histogram for NO2 in Barcelona\", xlab = \"NO2 estimates (ppb)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation of histogram: The above figure output describes the shape for ambient measures of NO\\(_2\\) in Barcelona which appears bell-shaped centered around 60ppb. Note that the frequency bars in this graph are essentially the same as the frequency values in the table.\n\n\nLastly, we then compute its cumulative frequency with the cumsum() function to support the interpretation. The coding needs a bit of hacking because we need to force a starting zero element for this graph to work.\n\ncumulfreq0 &lt;- c(0, cumsum(frequency_results$Frequency))\nplot(classes, cumulfreq0)\n\n\n\n\n\n\n\n\nApply the appropriate cosmetics to output by adding the following:\n\nMain title\nAxis titles for x- and y-axis\nConnecting the points to form a full cumulative frequency/ogive plot\n\n\ncumulfreq0 &lt;- c(0, cumsum(frequency_results$Frequency))\nplot(classes, cumulfreq0, main=\"Cumulative Frequency for N02 in Barcelona\", xlab=\"NO2 estimates (ppb)\", ylab=\"Cumulative Frequencies\")\nlines(classes, cumulfreq0)",
    "crumbs": [
      "Core Content",
      "Week 3: Examining Data I"
    ]
  },
  {
    "objectID": "04-frequency_distribution.html#exercise",
    "href": "04-frequency_distribution.html#exercise",
    "title": "Week 3: Examining Data I",
    "section": "",
    "text": "Generate a Frequency Distribution Table for the PM\\(_10\\) variable in the air_quality_data data frame object. It should contain the following:\n\n\nFrequency\nRelative Frequency\nCumulative Frequency\nCumulative Relative Frequency\n\n\n\n\n\n\n\nNote\n\n\n\nHINTS to consider:\n\nUse the min() and max() functions to know the minimum and maximum values for PM\\(_10\\) in the dataset.\nUse the sequence function i.e., seq() to generate a sequence of numbers to be made classes for grouping the PM\\(_10\\) values. Consider using the interval of 5 in your generating the sequence of numbers.\nUse the cut accordingly to create the groups\nUse both the table() and data.frame() functions accordingly to generate the desired outputs\nFor the cumulative estimates - use the cumsum() function\n\n\n\n\nWhat would the appropriate interpretation be for the relative frequency for highest PM\\(_10\\) group measured in Barcelona?\nGenerate a histogram plot using hist() function that is fully annotated with title and x- and y-titles.\n\n\nHave a go at these questions before revealing the solution codes and output below\n\n\n\nClick here to see solution code:\n\n\n# Solutions for 1\n# Using PM10\nmin(air_quality_data$PM10_est)\n\n[1] 3\n\nmax(air_quality_data$PM10_est)\n\n[1] 60\n\n# create classes\nclasses_PM10 &lt;- seq(0, 60, 5)\nair_quality_data$Groups_PM10 &lt;- cut(air_quality_data$PM10_est, breaks = classes_PM10)\ntable(air_quality_data$Groups_PM10)\n\n\n  (0,5]  (5,10] (10,15] (15,20] (20,25] (25,30] (30,35] (35,40] (40,45] (45,50] \n      2       9      20      60      87     143     140     124      75      38 \n(50,55] (55,60] \n     18       2 \n\n# Generating Frequency Tables\nfrequency_table_PM10 &lt;- data.frame(table(air_quality_data$Groups_PM10))\n# renaming columns\ncolnames(frequency_table_PM10)[1] &lt;- \"Groups\"\ncolnames(frequency_table_PM10)[2] &lt;- \"Frequency\"\n# Calculation of relative frequency (proportion)\nfrequency_table_PM10$relativeFreq &lt;- frequency_table_PM10$Frequency/718\n# Calculation of cumulative frequency and cumulative relative frequency (proportion)\nfrequency_table_PM10$cumulativeFreq &lt;- cumsum(frequency_table_PM10$Frequency)\nfrequency_table_PM10$cumulativeRelFreq &lt;- cumsum(frequency_table_PM10$relativeFreq)\n# show table\nhead(frequency_table_PM10, 12)\n\n    Groups Frequency relativeFreq cumulativeFreq cumulativeRelFreq\n1    (0,5]         2  0.002785515              2       0.002785515\n2   (5,10]         9  0.012534819             11       0.015320334\n3  (10,15]        20  0.027855153             31       0.043175487\n4  (15,20]        60  0.083565460             91       0.126740947\n5  (20,25]        87  0.121169916            178       0.247910864\n6  (25,30]       143  0.199164345            321       0.447075209\n7  (30,35]       140  0.194986072            461       0.642061281\n8  (35,40]       124  0.172701950            585       0.814763231\n9  (40,45]        75  0.104456825            660       0.919220056\n10 (45,50]        38  0.052924791            698       0.972144847\n11 (50,55]        18  0.025069638            716       0.997214485\n12 (55,60]         2  0.002785515            718       1.000000000\n\n# Solutions for 2\n# For PM10, the group with the highest frequency value is `25-30μm` (i.e., 143) which accounts for 0.1991\n# (19.91%) of the data.\n\n# Solutions for 3\n# histogram plot\nhist(air_quality_data$PM10_est, \n    breaks = classes_PM10, main = \"Histogram: PM10 in Barcelona\", \n    xlab = \"PM10 estimates [μm]\")",
    "crumbs": [
      "Core Content",
      "Week 3: Examining Data I"
    ]
  },
  {
    "objectID": "07-sourcing_data_download.html",
    "href": "07-sourcing_data_download.html",
    "title": "Week 6: Sourcing Data I",
    "section": "",
    "text": "In many real-world research projects (including environmental epidemiology, quantitative criminology, or disaster risk reduction sciences), you will not start with a clean ready-to-use dataset. Instead, you will need to locate, download, inspect, clean, and then analyse the data particularly if your project is full-on quantitative or mixed methods. This tutorial uses waste related data from the London DATASTORE as working example to show you the typical workflow from sourcing, preparing and analysis open datasets.\n\n\nBy the end of this tutorial, students will be able to:\n\nTaking advantage of open-data sources.\nHow to perform a download (directly from website) and import the raw data into R using base functions i.e., download.file() utilising the data’s Uniform Resource Locator (URL).\nShow one instance of doing a descriptive analysis on groups using the tapply() function.\n\n\n\n\n\n\n\nWarning\n\n\n\nInstructions\nThis time around, no datasets are shared as we are going to download them in a streamlined way through RStudio as a demonstration. Therefore, in your computer, do the following:\n\nGo to the folder named GEOG0186.\nNext, create a new sub-folder within GEOG0186 and rename it as Week 6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\n\n\n\n\n\n\n\nThe London DATASTORE is an open data-sharing platform created and maintained by the Greater London Authority (GLA) to make data about London freely available to everyone — researchers, policymakers, journalists, businesses, and the public. This initiative was launched somewhere around 2010 as one of the first open data initiatives by a major city government to: 1.) promote transparency in how London is governed; 2.) encourage innovation by allowing developers and analysts to build tools and insights using public data; and 3.) support evidence-based policy and research on London’s economy, environment, transport, housing, health, and more.\nThe London Datastore hosts thousands of cleaned datasets from across London’s public sector, including:\n\nDemographics: population estimates, ethnicity, age structure, migration.\nEconomy: employment, business activity, income, inflation.\nEnvironment: air quality, carbon emissions, green spaces.\nHousing: affordability, homelessness, planning permissions.\nTransport: cycling counts, traffic volumes, public transport usage.\nHealth: life expectancy, hospital admissions, wellbeing indicators.\nCrime and Safety: police-recorded offences, fire brigade incidents.\n\nDatasets come directly from the GLA, Transport for London (TfL), Metropolitan Police, NHS, ONS, and other partners.\nWe are going to download a series of datasets on waste related issues such as recycling, waste reuse centers and fly-tipping in order to establish a workflows.\nSo, let’s begin!\n\n\nWe start by setting the work directory to the appropriate folder location in our computer. In this step, we setting the directory to the folder called Week 6, so when we start the bulk downloads - the datasets are automatically sent to that folder and not in the Downloads location.\nAt this point, you should be familiar with setting up your working directory with the setwd() function.\nFor Windows:\n\nsetwd(\"C:/Users/accountName/Desktop/GEOG0186/Week 6\")\n\nFor Macs:\n\nsetwd(\"/Users/accountName/Desktop/GEOG0186/Week 6\")\n\n\n\n\nWe are going to download the following datasets from the website:\n\nHousehold Waste Recycling Rates, Borough\nWaste Re-use Centres\nFly-tipping Incidents\n\n**Here are the steps for navigating through the webpage to these datasets:\nOn the home page CLICK HERE click on the [Data] tab at the top-left corner of the webpage.\n\n\n\n\n\n\n\n\n\nThis should show the list of all available datasets at our disposal that are freely available to download. Let search of the above datasets by typing in the word waste in the search bar:\n\n\n\n\n\n\n\n\n\nIt will produce a list of all waste-related datasets. The one, we are going for, is the dataset titled: [Household Waste Recycling Rates, Borough], which contains information proportion of household waste that are recycled or composted. Click on it.\n\n\n\n\n\n\n\n\n\nIt will show the META data behind this file. If you scroll down, you will see two versions of the file: 1.) An EXCEL (.xls) file; or 2.) A Comma Separated Value (.csv) file.\nWe want CSV (.csv) file. Now, you can straight-up download the file by simply clicking on the [Download] button… but we are going to do the downloads and coded fashion. What we are going to do is extract the URL (aka weblink) associated with the CSV dataset that we want to download.\nWe can do this by Right-clicking on the [Download] button for the CSV version, and then select [Copy Link] in the drop down menu - this copied piece of information is what we are going to feed into RStudio.\n\n\n\n\n\n\n\n\n\nNow, let us go into RStudio!\n\n\n\nThe function download.file() can be used to download a single file as described by URL (i.e., the link of the dataset) from the internet and store it in destination set by our work directory. Please, the url which we hve copied must start with a scheme such as http://, https:// or file:// in order for it to work!\nLet’s paste link and create it as an object:\n\n# assign the link to a character object\nURL_london_recycling &lt;- \"https://data.london.gov.uk/download/vd67o/15ddc38a-0a37-4f69-98b5-e69e549b39d3/Household-rcycling-borough.csv\"\n\nWe are going to use download.file() to download the dataset and save it with the file name London_households_recycling_borough.csv\n\n# ask RStudio to download the CSV file to your working directory\ndownload.file(URL_london_recycling, \n    destfile = \"London_households_recycling_borough.csv\", \n    mode = \"wb\")\n\nIf you check in your work directory folder - you see that the dataset has been downloaded and automatically save there.\n\n\n\n\n\n\n\n\n\nYou can open the dataset and its all in order:\n\n# use read.csv() function to open it\nrecycling_data &lt;- read.csv(\"London_households_recycling_borough.csv\")\n# inspection of data\n# show structure\nstr(recycling_data)\n\n'data.frame':   903 obs. of  4 variables:\n $ Code           : chr  \"E09000001\" \"E09000002\" \"E09000003\" \"E09000004\" ...\n $ Area           : chr  \"City of London\" \"Barking and Dagenham\" \"Barnet\" \"Bexley\" ...\n $ Year           : chr  \"2003/04\" \"2003/04\" \"2003/04\" \"2003/04\" ...\n $ Recycling_Rates: int  8 5 16 21 10 20 19 14 15 16 ...\n\n# show the header\nhead(recycling_data, n = 5)\n\n       Code                 Area    Year Recycling_Rates\n1 E09000001       City of London 2003/04               8\n2 E09000002 Barking and Dagenham 2003/04               5\n3 E09000003               Barnet 2003/04              16\n4 E09000004               Bexley 2003/04              21\n5 E09000005                Brent 2003/04              10\n\n\n\n\n\nSuppose you want to download and open the dataset straight away in RStudio. That is a straightforward case — since the link of the file is a direct CSV download link, you can read it straight into RStudio in a single line of code using the read.csv().\n\n# read directly from URL\nrecycling_data_2 &lt;- read.csv(\"https://data.london.gov.uk/download/vd67o/15ddc38a-0a37-4f69-98b5-e69e549b39d3/Household-rcycling-borough.csv\")\n\n# inspection of data\n# show structure\nstr(recycling_data_2)\n\n'data.frame':   903 obs. of  4 variables:\n $ Code           : chr  \"E09000001\" \"E09000002\" \"E09000003\" \"E09000004\" ...\n $ Area           : chr  \"City of London\" \"Barking and Dagenham\" \"Barnet\" \"Bexley\" ...\n $ Year           : chr  \"2003/04\" \"2003/04\" \"2003/04\" \"2003/04\" ...\n $ Recycling_Rates: int  8 5 16 21 10 20 19 14 15 16 ...\n\n# show the header\nhead(recycling_data_2)\n\n       Code                 Area    Year Recycling_Rates\n1 E09000001       City of London 2003/04               8\n2 E09000002 Barking and Dagenham 2003/04               5\n3 E09000003               Barnet 2003/04              16\n4 E09000004               Bexley 2003/04              21\n5 E09000005                Brent 2003/04              10\n6 E09000006              Bromley 2003/04              20\n\n\nWe just simply inserted the link with CSV file name into read.csv() function.\n\n\n\n\ntapply() stands for “table apply” — it lets you apply a function (like mean(), sum(), or length()) to subgroups of a vector, defined by one or more grouping factors. Meaning that, it let’s us calculate summaries by groups - for instance, we can calculate mean recycling rates by the Borough in London to see which one is doing well as a whole in terms of household waste management.\nThis calculation is simple:\n\navg_recycling_rates &lt;- tapply(recycling_data$Recycling_Rates, recycling_data$Area, mean, na.rm = TRUE)\navg_recycling_rates\n\n    Barking and Dagenham                   Barnet                   Bexley \n                24.28571                 31.14286                 47.09524 \n                   Brent                  Bromley                   Camden \n                30.71429                 42.42857                 28.00000 \n          City of London                  Croydon                   Ealing \n                29.61905                 33.90476                 38.90476 \n                    East            East Midlands                  Enfield \n                44.28571                 41.23810                 32.04762 \n                 England                Greenwich                  Hackney \n                38.71429                 31.95238                 23.33333 \n  Hammersmith and Fulham                 Haringey                   Harrow \n                24.42857                 27.61905                 37.66667 \n                Havering               Hillingdon                 Hounslow \n                30.47619                 37.90476                 30.04762 \n               Islington   Kensington and Chelsea     Kingston upon Thames \n                27.14286                 25.42857                 40.47619 \n                 Lambeth                 Lewisham                   London \n                26.80952                 19.71429                 29.80952 \n                  Merton                   Newham               North East \n                34.14286                 16.76190                 31.66667 \n              North West                Redbridge     Richmond upon Thames \n                39.23810                 25.47619                 39.14286 \n              South East               South West                Southwark \n                40.80952                 43.66667                 27.28571 \n                  Sutton            Tower Hamlets           Waltham Forest \n                37.85714                 20.28571                 29.28571 \n              Wandsworth            West Midlands              Westminster \n                22.85714                 36.66667                 20.61905 \nYorkshire and the Humber \n                37.19048 \n\n\n\n\n\nHave ago at using the download.file() function for saving and importing the remaining two datasets in RStudio\n\n\nWaste Re-use Centres\nFly-tipping Incidents\n\n\n\n\n\n\n\nImportant\n\n\n\nHINTS\n\nGo to the website: London DATASTORE\nNavigate through the website. You can use the search bar search for the desired dataset.\nRemember to extract the URL link to dataset by right-clicking the [Download] button and selecting [Copy link]\nUse CSV version of the datasets.\nUse the read.csv() to import the downloaded datasets into RStudio: 1.) name the fly-tipping records as flytipping_data; 2.) name the waste re-use centre records as waste_reuse_data.",
    "crumbs": [
      "Core Content",
      "Week 6: Sourcing Data I"
    ]
  },
  {
    "objectID": "07-sourcing_data_download.html#learning-outcomes",
    "href": "07-sourcing_data_download.html#learning-outcomes",
    "title": "Week 6: Sourcing Data I",
    "section": "",
    "text": "By the end of this tutorial, students will be able to:\n\nTaking advantage of open-data sources.\nHow to perform a download (directly from website) and import the raw data into R using base functions i.e., download.file() utilising the data’s Uniform Resource Locator (URL).\nShow one instance of doing a descriptive analysis on groups using the tapply() function.\n\n\n\n\n\n\n\nWarning\n\n\n\nInstructions\nThis time around, no datasets are shared as we are going to download them in a streamlined way through RStudio as a demonstration. Therefore, in your computer, do the following:\n\nGo to the folder named GEOG0186.\nNext, create a new sub-folder within GEOG0186 and rename it as Week 6.",
    "crumbs": [
      "Core Content",
      "Week 6: Sourcing Data I"
    ]
  },
  {
    "objectID": "07-sourcing_data_download.html#what-is-the-london-datastore-length-002841",
    "href": "07-sourcing_data_download.html#what-is-the-london-datastore-length-002841",
    "title": "Week 6: Sourcing Data I",
    "section": "",
    "text": "[Watch on YouTube]\n\n\n\n\n\n\n\n\n\nThe London DATASTORE is an open data-sharing platform created and maintained by the Greater London Authority (GLA) to make data about London freely available to everyone — researchers, policymakers, journalists, businesses, and the public. This initiative was launched somewhere around 2010 as one of the first open data initiatives by a major city government to: 1.) promote transparency in how London is governed; 2.) encourage innovation by allowing developers and analysts to build tools and insights using public data; and 3.) support evidence-based policy and research on London’s economy, environment, transport, housing, health, and more.\nThe London Datastore hosts thousands of cleaned datasets from across London’s public sector, including:\n\nDemographics: population estimates, ethnicity, age structure, migration.\nEconomy: employment, business activity, income, inflation.\nEnvironment: air quality, carbon emissions, green spaces.\nHousing: affordability, homelessness, planning permissions.\nTransport: cycling counts, traffic volumes, public transport usage.\nHealth: life expectancy, hospital admissions, wellbeing indicators.\nCrime and Safety: police-recorded offences, fire brigade incidents.\n\nDatasets come directly from the GLA, Transport for London (TfL), Metropolitan Police, NHS, ONS, and other partners.\nWe are going to download a series of datasets on waste related issues such as recycling, waste reuse centers and fly-tipping in order to establish a workflows.\nSo, let’s begin!\n\n\nWe start by setting the work directory to the appropriate folder location in our computer. In this step, we setting the directory to the folder called Week 6, so when we start the bulk downloads - the datasets are automatically sent to that folder and not in the Downloads location.\nAt this point, you should be familiar with setting up your working directory with the setwd() function.\nFor Windows:\n\nsetwd(\"C:/Users/accountName/Desktop/GEOG0186/Week 6\")\n\nFor Macs:\n\nsetwd(\"/Users/accountName/Desktop/GEOG0186/Week 6\")\n\n\n\n\nWe are going to download the following datasets from the website:\n\nHousehold Waste Recycling Rates, Borough\nWaste Re-use Centres\nFly-tipping Incidents\n\n**Here are the steps for navigating through the webpage to these datasets:\nOn the home page CLICK HERE click on the [Data] tab at the top-left corner of the webpage.\n\n\n\n\n\n\n\n\n\nThis should show the list of all available datasets at our disposal that are freely available to download. Let search of the above datasets by typing in the word waste in the search bar:\n\n\n\n\n\n\n\n\n\nIt will produce a list of all waste-related datasets. The one, we are going for, is the dataset titled: [Household Waste Recycling Rates, Borough], which contains information proportion of household waste that are recycled or composted. Click on it.\n\n\n\n\n\n\n\n\n\nIt will show the META data behind this file. If you scroll down, you will see two versions of the file: 1.) An EXCEL (.xls) file; or 2.) A Comma Separated Value (.csv) file.\nWe want CSV (.csv) file. Now, you can straight-up download the file by simply clicking on the [Download] button… but we are going to do the downloads and coded fashion. What we are going to do is extract the URL (aka weblink) associated with the CSV dataset that we want to download.\nWe can do this by Right-clicking on the [Download] button for the CSV version, and then select [Copy Link] in the drop down menu - this copied piece of information is what we are going to feed into RStudio.\n\n\n\n\n\n\n\n\n\nNow, let us go into RStudio!\n\n\n\nThe function download.file() can be used to download a single file as described by URL (i.e., the link of the dataset) from the internet and store it in destination set by our work directory. Please, the url which we hve copied must start with a scheme such as http://, https:// or file:// in order for it to work!\nLet’s paste link and create it as an object:\n\n# assign the link to a character object\nURL_london_recycling &lt;- \"https://data.london.gov.uk/download/vd67o/15ddc38a-0a37-4f69-98b5-e69e549b39d3/Household-rcycling-borough.csv\"\n\nWe are going to use download.file() to download the dataset and save it with the file name London_households_recycling_borough.csv\n\n# ask RStudio to download the CSV file to your working directory\ndownload.file(URL_london_recycling, \n    destfile = \"London_households_recycling_borough.csv\", \n    mode = \"wb\")\n\nIf you check in your work directory folder - you see that the dataset has been downloaded and automatically save there.\n\n\n\n\n\n\n\n\n\nYou can open the dataset and its all in order:\n\n# use read.csv() function to open it\nrecycling_data &lt;- read.csv(\"London_households_recycling_borough.csv\")\n# inspection of data\n# show structure\nstr(recycling_data)\n\n'data.frame':   903 obs. of  4 variables:\n $ Code           : chr  \"E09000001\" \"E09000002\" \"E09000003\" \"E09000004\" ...\n $ Area           : chr  \"City of London\" \"Barking and Dagenham\" \"Barnet\" \"Bexley\" ...\n $ Year           : chr  \"2003/04\" \"2003/04\" \"2003/04\" \"2003/04\" ...\n $ Recycling_Rates: int  8 5 16 21 10 20 19 14 15 16 ...\n\n# show the header\nhead(recycling_data, n = 5)\n\n       Code                 Area    Year Recycling_Rates\n1 E09000001       City of London 2003/04               8\n2 E09000002 Barking and Dagenham 2003/04               5\n3 E09000003               Barnet 2003/04              16\n4 E09000004               Bexley 2003/04              21\n5 E09000005                Brent 2003/04              10\n\n\n\n\n\nSuppose you want to download and open the dataset straight away in RStudio. That is a straightforward case — since the link of the file is a direct CSV download link, you can read it straight into RStudio in a single line of code using the read.csv().\n\n# read directly from URL\nrecycling_data_2 &lt;- read.csv(\"https://data.london.gov.uk/download/vd67o/15ddc38a-0a37-4f69-98b5-e69e549b39d3/Household-rcycling-borough.csv\")\n\n# inspection of data\n# show structure\nstr(recycling_data_2)\n\n'data.frame':   903 obs. of  4 variables:\n $ Code           : chr  \"E09000001\" \"E09000002\" \"E09000003\" \"E09000004\" ...\n $ Area           : chr  \"City of London\" \"Barking and Dagenham\" \"Barnet\" \"Bexley\" ...\n $ Year           : chr  \"2003/04\" \"2003/04\" \"2003/04\" \"2003/04\" ...\n $ Recycling_Rates: int  8 5 16 21 10 20 19 14 15 16 ...\n\n# show the header\nhead(recycling_data_2)\n\n       Code                 Area    Year Recycling_Rates\n1 E09000001       City of London 2003/04               8\n2 E09000002 Barking and Dagenham 2003/04               5\n3 E09000003               Barnet 2003/04              16\n4 E09000004               Bexley 2003/04              21\n5 E09000005                Brent 2003/04              10\n6 E09000006              Bromley 2003/04              20\n\n\nWe just simply inserted the link with CSV file name into read.csv() function.",
    "crumbs": [
      "Core Content",
      "Week 6: Sourcing Data I"
    ]
  },
  {
    "objectID": "07-sourcing_data_download.html#tapply-for-group-analysis",
    "href": "07-sourcing_data_download.html#tapply-for-group-analysis",
    "title": "Week 6: Sourcing Data I",
    "section": "",
    "text": "tapply() stands for “table apply” — it lets you apply a function (like mean(), sum(), or length()) to subgroups of a vector, defined by one or more grouping factors. Meaning that, it let’s us calculate summaries by groups - for instance, we can calculate mean recycling rates by the Borough in London to see which one is doing well as a whole in terms of household waste management.\nThis calculation is simple:\n\navg_recycling_rates &lt;- tapply(recycling_data$Recycling_Rates, recycling_data$Area, mean, na.rm = TRUE)\navg_recycling_rates\n\n    Barking and Dagenham                   Barnet                   Bexley \n                24.28571                 31.14286                 47.09524 \n                   Brent                  Bromley                   Camden \n                30.71429                 42.42857                 28.00000 \n          City of London                  Croydon                   Ealing \n                29.61905                 33.90476                 38.90476 \n                    East            East Midlands                  Enfield \n                44.28571                 41.23810                 32.04762 \n                 England                Greenwich                  Hackney \n                38.71429                 31.95238                 23.33333 \n  Hammersmith and Fulham                 Haringey                   Harrow \n                24.42857                 27.61905                 37.66667 \n                Havering               Hillingdon                 Hounslow \n                30.47619                 37.90476                 30.04762 \n               Islington   Kensington and Chelsea     Kingston upon Thames \n                27.14286                 25.42857                 40.47619 \n                 Lambeth                 Lewisham                   London \n                26.80952                 19.71429                 29.80952 \n                  Merton                   Newham               North East \n                34.14286                 16.76190                 31.66667 \n              North West                Redbridge     Richmond upon Thames \n                39.23810                 25.47619                 39.14286 \n              South East               South West                Southwark \n                40.80952                 43.66667                 27.28571 \n                  Sutton            Tower Hamlets           Waltham Forest \n                37.85714                 20.28571                 29.28571 \n              Wandsworth            West Midlands              Westminster \n                22.85714                 36.66667                 20.61905 \nYorkshire and the Humber \n                37.19048 \n\n\n\n\n\nHave ago at using the download.file() function for saving and importing the remaining two datasets in RStudio\n\n\nWaste Re-use Centres\nFly-tipping Incidents\n\n\n\n\n\n\n\nImportant\n\n\n\nHINTS\n\nGo to the website: London DATASTORE\nNavigate through the website. You can use the search bar search for the desired dataset.\nRemember to extract the URL link to dataset by right-clicking the [Download] button and selecting [Copy link]\nUse CSV version of the datasets.\nUse the read.csv() to import the downloaded datasets into RStudio: 1.) name the fly-tipping records as flytipping_data; 2.) name the waste re-use centre records as waste_reuse_data.",
    "crumbs": [
      "Core Content",
      "Week 6: Sourcing Data I"
    ]
  },
  {
    "objectID": "06-data_visualisation.html",
    "href": "06-data_visualisation.html",
    "title": "Week 5: Examining Data III",
    "section": "",
    "text": "Data visualisation helps geographers see patterns, trends, and relationships that are not immediately clear from tables or numbers. For instance, plotting rainfall against temperature might show a seasonal pattern, or visualising air pollution across cities could reveal urban–rural contrasts.\nIn R, data visualisation can be done using different libraries (e.g., ggplot2, tmap and many more), but Base R offers a powerful and flexible way to start — and it helps you understand what’s happening behind the scenes. The plot() function is the core tool for making quick, informative graphics in R.\nBy the end of this tutorial, you will be able to:\n\nConstruct a plot from scratch using the plot() function. We will focus a lot on coding a scatterplot.\nUnderstand the syntax and arguments (i.e., expression(), xlab, ylab, main, xlim, ylim and many more) for fully customising a graphical plot\nHow to add elements to an existing plot which includes additional functions like major and minor ticks to the x-y axis using the axis(), as well as control the boundary of the plot using box(), and incorporate a legend block in graph using the legend() function to distinguish between groups.\nBase R offers simple tools for visualising distributions - examples include hist() and boxplot() which we had already covered. For the box plots, we will learn how to visualise multiple categories.\n\n\n\n\n\n\n\nWarning\n\n\n\nWe will be using the following datasets, Barcelona_Air_Pollution_DayNight-Time_data.csv, for this week’s tutorial. You can download it from [HERE].\nInstructions In your computer, do the following:\n\nGo to the folder named GEOG0186 - this should have already been created in Week 2\nNext, create a new sub-folder within GEOG0186 and rename it as Week 5.\nFrom the downloaded folder Dataset for Week 5, make sure to unzip and transfer ALL the datasets directly to the Week 5 folder.\n\n\n\n\n\n\nLast week, we learned how to summarise data using descriptive statistics such as measures of central tendency and variability. Now, we are going to create various plots to examine the relationship between nitrogen dioxide (NO₂) and particulate matter (PM₁₀) in Barcelona. Doing this type of analysis helps us visually explore the association between two air pollutants, identify possible trends or clusters, and better understand the patterns of air quality in the city.\nThe emphasis is building a plot from scratch and fully customising it. Alright, let’s begin!\n\n\nLet us import for following dataset Barcelona_Air_Pollution_DayNight-Time_data.csv into RStudio, and call this object air_pollution_daily_data.\nSetting the work directory should be second nature to you now at the point. Remember - always make sure that your work directory is linked to your folder containing your data.\nFor Windows:\n\nsetwd(\"C:/Users/accountName/Desktop/GEOG0186/Week 5\")\n\nFor Macs:\n\nsetwd(\"/Users/accountName/Desktop/GEOG0186/Week 5\")\n\nNow, import you the data set as follows:\n\nair_pollution_daily_data &lt;- read.csv(\"Barcelona_Air_Pollution_DayNight-Time_data.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nImported data contains daily measures (averaged) for Nitrogen Oxide and Particular Matter (10) for November from the 1s, up to the 30th, in 2018. The averaged measures for both air contaminants are categorised by time of observation i.e., Day-Time or Night-Time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\nWe begin by visualising the data using the plot() function without any customisation. This shows the basic relationship between daily average NO₂ and PM₁₀ concentrations in a form of scatterplot.\n\n# Barebones scatterplot\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\nplot(x, y) draws a default scatterplot of two numeric vectors with NO2_Daily_Average drawn on the x-axis and PM10_Daily_Average drawn on the y-axis.\nX-axis is NO2_Daily_Average; and Y-axis is PM10_Daily_Average.\nDefault labels (i.e., variable names) as code and symbols are used and printed on the output.\nBy default, no title is printed on the output.\n\n\n\n\nNow we include the x-axis label, y-axis label, and an informative title using xlab, ylab, and main.\n\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = \"Daily Average NO2 (ppb)\", \n     ylab = \"Daily Average PM10 (μg/m³)\",\n     main = \"Scatterplot: Examining the impacts of NO2 on PM10 in Barcelona\"\n)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\nxlab and ylab arguments in the plot()function sets human-readable axis titles (units included). Note that it still plain text (NO₂ and PM₁₀ are not yet typeset chemically with the appropriate subscript!).\nmain adds a descriptive title so the plot is self-contained.\n\n\n\n\nFor scientific notation (e.g., NO₂ and PM₁₀), we can use the expression() function wrapped around the whole text, and then apply the subscript notation [ ]* to the piece of text we want to render as a subscript.\n\n# Adding subscripts to axis labels\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(\"Daily Average NO\"[2]*\" (ppb)\"), \n     ylab = expression(\"Daily Average PM\"[10]*\" (μg/m³)\"),\n     main = expression(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")\n)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\nexpression() lets R render block of text - particularly useful for maths notation.\nUse the square brackets [ ] on text/number to make it a subscript: e.g., \"NO\"[2]\" -&gt; NO₂.\n* concatenates text/math parts inside expression() (no space added unless in quotes).\n\n\n\n\nWhat about making the labels boldface? You can wrap the text labeling with the bold() function inside the expression() code. For example:\n\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\"))\n)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\nbold(...) (inside expression) renders that text in bold. You can use for titles/labels you want to stand out in reports or slides.\nThere are other functions like bolditalic() which you can experiment with in your own time.\nAn important note - bold() or bolditalic() can only work inside expression() function.\n\n\n\n\nYou can use the following arguments - ylim and xlim to control the y- and x-axis, respectively. Before setting axis limits, check the data range with summary() function to have an idea of the minimum and maximum value to inform how you will do the set-up for this.\n\nsummary(air_pollution_daily_data$NO2_Daily_Average)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  47.83   56.50   60.01   59.69   62.69   75.67 \n\nsummary(air_pollution_daily_data$PM10_Daily_Average)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.17   30.46   31.83   31.81   33.96   37.33 \n\n\nBased on the printed output - we will set the limits of our y-axis to be between 20 to 40 for the NO₂ and PM₁₀; while for NO₂, it can be set between 40 to 80 for the x-axis for ylim and xlim arguments in the plot() function, respectively.\n\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80)\n)\n\n\n\n\n\n\n\n\n\n\n\nYou can use bty argument in the plot() function to change which outer sides of the box/frame are drawn. The following options are:\n\n\n\nOption\nFrame drawn\n\n\n\n\n\"o\"\ncomplete box (default)\n\n\n\"n\"\nno box\n\n\n\"7\"\ntop + right\n\n\n\"L\"\nbottom + left (recommended for clean visuals)\n\n\n\nTrust me on this… just pick \"L\" always! Anything else makes the plot look funky!\n\n# add \"bty=...\" to control the outer frame/box type of plot\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\"\n)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\nbty = \"L\" draws a minimalist L-shaped frame (bottom + left), which is a clean, modern look that reduces clutter.\n\n\n\n\nThe pch= argument in plot() controls the symbology of points in the scatterplot. It contains up to 25 different symbols which are selectable from 1-25 (see above image). Here, we are choosing the 25th symbol which is a triangle.\n\n# Controlling the symbol type\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")),\n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\",\n     pch = 25\n)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\npch chooses the plotting character (symbol).\nTip: Shapes 21–25 can be filled; include the argument bg= to set the fill colour of shape and col= for the outline.\n\n\n\n\nThe las argument in the plot() function controls the overall orientation of the numbers labelled on the ticks of BOTH axes.\n\n\n\nValue\nOrientation\n\n\n\n\n0\nparallel to the axis\n\n\n1\nhorizontal\n\n\n2\nperpendicular\n\n\n3\nvertical\n\n\n\nAt the moment, the orientation is parallel i.e., the numbers on the y-axis are parallel to the axis itself. We want it to be horizontal akin to how the values are already oriented on the x-axis.\n\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\",\n     pch = 25,\n     las = 1\n)\n\n\n\n\n\n\n\n\n\n\n\nYou can control the overall colour scheme for points, titles, and axis labels with col, col.main and col.lab, respectively:\n\n# add colour to pooints\n# add colour to main title\n# add colour to axis title\n\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\",\n     pch = 25,\n     las = 1,\n     col = \"darkolivegreen\",\n     col.main = \"darkolivegreen\",\n     col.lab = \"darkolivegreen\"\n)\n\n\n\n\n\n\n\n\n\n\n\nHere, we want to add major and minor ticks to the graph for it to look professional. We remove the default axes and redraw them manually with both major and minor ticks using the axis(). This is a function that works outside of plot().\nThere is extreme flexibility in how you can code a plot up!\n\n# Remove the default axes for full control by adding `axes = FALSE` in the `plot()`\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\",\n     pch = 25,\n     las = 1,\n     col = \"darkolivegreen\",\n     col.main = \"darkolivegreen\",\n     col.lab = \"darkolivegreen\",\n     axes = FALSE\n)\n\n# Here, we have full control of the x-axis - add the major ticks\naxis(1, at = seq(40, 80, by = 10), labels = seq(40, 80, by = 10), tcl = -0.7)\n# Here, we have full control of the y-axis - add the major ticks\naxis(2, at = seq(20, 40, by = 5), labels = seq(20, 40, by = 5), las = 1, tcl = -0.7)\n\n# Again, full control of the x-axis - add the minor ticks\naxis(1, at = seq(40, 80, by = 1), labels = FALSE, tcl = -0.3)\n# Again, full control of the y-axis - add the minor ticks\naxis(2, at = seq(20, 40, by = 1), labels = FALSE, tcl = -0.3)\n\n# Add \"L\" box back\nbox(bty = \"L\")\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\naxes = FALSE suppresses the default axes so you can rebuild them.\naxis(1, ...), axis(2, ...) draw the bottom (x) and left (y) axes, respectively.\nat sets tick positions, where we draw a sequence of numbers starting from 40 to 80 at intervals of 10 on the x-axis using the seq(). A similar operation is carried out on y-axis.\nlabels controls tick label text where we apply the sequence on numbers to break axis accordingly with the ticks;\ntcl sets tick length (negative points outward).\nUnlike the Major ticks, the Minor ticks have labels = FALSE where we don’t apply the numbers to avoid clutter.\nbox(bty = \"L\") simply redraws the minimalist “L-shaped” frame.\n\n\n\n\nWe can now distinguish day-time and night-time observations using colour and symbol differences. This will be involve some simple brute force coding to define the list of categories and map them to a symbol and colour:\n\n# Define categories for legend\ncategories &lt;- c(\"Day-time\", \"Night-time\")\n\n# Map categories to colours and shapes automatically\ncolours &lt;- c(\"lightblue\", \"grey\")[as.factor(air_pollution_daily_data$DayType)]\nshapes &lt;- c(25, 17)[as.factor(air_pollution_daily_data$DayType)]\n\nHere, we converting the list of categorical string labels into a factor as 1 = \"Day-Time\" and 2 = \"Night-Time\", and then map the colour scheme of lightblue to 1 and grey to 2. We are also mapping the symbology of 25 to 1, and 17 to 2.\nWe insert these objects into the plot() for pch, bg and col to show the distinction between points based on category.\nLastly, we add the legend() outside of the plot() function.\n\n# Final customised plot\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\",\n     pch = shapes,\n     bg = colours,\n     col = colours,\n     las = 1,\n     col.main = \"darkolivegreen\",\n     col.lab = \"darkolivegreen\",\n     axes = FALSE\n)\n\n# Add Major ticks\naxis(1, at = seq(40, 80, by = 10), labels = seq(40, 80, by = 10), tcl = -0.7)\naxis(2, at = seq(20, 40, by = 5), labels = seq(20, 40, by = 5), las = 1, tcl = -0.7)\n\n# Add Minor ticks\naxis(1, at = seq(40, 80, by = 1), labels = FALSE, tcl = -0.3)\naxis(2, at = seq(20, 40, by = 1), labels = FALSE, tcl = -0.3)\n\n# Add box back\nbox(bty = \"L\")\n\n# Add legend\nlegend(\"topright\", legend = categories, bty = \"n\",\n       pch = c(25, 17), \n       col = c(\"lightblue\", \"grey\"),\n       pt.bg = c(\"lightblue\", \"grey\"),\n       title = expression(bold(\"Observation Type\")))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nINTERPRETATION: The scatterplot shows the relationship between the two air pollution contaminants. By ‘eye-balling’ this, it seems that increasing levels of Nitrogen Dioxide increases the particular matter (10) as well. It is difficult to say however, without performing an evidence-based test like Correlation or a Linear Regression - which is a story for another day (i.e., Week 8).\n\n\nThat’s how you build a graph! You have now been exposed to making a boxplot(), hist() and scatterplot through plot(). Let’s do a final stretch with the boxplots - when dealing with multiple categories!\n\n\n\n\nSuppose we want to visual the distribution of these contaminants broken down by day/night-time. Instead of plotting them separately, you can use the ~ sign to achieve the breakdown.\nBox plot for NO2:\n\nboxplot(air_pollution_daily_data$NO2_Daily_Average ~ air_pollution_daily_data$DayType,\n    col = c(\"lightblue\", \"grey\"),\n    main = expression(bold(\"NO\"[2]*\" Concentrations\")),\n    ylab = expression(bold(\"Concentration (μg/m³)\")),\n    xlab = \"\"\n    )\n\n\n\n\n\n\n\n\nBox plot for PM10:\n\nboxplot(air_pollution_daily_data$PM10_Daily_Average ~ air_pollution_daily_data$DayType,\n    col = c(\"lightblue\", \"grey\"),\n    main = expression(bold(\"PM\"[10]*\" Concentrations\")),\n    ylab = expression(bold(\"Concentration (μg/m³)\")),\n    xlab = \"\"\n    )\n\n\n\n\n\n\n\n\nIf you want to visual the pair of images side-by-side, you can use the par(mfrow = c(1,2)) code creates a grid with 2 columns to place each boxplot inside:\n\npar(mfrow = c(1, 2))\n\nboxplot(air_pollution_daily_data$NO2_Daily_Average ~ air_pollution_daily_data$DayType,\n    col = c(\"lightblue\", \"grey\"),\n    main = expression(bold(\"NO\"[2]*\" Concentrations\")),\n    ylab = expression(bold(\"Concentration (μg/m³)\")),\n    xlab = \"\"\n)\n\nboxplot(air_pollution_daily_data$PM10_Daily_Average ~ air_pollution_daily_data$DayType,\n    col = c(\"lightblue\", \"grey\"),\n    main = expression(bold(\"PM\"[10]*\" Concentrations\")),\n    ylab = expression(bold(\"Concentration (μg/m³)\")),\n    xlab = \"\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen using this code, par(mfrow = c(1, 2)). Your plot window is split into 2 grids, and that setting will remain like that until explicitly tell RStudio remove it. You can overide this setting by running the following code:\n\n# reset plot layout\npar(mfrow = c(1, 1))",
    "crumbs": [
      "Core Content",
      "Week 5: Examining Data III"
    ]
  },
  {
    "objectID": "06-data_visualisation.html#learning-outcomes",
    "href": "06-data_visualisation.html#learning-outcomes",
    "title": "Week 5: Examining Data III",
    "section": "",
    "text": "Data visualisation helps geographers see patterns, trends, and relationships that are not immediately clear from tables or numbers. For instance, plotting rainfall against temperature might show a seasonal pattern, or visualising air pollution across cities could reveal urban–rural contrasts.\nIn R, data visualisation can be done using different libraries (e.g., ggplot2, tmap and many more), but Base R offers a powerful and flexible way to start — and it helps you understand what’s happening behind the scenes. The plot() function is the core tool for making quick, informative graphics in R.\nBy the end of this tutorial, you will be able to:\n\nConstruct a plot from scratch using the plot() function. We will focus a lot on coding a scatterplot.\nUnderstand the syntax and arguments (i.e., expression(), xlab, ylab, main, xlim, ylim and many more) for fully customising a graphical plot\nHow to add elements to an existing plot which includes additional functions like major and minor ticks to the x-y axis using the axis(), as well as control the boundary of the plot using box(), and incorporate a legend block in graph using the legend() function to distinguish between groups.\nBase R offers simple tools for visualising distributions - examples include hist() and boxplot() which we had already covered. For the box plots, we will learn how to visualise multiple categories.\n\n\n\n\n\n\n\nWarning\n\n\n\nWe will be using the following datasets, Barcelona_Air_Pollution_DayNight-Time_data.csv, for this week’s tutorial. You can download it from [HERE].\nInstructions In your computer, do the following:\n\nGo to the folder named GEOG0186 - this should have already been created in Week 2\nNext, create a new sub-folder within GEOG0186 and rename it as Week 5.\nFrom the downloaded folder Dataset for Week 5, make sure to unzip and transfer ALL the datasets directly to the Week 5 folder.",
    "crumbs": [
      "Core Content",
      "Week 5: Examining Data III"
    ]
  },
  {
    "objectID": "06-data_visualisation.html#analysing-air-pollution-data-in-barcelona-part-iii",
    "href": "06-data_visualisation.html#analysing-air-pollution-data-in-barcelona-part-iii",
    "title": "Week 5: Examining Data III",
    "section": "",
    "text": "Last week, we learned how to summarise data using descriptive statistics such as measures of central tendency and variability. Now, we are going to create various plots to examine the relationship between nitrogen dioxide (NO₂) and particulate matter (PM₁₀) in Barcelona. Doing this type of analysis helps us visually explore the association between two air pollutants, identify possible trends or clusters, and better understand the patterns of air quality in the city.\nThe emphasis is building a plot from scratch and fully customising it. Alright, let’s begin!\n\n\nLet us import for following dataset Barcelona_Air_Pollution_DayNight-Time_data.csv into RStudio, and call this object air_pollution_daily_data.\nSetting the work directory should be second nature to you now at the point. Remember - always make sure that your work directory is linked to your folder containing your data.\nFor Windows:\n\nsetwd(\"C:/Users/accountName/Desktop/GEOG0186/Week 5\")\n\nFor Macs:\n\nsetwd(\"/Users/accountName/Desktop/GEOG0186/Week 5\")\n\nNow, import you the data set as follows:\n\nair_pollution_daily_data &lt;- read.csv(\"Barcelona_Air_Pollution_DayNight-Time_data.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nImported data contains daily measures (averaged) for Nitrogen Oxide and Particular Matter (10) for November from the 1s, up to the 30th, in 2018. The averaged measures for both air contaminants are categorised by time of observation i.e., Day-Time or Night-Time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\nWe begin by visualising the data using the plot() function without any customisation. This shows the basic relationship between daily average NO₂ and PM₁₀ concentrations in a form of scatterplot.\n\n# Barebones scatterplot\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\nplot(x, y) draws a default scatterplot of two numeric vectors with NO2_Daily_Average drawn on the x-axis and PM10_Daily_Average drawn on the y-axis.\nX-axis is NO2_Daily_Average; and Y-axis is PM10_Daily_Average.\nDefault labels (i.e., variable names) as code and symbols are used and printed on the output.\nBy default, no title is printed on the output.\n\n\n\n\nNow we include the x-axis label, y-axis label, and an informative title using xlab, ylab, and main.\n\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = \"Daily Average NO2 (ppb)\", \n     ylab = \"Daily Average PM10 (μg/m³)\",\n     main = \"Scatterplot: Examining the impacts of NO2 on PM10 in Barcelona\"\n)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\nxlab and ylab arguments in the plot()function sets human-readable axis titles (units included). Note that it still plain text (NO₂ and PM₁₀ are not yet typeset chemically with the appropriate subscript!).\nmain adds a descriptive title so the plot is self-contained.\n\n\n\n\nFor scientific notation (e.g., NO₂ and PM₁₀), we can use the expression() function wrapped around the whole text, and then apply the subscript notation [ ]* to the piece of text we want to render as a subscript.\n\n# Adding subscripts to axis labels\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(\"Daily Average NO\"[2]*\" (ppb)\"), \n     ylab = expression(\"Daily Average PM\"[10]*\" (μg/m³)\"),\n     main = expression(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")\n)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\nexpression() lets R render block of text - particularly useful for maths notation.\nUse the square brackets [ ] on text/number to make it a subscript: e.g., \"NO\"[2]\" -&gt; NO₂.\n* concatenates text/math parts inside expression() (no space added unless in quotes).\n\n\n\n\nWhat about making the labels boldface? You can wrap the text labeling with the bold() function inside the expression() code. For example:\n\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\"))\n)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\nbold(...) (inside expression) renders that text in bold. You can use for titles/labels you want to stand out in reports or slides.\nThere are other functions like bolditalic() which you can experiment with in your own time.\nAn important note - bold() or bolditalic() can only work inside expression() function.\n\n\n\n\nYou can use the following arguments - ylim and xlim to control the y- and x-axis, respectively. Before setting axis limits, check the data range with summary() function to have an idea of the minimum and maximum value to inform how you will do the set-up for this.\n\nsummary(air_pollution_daily_data$NO2_Daily_Average)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  47.83   56.50   60.01   59.69   62.69   75.67 \n\nsummary(air_pollution_daily_data$PM10_Daily_Average)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.17   30.46   31.83   31.81   33.96   37.33 \n\n\nBased on the printed output - we will set the limits of our y-axis to be between 20 to 40 for the NO₂ and PM₁₀; while for NO₂, it can be set between 40 to 80 for the x-axis for ylim and xlim arguments in the plot() function, respectively.\n\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80)\n)\n\n\n\n\n\n\n\n\n\n\n\nYou can use bty argument in the plot() function to change which outer sides of the box/frame are drawn. The following options are:\n\n\n\nOption\nFrame drawn\n\n\n\n\n\"o\"\ncomplete box (default)\n\n\n\"n\"\nno box\n\n\n\"7\"\ntop + right\n\n\n\"L\"\nbottom + left (recommended for clean visuals)\n\n\n\nTrust me on this… just pick \"L\" always! Anything else makes the plot look funky!\n\n# add \"bty=...\" to control the outer frame/box type of plot\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\"\n)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\nbty = \"L\" draws a minimalist L-shaped frame (bottom + left), which is a clean, modern look that reduces clutter.\n\n\n\n\nThe pch= argument in plot() controls the symbology of points in the scatterplot. It contains up to 25 different symbols which are selectable from 1-25 (see above image). Here, we are choosing the 25th symbol which is a triangle.\n\n# Controlling the symbol type\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")),\n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\",\n     pch = 25\n)\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\npch chooses the plotting character (symbol).\nTip: Shapes 21–25 can be filled; include the argument bg= to set the fill colour of shape and col= for the outline.\n\n\n\n\nThe las argument in the plot() function controls the overall orientation of the numbers labelled on the ticks of BOTH axes.\n\n\n\nValue\nOrientation\n\n\n\n\n0\nparallel to the axis\n\n\n1\nhorizontal\n\n\n2\nperpendicular\n\n\n3\nvertical\n\n\n\nAt the moment, the orientation is parallel i.e., the numbers on the y-axis are parallel to the axis itself. We want it to be horizontal akin to how the values are already oriented on the x-axis.\n\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\",\n     pch = 25,\n     las = 1\n)\n\n\n\n\n\n\n\n\n\n\n\nYou can control the overall colour scheme for points, titles, and axis labels with col, col.main and col.lab, respectively:\n\n# add colour to pooints\n# add colour to main title\n# add colour to axis title\n\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\",\n     pch = 25,\n     las = 1,\n     col = \"darkolivegreen\",\n     col.main = \"darkolivegreen\",\n     col.lab = \"darkolivegreen\"\n)\n\n\n\n\n\n\n\n\n\n\n\nHere, we want to add major and minor ticks to the graph for it to look professional. We remove the default axes and redraw them manually with both major and minor ticks using the axis(). This is a function that works outside of plot().\nThere is extreme flexibility in how you can code a plot up!\n\n# Remove the default axes for full control by adding `axes = FALSE` in the `plot()`\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\",\n     pch = 25,\n     las = 1,\n     col = \"darkolivegreen\",\n     col.main = \"darkolivegreen\",\n     col.lab = \"darkolivegreen\",\n     axes = FALSE\n)\n\n# Here, we have full control of the x-axis - add the major ticks\naxis(1, at = seq(40, 80, by = 10), labels = seq(40, 80, by = 10), tcl = -0.7)\n# Here, we have full control of the y-axis - add the major ticks\naxis(2, at = seq(20, 40, by = 5), labels = seq(20, 40, by = 5), las = 1, tcl = -0.7)\n\n# Again, full control of the x-axis - add the minor ticks\naxis(1, at = seq(40, 80, by = 1), labels = FALSE, tcl = -0.3)\n# Again, full control of the y-axis - add the minor ticks\naxis(2, at = seq(20, 40, by = 1), labels = FALSE, tcl = -0.3)\n\n# Add \"L\" box back\nbox(bty = \"L\")\n\n\n\n\n\n\n\n\nWhat is this code doing?\n\naxes = FALSE suppresses the default axes so you can rebuild them.\naxis(1, ...), axis(2, ...) draw the bottom (x) and left (y) axes, respectively.\nat sets tick positions, where we draw a sequence of numbers starting from 40 to 80 at intervals of 10 on the x-axis using the seq(). A similar operation is carried out on y-axis.\nlabels controls tick label text where we apply the sequence on numbers to break axis accordingly with the ticks;\ntcl sets tick length (negative points outward).\nUnlike the Major ticks, the Minor ticks have labels = FALSE where we don’t apply the numbers to avoid clutter.\nbox(bty = \"L\") simply redraws the minimalist “L-shaped” frame.\n\n\n\n\nWe can now distinguish day-time and night-time observations using colour and symbol differences. This will be involve some simple brute force coding to define the list of categories and map them to a symbol and colour:\n\n# Define categories for legend\ncategories &lt;- c(\"Day-time\", \"Night-time\")\n\n# Map categories to colours and shapes automatically\ncolours &lt;- c(\"lightblue\", \"grey\")[as.factor(air_pollution_daily_data$DayType)]\nshapes &lt;- c(25, 17)[as.factor(air_pollution_daily_data$DayType)]\n\nHere, we converting the list of categorical string labels into a factor as 1 = \"Day-Time\" and 2 = \"Night-Time\", and then map the colour scheme of lightblue to 1 and grey to 2. We are also mapping the symbology of 25 to 1, and 17 to 2.\nWe insert these objects into the plot() for pch, bg and col to show the distinction between points based on category.\nLastly, we add the legend() outside of the plot() function.\n\n# Final customised plot\nplot(air_pollution_daily_data$NO2_Daily_Average, air_pollution_daily_data$PM10_Daily_Average,\n     xlab = expression(bold(\"Daily Average NO\"[2]*\" (ppb)\")), \n     ylab = expression(bold(\"Daily Average PM\"[10]*\" (μg/m³)\")),\n     main = expression(bold(\"Scatterplot: Examining the impacts of NO\"[2]*\" on PM\"[10]*\" in Barcelona\")), \n     ylim = c(20, 40),\n     xlim = c(40, 80),\n     bty = \"L\",\n     pch = shapes,\n     bg = colours,\n     col = colours,\n     las = 1,\n     col.main = \"darkolivegreen\",\n     col.lab = \"darkolivegreen\",\n     axes = FALSE\n)\n\n# Add Major ticks\naxis(1, at = seq(40, 80, by = 10), labels = seq(40, 80, by = 10), tcl = -0.7)\naxis(2, at = seq(20, 40, by = 5), labels = seq(20, 40, by = 5), las = 1, tcl = -0.7)\n\n# Add Minor ticks\naxis(1, at = seq(40, 80, by = 1), labels = FALSE, tcl = -0.3)\naxis(2, at = seq(20, 40, by = 1), labels = FALSE, tcl = -0.3)\n\n# Add box back\nbox(bty = \"L\")\n\n# Add legend\nlegend(\"topright\", legend = categories, bty = \"n\",\n       pch = c(25, 17), \n       col = c(\"lightblue\", \"grey\"),\n       pt.bg = c(\"lightblue\", \"grey\"),\n       title = expression(bold(\"Observation Type\")))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nINTERPRETATION: The scatterplot shows the relationship between the two air pollution contaminants. By ‘eye-balling’ this, it seems that increasing levels of Nitrogen Dioxide increases the particular matter (10) as well. It is difficult to say however, without performing an evidence-based test like Correlation or a Linear Regression - which is a story for another day (i.e., Week 8).\n\n\nThat’s how you build a graph! You have now been exposed to making a boxplot(), hist() and scatterplot through plot(). Let’s do a final stretch with the boxplots - when dealing with multiple categories!\n\n\n\n\nSuppose we want to visual the distribution of these contaminants broken down by day/night-time. Instead of plotting them separately, you can use the ~ sign to achieve the breakdown.\nBox plot for NO2:\n\nboxplot(air_pollution_daily_data$NO2_Daily_Average ~ air_pollution_daily_data$DayType,\n    col = c(\"lightblue\", \"grey\"),\n    main = expression(bold(\"NO\"[2]*\" Concentrations\")),\n    ylab = expression(bold(\"Concentration (μg/m³)\")),\n    xlab = \"\"\n    )\n\n\n\n\n\n\n\n\nBox plot for PM10:\n\nboxplot(air_pollution_daily_data$PM10_Daily_Average ~ air_pollution_daily_data$DayType,\n    col = c(\"lightblue\", \"grey\"),\n    main = expression(bold(\"PM\"[10]*\" Concentrations\")),\n    ylab = expression(bold(\"Concentration (μg/m³)\")),\n    xlab = \"\"\n    )\n\n\n\n\n\n\n\n\nIf you want to visual the pair of images side-by-side, you can use the par(mfrow = c(1,2)) code creates a grid with 2 columns to place each boxplot inside:\n\npar(mfrow = c(1, 2))\n\nboxplot(air_pollution_daily_data$NO2_Daily_Average ~ air_pollution_daily_data$DayType,\n    col = c(\"lightblue\", \"grey\"),\n    main = expression(bold(\"NO\"[2]*\" Concentrations\")),\n    ylab = expression(bold(\"Concentration (μg/m³)\")),\n    xlab = \"\"\n)\n\nboxplot(air_pollution_daily_data$PM10_Daily_Average ~ air_pollution_daily_data$DayType,\n    col = c(\"lightblue\", \"grey\"),\n    main = expression(bold(\"PM\"[10]*\" Concentrations\")),\n    ylab = expression(bold(\"Concentration (μg/m³)\")),\n    xlab = \"\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen using this code, par(mfrow = c(1, 2)). Your plot window is split into 2 grids, and that setting will remain like that until explicitly tell RStudio remove it. You can overide this setting by running the following code:\n\n# reset plot layout\npar(mfrow = c(1, 1))",
    "crumbs": [
      "Core Content",
      "Week 5: Examining Data III"
    ]
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/01-Overview.html",
    "href": "datasets/All Data/Lucy_rmdfiles/01-Overview.html",
    "title": "Overview",
    "section": "",
    "text": "The pollutants that reduce air quality in urban environments can vary considerably on small spatial scales. This is an important consideration when interpreting patterns in air quality data but is also important for individual personal exposure.\nThe aim of this practical is to try and determine changes in air quality around your allocated field location in order to test a hypothesis of your choosing. Your hypothesis may depend on the features of your street and can be anything that you like, for example:\n\nAir quality is worse in areas of slow moving traffic.\nAir quality improves further from roads.\nPub smoking areas impact local air quality.\n\n\nRemember You will need to compare two groups during the computer practical, so please bear this in mind when choosing and testing your hypothesis. The sample size requirement is likely much larger than you think in order to test significance.\n\n\n\nYou should use the air quality monitor to test your hypothesis. Think about the best places and times to take measurements, the number of replicate measurements that you need to take, how often you should undertake the measurements and so on. These measured data should be recorded in your field notebooks along with the usual information of dates, times, location, personnel. Also, think about any additional data or observations that you need to take in order to undertake the statistical test and to better interpret your measurements.\n\nRemember Your group is responsible for your monitor. They are not designed to be left outside and will be damaged if they get wet.\n\n\n\n\nBy this point, you should have already decided on your hypothesis and visited your allocated street to undertake your measurements. We will analyse the data that you have collected in the field using R. More information is available on the “Tasks” page."
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/01-Overview.html#in-the-field",
    "href": "datasets/All Data/Lucy_rmdfiles/01-Overview.html#in-the-field",
    "title": "Overview",
    "section": "",
    "text": "You should use the air quality monitor to test your hypothesis. Think about the best places and times to take measurements, the number of replicate measurements that you need to take, how often you should undertake the measurements and so on. These measured data should be recorded in your field notebooks along with the usual information of dates, times, location, personnel. Also, think about any additional data or observations that you need to take in order to undertake the statistical test and to better interpret your measurements.\n\nRemember Your group is responsible for your monitor. They are not designed to be left outside and will be damaged if they get wet."
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/01-Overview.html#during-the-computer-practical",
    "href": "datasets/All Data/Lucy_rmdfiles/01-Overview.html#during-the-computer-practical",
    "title": "Overview",
    "section": "",
    "text": "By this point, you should have already decided on your hypothesis and visited your allocated street to undertake your measurements. We will analyse the data that you have collected in the field using R. More information is available on the “Tasks” page."
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/index.html",
    "href": "datasets/All Data/Lucy_rmdfiles/index.html",
    "title": "GEOG0014: Geography in the Field 2 (GIF2) 2024/25",
    "section": "",
    "text": "Welcome to the first GEOG0014 Geography in the Field 2 practical. You will use and build on some of the techniques you learned in Term 1 in GEOG0013 Geography in the Field 1. Data analysis will be undertaken using R, which was introduced to you in Term 1 by Dr Anwar Musah.\n\nImportant! If you did not download R/RStudio on your personal computer for the GEOG0013 practicals, then you will need to do so before attending your timetabled practical for GEOG0014 as these will not be held in UCL cluster rooms. If you are having difficulties installing RStudio, and installation is not possible because you are using a tablet, you can use the UCL Desktop Cloud system to work remotely and gain access to RStudio from your laptop/PC/tablet.\n\nFor help getting started with R, you can revisit Anwar’s video ‘Introduction to Statistics’ ‘Part 1 Getting started with R studio’ You will also find it helpful to refer more generally to the GEOG0013 introduction to statistics from term 1.\nSince you will hand in a group report, you can complete the practical together as a group. You will need to complete the next R practical individually; now would be a good opportunity to work together, so that all your group have (re)familiarised themselves with R and know how to complete the practical.\n\n\n\n\n\n\n\n\n\nDate\nSession\n\n\n\n\nMonday 20th Jan (12-1pm)\nLecture + introduction to the practical\n\n\nTuesday 21st Jan (10am-12pm)\nCollection of monitors One representative from each group must visit Ian Patmore (Basement lab; North-west Wing) to collect, sign out and be instructed in the use of the air quality monitor.\n\n\nFriday 24th Jan (2-5pm)\nTimetabled period for the field practical (but in practice, you have from the afternoon of 21st Jan to Thursday 30th to undertake the fieldwork)\n\n\nFriday 31st Jan (check your timetable for your allocated practical)\nComputer practical for statistical analysis of your field data It is recommended to have read through this practical handbook before attending the practical.\n\n\nTuesday 4th Feb OR Thursday 6th Feb (depending on your timetable)\nDrop in seminar for help with the practical You must also sign back your air quality monitors at this session. Important: Your report will not be accepted as complete until the monitors are returned unless there are extenuating circumstances\n\n\nFriday 14th Feb (12 noon)\nDeadline for handing in group reports on Moodle\n\n\n\n\n\n\nYour first point of call to receive help is to attend the practical and the seminar for your group. At these, you can speak to me and/or the PGTAs. There is also a wealth of help and support for R on the internet; you may wish to search for a solution on stack overflow. If you have followed these steps and are still stuck, you can book my ASF hours. Please do not email questions to the PGTA leading your seminar and do not send code via email to any of the teaching team expecting them to correct any errors."
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/index.html#timetable",
    "href": "datasets/All Data/Lucy_rmdfiles/index.html#timetable",
    "title": "GEOG0014: Geography in the Field 2 (GIF2) 2024/25",
    "section": "",
    "text": "Date\nSession\n\n\n\n\nMonday 20th Jan (12-1pm)\nLecture + introduction to the practical\n\n\nTuesday 21st Jan (10am-12pm)\nCollection of monitors One representative from each group must visit Ian Patmore (Basement lab; North-west Wing) to collect, sign out and be instructed in the use of the air quality monitor.\n\n\nFriday 24th Jan (2-5pm)\nTimetabled period for the field practical (but in practice, you have from the afternoon of 21st Jan to Thursday 30th to undertake the fieldwork)\n\n\nFriday 31st Jan (check your timetable for your allocated practical)\nComputer practical for statistical analysis of your field data It is recommended to have read through this practical handbook before attending the practical.\n\n\nTuesday 4th Feb OR Thursday 6th Feb (depending on your timetable)\nDrop in seminar for help with the practical You must also sign back your air quality monitors at this session. Important: Your report will not be accepted as complete until the monitors are returned unless there are extenuating circumstances\n\n\nFriday 14th Feb (12 noon)\nDeadline for handing in group reports on Moodle"
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/index.html#getting-help",
    "href": "datasets/All Data/Lucy_rmdfiles/index.html#getting-help",
    "title": "GEOG0014: Geography in the Field 2 (GIF2) 2024/25",
    "section": "",
    "text": "Your first point of call to receive help is to attend the practical and the seminar for your group. At these, you can speak to me and/or the PGTAs. There is also a wealth of help and support for R on the internet; you may wish to search for a solution on stack overflow. If you have followed these steps and are still stuck, you can book my ASF hours. Please do not email questions to the PGTA leading your seminar and do not send code via email to any of the teaching team expecting them to correct any errors."
  },
  {
    "objectID": "05-summary_measures.html",
    "href": "05-summary_measures.html",
    "title": "Week 4: Examining Data II",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nUnderstand what central tendency measures are for exploring data\nHow to compute central tendency measures such as mean, and median\nHow to compute range measures such as minimum, maximum and interquartile range\nHow to compute measures of uncertainty surrounding the mean such as variance and standard deviation\nHow to translate the central tendency and range measures visually as a box plot\n\n\n\n\n\n\n\nWarning\n\n\n\nWe will using the same dataset from last week i.e., Barcelona_Air_Pollution_data.csv. If you have not already - you can download it from [HERE].\nInstructions In your computer, do the following:\n\nGo to the folder named GEOG0186 - this should have been created in Week 2\nNext, create a new sub-folder within GEOG0186 and rename it as Week 4.\nFrom the downloaded folder Dataset for Week 3 (using last week’s dataset), make sure to unzip and transfer ALL the datasets directly to the Week 4 folder.\n\n\n\n\n\n\nThe theory videos are lectures covering what central tendency measures, range values and measures for variability are. You are welcome to jump straight to the coding exercise by clicking HERE if you want to skip this section!\nCentral tendency measures contains a list of summary measurements that allows the user to summarize the data to some central measure and to gauge the spread of the data (i.e., errors/departures from central measure). It is best for continuous variables, we can hence compute the following summary measurements (watch video below see to definitions):\n[Theory]: Central tendency measures (Length: 17:12 minutes)\n\n\n\n\n\n\n\n\n\nWatch on YouTube LINK\n[Theory]: Range values and interquartile ranges (Length: 18:32 minutes)\n\n\n\n\n\n\n\n\n\nWatch on YouTube [LINK]\n[Theory]: Variance & standard deviation (Length: 13:39 minutes)\n\n\n\n\n\n\n\n\n\nWatch on YouTube [LINK]\n\n\n\nHere, we will focus on descriptive statistics - last week, we learnt how to do frequency distributions. Now, we are going to compute measure for central tendency and variability. in such data set. Doing these type of analysis we help us understand the distribution of the dataset more making the interpretation a lot easier.\nLet’s begin!\n\n\nLet us import for following dataset Barcelona_Air_Pollution_data.csv into RStudio, and call this object air_ quality_data.\nRemember - always make sure that your work directory is linked to your folder containing your data.\nFor Windows:\n\nsetwd(\"C:/Users/accountName/Desktop/GEOG0186/Week 4\")\n\nFor Macs:\n\nsetwd(\"/Users/accountName/Desktop/GEOG0186/Week 4\")\n\nNow, import you the data set as follows:\n\nair_quality_data &lt;- read.csv(\"Barcelona_Air_Pollution_data.csv\")\n\nWe are going to compute the following measures:\n\nMinimum and Maximum values\nUpper and Lower Quartiles (or Interquartile ranges)\nMean\nMedian\nVariance and Standard deviation\n\n\n\n\nThe range values basically corresponds to extreme values in the dataset i.e., the lowest and highest that is present. You can use the functions min() and max() to report these extreme values.\n\nmin(air_quality_data$NO2_est)\n\n[1] 2\n\nmax(air_quality_data$NO2_est)\n\n[1] 130\n\n\nThe range is difference between the minimum and maximum value. R does not have a function for this, so you will have to compute it on your own:\n\nmax(air_quality_data$NO2_est) - min(air_quality_data$NO2_est)\n\n[1] 128\n\n\n\n\n\nDivide a range of data into four equal parts (i.e., 100/4 parts = 25%). For example:\n\nLower quartile: It’s the number below which lies the 25% of the bottom data.\nMedian: Divides the range in the middle & has 50% of the data below it.\nUpper quartile: It has 75% of the data below it & the top 25% of the data above it.\n\nThis can be easily computed using the function quantile(, probs = c(0.25, 0.5, 0.75))\n\nquantile(air_quality_data$NO2_est, probs = c(0.25, 0.5, 0.75))\n\n25% 50% 75% \n 46  59  74 \n\n\nYou can compute the interquartile range (IQR) in R, which is the difference between the 3rd quartile (Q3) and the 1st quartile (Q1) using the IQR():\n\nIQR(air_quality_data$NO2_est)\n\n[1] 28\n\n\n\n\n\nThe mean is the average of a set of numbers — it’s found by adding up all the values and then dividing by the number of values. Its the most common statistic used to summarise continuous data! It is a central estimate. Computing the mean in R is very simple - all you have to do is use the mean() function.\n\nmean(air_quality_data$NO2_est)\n\n[1] 59.6922\n\n\n\n\n\nLike mean, the median is also a central estimate! However, The median is actually the middle-most value of a dataset when the values are arranged in order. Its the another common statistic used to summarise continuous data especially if it not following a normal distribution (which we will cover in week 6)! Computing a median in R is also very simple - all you have to do is use the median() function.\n\nmedian(air_quality_data$NO2_est)\n\n[1] 59\n\n\nThese descriptive estimates are easy to compute. Can be tedious if you will have to use all these functions! You can compute these measure of the fly as summaries by simply use the summary() function on the variable of interest (i.e., NO2_est from the original dataset).\n\n# compute all descriptive summaries measurements\nsummary(air_quality_data$NO2_est)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00   46.00   59.00   59.69   74.00  130.00 \n\n\nAs you can see using summary() automatically gives you almost all the summary estimates needed for interpretation. The NO\\(_2\\) air levels in Barcelona was 59.69ppb (with median 59.00pbb). The lowest and highest values are 2.0ppb and 130.0ppb, respectively (with 25th and 75th percentiles being 46.00ppb and 74.00pbb, respectively.)\n\n\n\nFinally, you can compute the standard deviation using the sd() function as follows:\n\n# compute all descriptive summaries measurements\nsd(air_quality_data$NO2_est)\n\n[1] 20.60876\n\n\nThe standard deviation is ± 20.61pbb – this is the error around the mean of NO\\(_2\\) i.e., estimated as 59.69ppb. To visualise the distribution of a continuous variable – your ‘go-to’ plot is a boxplot. You can use the function boxplot() to generate one. Type the following to churn it:\n\n# Box plot\nboxplot(air_quality_data$NO2_est, ylab = \"NO2 estimates (ppb)\", main=\"Box plot: Summary of Nitrogen Dioxide in Barcelona\")\n\n\n\n\n\n\n\n\nThe above box plot is essential the visual representation of the summary results churned by the summary(). Here is the concise interpretation of the above results:\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: The overall mean air pollution levels for NO\\(_2\\) in Eixample (Barcelona) from 718 observation was 59.69ppb (with one SD of ±20.63ppb). 25% (at the lower end i.e., lower quartile) of the distribution for air population levels for Nitrogen Dioxide are below 46.00ppb (which is considered to cause low health impact); while from 75% onwards (i.e., upper quartile) of the distribution has air pollution levels of N0\\(_2\\) which is above 74.00pbb (which is consider to considered to cause some moderate health impact). The overall range in the distribution is 128ppb where the lowest observed value is 2.0ppb (minimum) and the highest observed value is 130ppb (maximum)\n\n\n\n\n\n\n\nCarry out a full-on descriptive analysis for the PM10 variable in the air_quality_data data frame object. In your interpretation, you will need to report the following altogether:\n\n\nMean\nStandard Deviation\nMedian\nInterquartile ranges\nMinimum and Maximum values\n\n\n\n\n\n\n\nNote\n\n\n\nHINTS to consider: Use the summary(), sd() and IQR() functions to derive the desired statistical outputs for PM10 in the dataset.\n\n\n\nGenerate a box plot using boxplot() function to show the distribution PM10. Ensure that the image is fully annotated with title and y-titles.\n\n\nHave a go at these questions before revealing the solution codes and output below\n\n\n\nClick here to see solution code:\n\n\n# Solutions for 1\n# Using PM10 report all summary measures for mean, median minimum and maximum\nsummary(air_quality_data$PM10_est)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3.00   26.00   32.00   31.81   38.00   60.00 \n\n# Get the IQRs\nquantile(air_quality_data$PM10_est, probs = c(0.25, 0.5, 0.75))\n\n25% 50% 75% \n 26  32  38 \n\nIQR(air_quality_data$PM10_est)\n\n[1] 12\n\n# show the SD\nsd(air_quality_data$PM10_est)\n\n[1] 9.544946\n\n# show boxplot\nboxplot(air_quality_data$PM10_est, ylab = \"PM10\", main=\"Box plot: Summary of PM10 in Barcelona\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: The overall mean of PM10 in Eixample (Barcelona) from 718 observation was 31.81 (with one SD of ±9.54). 25% (at the lower end i.e., lower quartile) of the distribution for this variable is below 26.00; while from 75% onwards (i.e., upper quartile) of the distribution is above 38.00. The overall range in the distribution is 57.00 where the lowest observed value is 3.0 (minimum) and the highest observed value is 60.0 (maximum)",
    "crumbs": [
      "Core Content",
      "Week 4: Examining Data II"
    ]
  },
  {
    "objectID": "05-summary_measures.html#learning-outcomes",
    "href": "05-summary_measures.html#learning-outcomes",
    "title": "Week 4: Examining Data II",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nUnderstand what central tendency measures are for exploring data\nHow to compute central tendency measures such as mean, and median\nHow to compute range measures such as minimum, maximum and interquartile range\nHow to compute measures of uncertainty surrounding the mean such as variance and standard deviation\nHow to translate the central tendency and range measures visually as a box plot\n\n\n\n\n\n\n\nWarning\n\n\n\nWe will using the same dataset from last week i.e., Barcelona_Air_Pollution_data.csv. If you have not already - you can download it from [HERE].\nInstructions In your computer, do the following:\n\nGo to the folder named GEOG0186 - this should have been created in Week 2\nNext, create a new sub-folder within GEOG0186 and rename it as Week 4.\nFrom the downloaded folder Dataset for Week 3 (using last week’s dataset), make sure to unzip and transfer ALL the datasets directly to the Week 4 folder.",
    "crumbs": [
      "Core Content",
      "Week 4: Examining Data II"
    ]
  },
  {
    "objectID": "05-summary_measures.html#short-lecture-videos-optional",
    "href": "05-summary_measures.html#short-lecture-videos-optional",
    "title": "Week 4: Examining Data II",
    "section": "",
    "text": "The theory videos are lectures covering what central tendency measures, range values and measures for variability are. You are welcome to jump straight to the coding exercise by clicking HERE if you want to skip this section!\nCentral tendency measures contains a list of summary measurements that allows the user to summarize the data to some central measure and to gauge the spread of the data (i.e., errors/departures from central measure). It is best for continuous variables, we can hence compute the following summary measurements (watch video below see to definitions):\n[Theory]: Central tendency measures (Length: 17:12 minutes)\n\n\n\n\n\n\n\n\n\nWatch on YouTube LINK\n[Theory]: Range values and interquartile ranges (Length: 18:32 minutes)\n\n\n\n\n\n\n\n\n\nWatch on YouTube [LINK]\n[Theory]: Variance & standard deviation (Length: 13:39 minutes)\n\n\n\n\n\n\n\n\n\nWatch on YouTube [LINK]",
    "crumbs": [
      "Core Content",
      "Week 4: Examining Data II"
    ]
  },
  {
    "objectID": "05-summary_measures.html#analysing-air-pollution-data-in-barcelona-part-ii",
    "href": "05-summary_measures.html#analysing-air-pollution-data-in-barcelona-part-ii",
    "title": "Week 4: Examining Data II",
    "section": "",
    "text": "Here, we will focus on descriptive statistics - last week, we learnt how to do frequency distributions. Now, we are going to compute measure for central tendency and variability. in such data set. Doing these type of analysis we help us understand the distribution of the dataset more making the interpretation a lot easier.\nLet’s begin!\n\n\nLet us import for following dataset Barcelona_Air_Pollution_data.csv into RStudio, and call this object air_ quality_data.\nRemember - always make sure that your work directory is linked to your folder containing your data.\nFor Windows:\n\nsetwd(\"C:/Users/accountName/Desktop/GEOG0186/Week 4\")\n\nFor Macs:\n\nsetwd(\"/Users/accountName/Desktop/GEOG0186/Week 4\")\n\nNow, import you the data set as follows:\n\nair_quality_data &lt;- read.csv(\"Barcelona_Air_Pollution_data.csv\")\n\nWe are going to compute the following measures:\n\nMinimum and Maximum values\nUpper and Lower Quartiles (or Interquartile ranges)\nMean\nMedian\nVariance and Standard deviation\n\n\n\n\nThe range values basically corresponds to extreme values in the dataset i.e., the lowest and highest that is present. You can use the functions min() and max() to report these extreme values.\n\nmin(air_quality_data$NO2_est)\n\n[1] 2\n\nmax(air_quality_data$NO2_est)\n\n[1] 130\n\n\nThe range is difference between the minimum and maximum value. R does not have a function for this, so you will have to compute it on your own:\n\nmax(air_quality_data$NO2_est) - min(air_quality_data$NO2_est)\n\n[1] 128\n\n\n\n\n\nDivide a range of data into four equal parts (i.e., 100/4 parts = 25%). For example:\n\nLower quartile: It’s the number below which lies the 25% of the bottom data.\nMedian: Divides the range in the middle & has 50% of the data below it.\nUpper quartile: It has 75% of the data below it & the top 25% of the data above it.\n\nThis can be easily computed using the function quantile(, probs = c(0.25, 0.5, 0.75))\n\nquantile(air_quality_data$NO2_est, probs = c(0.25, 0.5, 0.75))\n\n25% 50% 75% \n 46  59  74 \n\n\nYou can compute the interquartile range (IQR) in R, which is the difference between the 3rd quartile (Q3) and the 1st quartile (Q1) using the IQR():\n\nIQR(air_quality_data$NO2_est)\n\n[1] 28\n\n\n\n\n\nThe mean is the average of a set of numbers — it’s found by adding up all the values and then dividing by the number of values. Its the most common statistic used to summarise continuous data! It is a central estimate. Computing the mean in R is very simple - all you have to do is use the mean() function.\n\nmean(air_quality_data$NO2_est)\n\n[1] 59.6922\n\n\n\n\n\nLike mean, the median is also a central estimate! However, The median is actually the middle-most value of a dataset when the values are arranged in order. Its the another common statistic used to summarise continuous data especially if it not following a normal distribution (which we will cover in week 6)! Computing a median in R is also very simple - all you have to do is use the median() function.\n\nmedian(air_quality_data$NO2_est)\n\n[1] 59\n\n\nThese descriptive estimates are easy to compute. Can be tedious if you will have to use all these functions! You can compute these measure of the fly as summaries by simply use the summary() function on the variable of interest (i.e., NO2_est from the original dataset).\n\n# compute all descriptive summaries measurements\nsummary(air_quality_data$NO2_est)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00   46.00   59.00   59.69   74.00  130.00 \n\n\nAs you can see using summary() automatically gives you almost all the summary estimates needed for interpretation. The NO\\(_2\\) air levels in Barcelona was 59.69ppb (with median 59.00pbb). The lowest and highest values are 2.0ppb and 130.0ppb, respectively (with 25th and 75th percentiles being 46.00ppb and 74.00pbb, respectively.)\n\n\n\nFinally, you can compute the standard deviation using the sd() function as follows:\n\n# compute all descriptive summaries measurements\nsd(air_quality_data$NO2_est)\n\n[1] 20.60876\n\n\nThe standard deviation is ± 20.61pbb – this is the error around the mean of NO\\(_2\\) i.e., estimated as 59.69ppb. To visualise the distribution of a continuous variable – your ‘go-to’ plot is a boxplot. You can use the function boxplot() to generate one. Type the following to churn it:\n\n# Box plot\nboxplot(air_quality_data$NO2_est, ylab = \"NO2 estimates (ppb)\", main=\"Box plot: Summary of Nitrogen Dioxide in Barcelona\")\n\n\n\n\n\n\n\n\nThe above box plot is essential the visual representation of the summary results churned by the summary(). Here is the concise interpretation of the above results:\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: The overall mean air pollution levels for NO\\(_2\\) in Eixample (Barcelona) from 718 observation was 59.69ppb (with one SD of ±20.63ppb). 25% (at the lower end i.e., lower quartile) of the distribution for air population levels for Nitrogen Dioxide are below 46.00ppb (which is considered to cause low health impact); while from 75% onwards (i.e., upper quartile) of the distribution has air pollution levels of N0\\(_2\\) which is above 74.00pbb (which is consider to considered to cause some moderate health impact). The overall range in the distribution is 128ppb where the lowest observed value is 2.0ppb (minimum) and the highest observed value is 130ppb (maximum)",
    "crumbs": [
      "Core Content",
      "Week 4: Examining Data II"
    ]
  },
  {
    "objectID": "05-summary_measures.html#exercise",
    "href": "05-summary_measures.html#exercise",
    "title": "Week 4: Examining Data II",
    "section": "",
    "text": "Carry out a full-on descriptive analysis for the PM10 variable in the air_quality_data data frame object. In your interpretation, you will need to report the following altogether:\n\n\nMean\nStandard Deviation\nMedian\nInterquartile ranges\nMinimum and Maximum values\n\n\n\n\n\n\n\nNote\n\n\n\nHINTS to consider: Use the summary(), sd() and IQR() functions to derive the desired statistical outputs for PM10 in the dataset.\n\n\n\nGenerate a box plot using boxplot() function to show the distribution PM10. Ensure that the image is fully annotated with title and y-titles.\n\n\nHave a go at these questions before revealing the solution codes and output below\n\n\n\nClick here to see solution code:\n\n\n# Solutions for 1\n# Using PM10 report all summary measures for mean, median minimum and maximum\nsummary(air_quality_data$PM10_est)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3.00   26.00   32.00   31.81   38.00   60.00 \n\n# Get the IQRs\nquantile(air_quality_data$PM10_est, probs = c(0.25, 0.5, 0.75))\n\n25% 50% 75% \n 26  32  38 \n\nIQR(air_quality_data$PM10_est)\n\n[1] 12\n\n# show the SD\nsd(air_quality_data$PM10_est)\n\n[1] 9.544946\n\n# show boxplot\nboxplot(air_quality_data$PM10_est, ylab = \"PM10\", main=\"Box plot: Summary of PM10 in Barcelona\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: The overall mean of PM10 in Eixample (Barcelona) from 718 observation was 31.81 (with one SD of ±9.54). 25% (at the lower end i.e., lower quartile) of the distribution for this variable is below 26.00; while from 75% onwards (i.e., upper quartile) of the distribution is above 38.00. The overall range in the distribution is 57.00 where the lowest observed value is 3.0 (minimum) and the highest observed value is 60.0 (maximum)",
    "crumbs": [
      "Core Content",
      "Week 4: Examining Data II"
    ]
  },
  {
    "objectID": "00-module_overview.html#getting-started-with-the-learning-materials-length-001351",
    "href": "00-module_overview.html#getting-started-with-the-learning-materials-length-001351",
    "title": "Quantitative Skills (QSkills)",
    "section": "Getting Started with the Learning Materials (Length: 00:13:51)",
    "text": "Getting Started with the Learning Materials (Length: 00:13:51)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]",
    "crumbs": [
      "Module Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "00-module_overview.html#self-guided-flipped-learning-approach",
    "href": "00-module_overview.html#self-guided-flipped-learning-approach",
    "title": "Quantitative Skills (QSkills)",
    "section": "Self-Guided & Flipped Learning Approach",
    "text": "Self-Guided & Flipped Learning Approach\nThis strand follows a more relaxed teaching structure. You will work independently at your own pace, with guidance provided along the way. We expect you to engage with the learning materials - supported by online tutorials, coding instructions, and guidance videos - before going into the computer practical sessions. This allows our session time to be used for discussion, problem-solving, and hands-on practice.",
    "crumbs": [
      "Module Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "00-module_overview.html#computer-practical-sessions-timetable",
    "href": "00-module_overview.html#computer-practical-sessions-timetable",
    "title": "Quantitative Skills (QSkills)",
    "section": "Computer Practical Sessions & Timetable",
    "text": "Computer Practical Sessions & Timetable\nThere are 10 computer practical sessions throughout Term 1, all delivered in-person and facilitated by Postgraduate Teaching Assistants (PGTAs).\nAll sessions take place every Wednesday between 9:00am and 12:00pm. The entire Geography cohort has been split into three groups (Computer Groups A, B, C and D), with practicals delivered separately in one-hour, back-to-back sessions each week to each group. The schedule is as follows:\n\nGroup A: 09:00am–10:00am, Christopher Ingold Building G20 – Public Cluster\nGroup B: 10:00am–11:00am, Birkbeck Malet Street 422/423 – Public Cluster\nGroup C: 11:00am–12:00pm, Birkbeck Malet Street 422/423 – Public Cluster\nGroup D: 11:00am–12:00pm, Bedford Way (26) Room G11– Public Cluster\n\n\n\n\n\n\n\nImportant\n\n\n\nThe above locations for Group A, B, C and D are public clusters with PC workstations. Since you will mostly be working through the materials at your own pace, often on your own laptops with RStudio installed, we highly recommend that you bring your own laptop to the computer practicals for ease of use.\n\n\nThe topics for the course are as follows:\n\n\n\n\n\n\n\n\nDates\nWeek\nList of Topics\n\n\n\n\n01/10/25\n1\nIntroduction I: Basics Building Blocks for Computing in RStudio\n\n\n08/10/25\n2\nIntroduction II: Handling Data Structures in RStudio\n\n\n15/10/25\n3\nExamining I: Frequency Distribution\n\n\n22/10/25\n4\nExamining II: Descriptive Statistics\n\n\n29/10/25\n5\nExamining III: Types of Data Visualisation\n\n\n\n\nReading Week\n\n\n12/11/25\n6\nSourcing Data I: Acquiring Data From Open Sources\n\n\n19/11/25\n7\nSourcing Data II: Sampling from Probability Distribution\n\n\n26/11/25\n8\nAir Quality I: Correlation & Regression\n\n\n03/12/25\n9\nAir Quality II: Box Plots and T-tests\n\n\n10/12/25\n10\nPortfolio Surgery & Wrap-up.",
    "crumbs": [
      "Module Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "00-module_overview.html#questions-discusson-forum-on-moodle",
    "href": "00-module_overview.html#questions-discusson-forum-on-moodle",
    "title": "Quantitative Skills (QSkills)",
    "section": "Questions & Discusson Forum on Moodle",
    "text": "Questions & Discusson Forum on Moodle\nOn Moodle, you can use the Ask a question section to post general queries about the learning materials on this website, as well as any technical issues you encounter during your self-guided study. Please refer to that section on Moodle, and either I or one of the PGTAs will respond with solutions.\n\n\n\n\n\n\nImportant\n\n\n\nWhile Moodle is the central point for information and communications for GEOG0186 Quantitative Skills and where all formative assessments are submitted. However, note that all the weekly content will only be hosted on this dedicated web page and not on Moodle.",
    "crumbs": [
      "Module Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "02-basic_building_blocks.html",
    "href": "02-basic_building_blocks.html",
    "title": "Week 1: Introduction I",
    "section": "",
    "text": "The learning objectives for today are:\n\nBecome familiar with the user interface in RStudio\nPractice basic coding in the R Console\nOpen an R script and create basic objects in RStudio\nUnderstand different data types\nData entry and variable creation in RStudio\n\nThese objectives serve as a gateway to learning RStudio and building a strong foundation. Let us begin!\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nWhen you open RStudio for the first time, you are greeted by its interface. The window is divided into three panels:\n\nR Console\nEnvironment\nFiles, Help, and Output\n\n\n\n\n\n\n\n\n\n\n\nPanel 1: The R Console allows you to type R code to perform quick commands and basic calculations. It also reports whether code executions from scripts are successful or have failed.\nPanel 2: The Environment shows the objects currently stored in RStudio’s memory, such as values, vectors, data frames, and spatial objects.\nPanel 3: This panel contains several tabs. The most important are: (i) Files: access folders on your computer to open datasets; (ii) Help: view help documentation for functions and commands; and (iii) Plots: review the plots you generate (e.g., histograms, scatterplots, maps).\n\nThe section at the top of RStudio is the Menu Bar. From here, you can access functions for saving, editing, and opening a new Script File. Opening a new Script File reveals a fourth panel above the Console.\nYou can open a Script File by:\n\nClicking on the File tab in the Menu Bar. A drop-down menu will appear. Scroll to New File. Under New File, click R Script. This opens a new script titled Untitled 1.\n\n\n\n\n\n\n\nImportant\n\n\n\nThroughout the course, and in all practical tutorials, you will be encouraged to use an R Script to collate and save the code you write for any statistical or spatial analysis. However, we will only begin working with scripts in section 1.4 of the tutorials. For now, let’s start with the absolute basics: interacting with the R Console and using it as a simple calculator for typing basic code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nThe R Console window (Panel 1) is where RStudio waits for you to tell it what to do. It displays the code you enter and the results of each command. You can type commands directly into this window for immediate execution.\nLet’s begin by using the Console as a basic calculator for addition (+), subtraction (-), multiplication (*), division (/), exponents (^), and other calculations.\nClick inside the R Console window, type 19+8, and press the Enter key (↵) to see the result. Then try the following calculations by typing them into the Console:\n\n# Perform addition\n19+8\n\n# Perform subtraction\n20-89\n\n# Perform multiplication\n18*20\n\n# Perform division\n27/3\n\n# To number to a power e.g., 2 raise to the power of 8\n2^8\n\n# Perform complex sums\n(5*(170-3.405)/91)+1002\n\n\n\n\n\n\n\nImportant\n\n\n\nThe text that follows a hash symbol # in the code chunk is a comment, not actual code. It explains what the line of code without the # is doing.\n\n\nIn addition to basic arithmetic operations, we can also use common mathematical functions such as exponentials and logarithms:\n\nexp() calculates the exponential\nlog() calculates the logarithm\n\nDo not worry about fully understanding these functions for now as you will likely use them later in Weeks 8 and 9 when transforming variables.\nTry the following by typing them into the R Console window:\n\n# use exp() to apply an exponential to a value\nexp(5) \n\n# use log() to transforrm a value on to a logarithm scale\nlog(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nLet’s open an R Script file!\nNow that we are familiar with using the Console as a calculator, let’s build on this and learn one of the most important features in R: the Assignment Operator.\nThe arrow symbol &lt;- is called the Assignment Operator. You type it by pressing the less-than symbol &lt; followed by the hyphen -. It allows you to assign values to an object in R.\nObjects are stored quantities in RStudio’s environment. They can hold anything from numeric values to character strings. For example, to create a numeric object called x and assign it the value 3, type: x &lt;- 3. When you enter x in the Console and press Enter (↵), R will return the numeric value 3.\nSimilarly, to create a string object called y and assign it the text \"Hello!\", type: y &lt;- \"Hello!\" When you enter y in the Console, it will return the text value Hello!.\nNow, let’s create the objects a, b, c, and d and assign them numeric values. Perform the following by typing them into the R Console:\n\n# Create an object called 'a' and assign the value 17 to it\na &lt;- 17\n\n# Type the object 'a' in console as a command to return value 17\na\n\n# Create an object called 'b' and assign the value 10 to it\nb &lt;- 10\n\n# Type the object 'b' in console as a command to return value 10\nb\n\n# Create an object called 'c' and assign the value 9 to it\nc &lt;- 9\n\n# Type the object 'c' in console as a command to return value 9\nc\n\n# Create an object called 'd' and assign the value 8 to it\nd &lt;- 8\n\n# Type the object 'd' in console as a command to return value 8\nd\n\nNotice how the objects a, b, c and d and its value are listed and stored in RStudio’s environment panel. We can perform the following maths operations with these object values:\n\n# type the following and return an answer\n(a + b + c + d)/5\n\n# type the following and return an answer\n(5*(a-c)/d)^2\n\nLet us create more objects but this time we will assign character string(s) to them. Please note that when typing a string of characters as data you will need to cover them with quotation marks \"...\". For example, say we want to create a string object called y and assign it with some text \"Hello!\". We do this by typing y &lt;- \"Hello!\".\nTry these examples of assigning the following character text to an object:\nLet’s create more objects, but this time we will assign character strings to them. When typing a string of characters as data, you must enclose them in quotation marks \"...\".\nFor example, to create a string object called y and assign it the text \"Hello!\", type: y &lt;- \"Hello!\"\nTry the following examples to assign character text to objects:\n\n# Create an object called 'e' and assign the character string \"RStudio\"\ne &lt;- \"RStudio\"\n\n# Type the object 'e' in the console as a command to return \"RStudio\"\ne\n\n# Create an object called 'f', assign character string \"Hello world\" \nf &lt;- \"Hello world\"\n\n# Type the object 'f' in the console as a command to return \"Hello world\"\nf\n\n# Create an object called 'g' and assign \"Blade Runner is amazing\"\ng &lt;- \"Blade Runner is amazing\"\n\n# Type the object 'g' in the console to return the result\ng\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nIn programming, variables are used to store information, and each has a type determined by the kind of data it holds (e.g., integer, character, factor, logical). Assigning an integer makes the variable type “int.” A variable is simply a reserved memory location where values are stored.\nTo use R effectively, you need a solid understanding of its basic data types, data structures, and how to work with them.\nR has a range of data types. The most common ones are:\n\nnumeric (whole, real or decimal)\nlogical\ncharacter\nfactor\n\nBelow are some basic examples of data points that are classed as character, numeric, or integer values, etc.\n\ncharacter: “a”, “swc”\nnumeric: 2.0, 15.5\nlogical: TRUE, FALSE\nfactor: 2 levels “boy”, “girl” (text categories with assigned numbers as a label)\n\nYou can use the class function i.e., class() on any object to identify its data type. For example, using the class() function on existing objects we created early on (i.e., e and f) in section 1.4 tells us that its a character object.\n\ne &lt;- \"RStudio\"\nf &lt;- \"Hello world\"\n\n\nclass(f)\n\n[1] \"character\"\n\nclass(e)\n\n[1] \"character\"\n\n\nAnother example, using the class() function on existing objects we created early on (i.e., a and b) in section 1.4 tells us that its a numeric object.\n\na &lt;- 17\nb &lt;- 10\n\n\nclass(a)\n\n[1] \"numeric\"\n\nclass(b)\n\n[1] \"numeric\"\n\n\nWe are now familiar with using the console and assigning numeric and string to objects, and identifying their data type. Let’s learn who to enter data into RStudio and create our first data frame.\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nHere, we will learn some basics coding etiquettes creating a dataset from scratch using the two functions:\n\nc() combine function\ndata.frame(): data frame function\n\nAs you have already seen, RStudio is an object-oriented software package and so entering data is slightly different from the usual way of inputting information into a spreadsheet (e.g., Microsoft Excel). Here, you will need to enter the information as a Vector object before combining them into a Data Frame object.\nConsider this crude example of data containing the additional health information for 4 people. It contains the variable (or column) names id, name, height, weight and gender.\n\n\n\nid\nname\nheight\nweight\ngender\n\n\n\n\n1\nKofi\n1.65\n64.2\nM\n\n\n2\nHarry\n1.77\n80.3\nM\n\n\n3\nHuijun\n1.70\n58.7\nF\n\n\n4\nFatima\n1.68\n75.0\nF\n\n\n\nNow, when entering data to RStudio it is not like Microsoft Excel where we enter data into the cells of a spreadsheet. In RStudio, data is entered as a sequence of elements and listed inside an object called a vector.\nFor instance, if we have three age values of 12, 57 and 26 years, and we want to enter this in RStudio in that order, we need to use the combine function c() and combine these three elements into a vector object. Hence, the code will be c(12, 57, 26). We can assign this data by typing this code as age &lt;- c(12, 57, 26). Any time you type age into RStudio’s console it will hence return these three values as a vector unless you chose to overwrite it with different information.\nLet us look at this more closely with the id variable in the above data. Each person has a unique ID number from 1 to 4. We are going to list the numbers 1, 2, 3 and 4 as a sequence of elements into a vector using the combine function c() and then assign it as a vector object calling it id.\n\n# Create 'id' vector object \nid &lt;- c(1, 2, 3, 4)\n\n# Type the vector object 'id' in console to see output or press the 'run' button\nid\n\n[1] 1 2 3 4\n\n\nNow, let us enter the information the same way for the remaining columns for name, height, weight and gender like we did for id:\n\n# Create 'name' vector object\nname &lt;- c(\"Kofi\", \"Harry\", \"Huijun\", \"Fatima\")\nname\n\n[1] \"Kofi\"   \"Harry\"  \"Huijun\" \"Fatima\"\n\n# Create 'height' (in meters) vector object\nheight &lt;- c(1.65, 1.77, 1.70, 1.68)\nheight\n\n[1] 1.65 1.77 1.70 1.68\n\n# Create 'weight' (in kg) vector object\nweight &lt;- c(64.2, 80.3, 58.7, 75.0)\nweight\n\n[1] 64.2 80.3 58.7 75.0\n\n# Create 'gender' vector object\ngender &lt;- c(\"M\", \"M\", \"F\", \"F\")\ngender\n\n[1] \"M\" \"M\" \"F\" \"F\"\n\n\nNow, that we have the vector objects ready. Let us bring them together to create a proper dataset. This new object is called a Data frame. We need to list the vectors inside the data.frame() function. For example:\n\n# Create a dataset (data frame)\ndataset &lt;- data.frame(id, name, height, weight, gender)\n\n# Type the data frame object 'dataset' in console to print the output in console\ndataset\n\n  id   name height weight gender\n1  1   Kofi   1.65   64.2      M\n2  2  Harry   1.77   80.3      M\n3  3 Huijun   1.70   58.7      F\n4  4 Fatima   1.68   75.0      F\n\n\nYou can also see dataset in a data viewer, by using the View() function to see in a spreadsheet:\n\nView(dataset)\n\n\n\n\n\n\n\nImportant\n\n\n\nThe column ‘id’ is a numeric variable with integers. The second column ‘name’ is a text variable with strings. The third & fourth columns ‘height’ and ‘weight’ are examples of numeric variables with real numbers with continuous measures. The variable ‘gender’ is a text variable with strings – however, this type of variable is classed as a categorical variable as individuals were categorised as either ‘M’ and ‘F’.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nTo access a variable by its name within a data frame, you will need to first type the name of the data frame followed by a $ (dollar sign), and then typing the variable’s name of interest. For instance, suppose you just want to see the height values in the Console viewer - you just type:\n\n# to access height - you need to type 'dataset$height'\ndataset$height\n\n[1] 1.65 1.77 1.70 1.68\n\n\nWe can use other columns or variables within our data frame to create another variable. This technique is essentially important when cleaning and managing data. From this dataset, it is possible to derive the body mass index bmi from height and weight using the formula:\n\n\\(BMI = weight/height^2\\)\n\nTo generate bmi into our data frame, we would need to access the height (m) and weight (kg) columns using the $ from the data frame its stored to, and apply the above formula as a code to generate the new bmi column:\n\n# Create 'bmi' in the data frame i.e.,'dataset' and calculate 'bmi'\n# using the $weight and $height\ndataset$bmi &lt;- dataset$weight/((dataset$height)^2)\n# View the data frame ‘dataset’ and you will see the new bmi variable inside\nView(dataset)\n\nYou can overwrite the height (m) column to change its units into centimeters by multiplying it to 100; equally, the weight (kg) column can be overwritten and converted from units of kilograms to grams by multiplying it to 1000.\n\n# using $height and *100 \ndataset$height &lt;- dataset$height*100\n# using $weight and *100\ndataset$weight &lt;- dataset$weight*1000\n# view updated dataset\ndataset\n\n  id   name height weight gender      bmi\n1  1   Kofi    165  64200      M 23.58127\n2  2  Harry    177  80300      M 25.63120\n3  3 Huijun    170  58700      F 20.31142\n4  4 Fatima    168  75000      F 26.57313\n\n\n\n\n\nWell done! You have learnt the basics of coding in RStudio - next week, we will learn further coding etiquettes for managing datasets which includes:\n\nImporting & exporting spreadsheets with RStudio\nSetting up work directories\nRow and column manipulations\nMerging two data frames.\n\nYou can now save your script for future use by clicking on the save icon in the menu bar. Saving an R-script for the first will prompt a Choose Encoding window to appear:\n\n\n\n\n\n\n\n\n\nSelect the default option UTF-8 (System default) and click OK and then save.",
    "crumbs": [
      "Core Content",
      "Week 1: Introduction I"
    ]
  },
  {
    "objectID": "02-basic_building_blocks.html#learning-objectives",
    "href": "02-basic_building_blocks.html#learning-objectives",
    "title": "Week 1: Introduction I",
    "section": "",
    "text": "The learning objectives for today are:\n\nBecome familiar with the user interface in RStudio\nPractice basic coding in the R Console\nOpen an R script and create basic objects in RStudio\nUnderstand different data types\nData entry and variable creation in RStudio\n\nThese objectives serve as a gateway to learning RStudio and building a strong foundation. Let us begin!",
    "crumbs": [
      "Core Content",
      "Week 1: Introduction I"
    ]
  },
  {
    "objectID": "02-basic_building_blocks.html#rstudio-interface-length-000708",
    "href": "02-basic_building_blocks.html#rstudio-interface-length-000708",
    "title": "Week 1: Introduction I",
    "section": "",
    "text": "[Watch on YouTube]\nWhen you open RStudio for the first time, you are greeted by its interface. The window is divided into three panels:\n\nR Console\nEnvironment\nFiles, Help, and Output\n\n\n\n\n\n\n\n\n\n\n\nPanel 1: The R Console allows you to type R code to perform quick commands and basic calculations. It also reports whether code executions from scripts are successful or have failed.\nPanel 2: The Environment shows the objects currently stored in RStudio’s memory, such as values, vectors, data frames, and spatial objects.\nPanel 3: This panel contains several tabs. The most important are: (i) Files: access folders on your computer to open datasets; (ii) Help: view help documentation for functions and commands; and (iii) Plots: review the plots you generate (e.g., histograms, scatterplots, maps).\n\nThe section at the top of RStudio is the Menu Bar. From here, you can access functions for saving, editing, and opening a new Script File. Opening a new Script File reveals a fourth panel above the Console.\nYou can open a Script File by:\n\nClicking on the File tab in the Menu Bar. A drop-down menu will appear. Scroll to New File. Under New File, click R Script. This opens a new script titled Untitled 1.\n\n\n\n\n\n\n\nImportant\n\n\n\nThroughout the course, and in all practical tutorials, you will be encouraged to use an R Script to collate and save the code you write for any statistical or spatial analysis. However, we will only begin working with scripts in section 1.4 of the tutorials. For now, let’s start with the absolute basics: interacting with the R Console and using it as a simple calculator for typing basic code.",
    "crumbs": [
      "Core Content",
      "Week 1: Introduction I"
    ]
  },
  {
    "objectID": "02-basic_building_blocks.html#using-r-console-as-a-calculator-length-000726",
    "href": "02-basic_building_blocks.html#using-r-console-as-a-calculator-length-000726",
    "title": "Week 1: Introduction I",
    "section": "",
    "text": "[Watch on YouTube]\nThe R Console window (Panel 1) is where RStudio waits for you to tell it what to do. It displays the code you enter and the results of each command. You can type commands directly into this window for immediate execution.\nLet’s begin by using the Console as a basic calculator for addition (+), subtraction (-), multiplication (*), division (/), exponents (^), and other calculations.\nClick inside the R Console window, type 19+8, and press the Enter key (↵) to see the result. Then try the following calculations by typing them into the Console:\n\n# Perform addition\n19+8\n\n# Perform subtraction\n20-89\n\n# Perform multiplication\n18*20\n\n# Perform division\n27/3\n\n# To number to a power e.g., 2 raise to the power of 8\n2^8\n\n# Perform complex sums\n(5*(170-3.405)/91)+1002\n\n\n\n\n\n\n\nImportant\n\n\n\nThe text that follows a hash symbol # in the code chunk is a comment, not actual code. It explains what the line of code without the # is doing.\n\n\nIn addition to basic arithmetic operations, we can also use common mathematical functions such as exponentials and logarithms:\n\nexp() calculates the exponential\nlog() calculates the logarithm\n\nDo not worry about fully understanding these functions for now as you will likely use them later in Weeks 8 and 9 when transforming variables.\nTry the following by typing them into the R Console window:\n\n# use exp() to apply an exponential to a value\nexp(5) \n\n# use log() to transforrm a value on to a logarithm scale\nlog(3)",
    "crumbs": [
      "Core Content",
      "Week 1: Introduction I"
    ]
  },
  {
    "objectID": "02-basic_building_blocks.html#creating-basic-objects-and-assignment-operator-length-001259",
    "href": "02-basic_building_blocks.html#creating-basic-objects-and-assignment-operator-length-001259",
    "title": "Week 1: Introduction I",
    "section": "",
    "text": "[Watch on YouTube]\nLet’s open an R Script file!\nNow that we are familiar with using the Console as a calculator, let’s build on this and learn one of the most important features in R: the Assignment Operator.\nThe arrow symbol &lt;- is called the Assignment Operator. You type it by pressing the less-than symbol &lt; followed by the hyphen -. It allows you to assign values to an object in R.\nObjects are stored quantities in RStudio’s environment. They can hold anything from numeric values to character strings. For example, to create a numeric object called x and assign it the value 3, type: x &lt;- 3. When you enter x in the Console and press Enter (↵), R will return the numeric value 3.\nSimilarly, to create a string object called y and assign it the text \"Hello!\", type: y &lt;- \"Hello!\" When you enter y in the Console, it will return the text value Hello!.\nNow, let’s create the objects a, b, c, and d and assign them numeric values. Perform the following by typing them into the R Console:\n\n# Create an object called 'a' and assign the value 17 to it\na &lt;- 17\n\n# Type the object 'a' in console as a command to return value 17\na\n\n# Create an object called 'b' and assign the value 10 to it\nb &lt;- 10\n\n# Type the object 'b' in console as a command to return value 10\nb\n\n# Create an object called 'c' and assign the value 9 to it\nc &lt;- 9\n\n# Type the object 'c' in console as a command to return value 9\nc\n\n# Create an object called 'd' and assign the value 8 to it\nd &lt;- 8\n\n# Type the object 'd' in console as a command to return value 8\nd\n\nNotice how the objects a, b, c and d and its value are listed and stored in RStudio’s environment panel. We can perform the following maths operations with these object values:\n\n# type the following and return an answer\n(a + b + c + d)/5\n\n# type the following and return an answer\n(5*(a-c)/d)^2\n\nLet us create more objects but this time we will assign character string(s) to them. Please note that when typing a string of characters as data you will need to cover them with quotation marks \"...\". For example, say we want to create a string object called y and assign it with some text \"Hello!\". We do this by typing y &lt;- \"Hello!\".\nTry these examples of assigning the following character text to an object:\nLet’s create more objects, but this time we will assign character strings to them. When typing a string of characters as data, you must enclose them in quotation marks \"...\".\nFor example, to create a string object called y and assign it the text \"Hello!\", type: y &lt;- \"Hello!\"\nTry the following examples to assign character text to objects:\n\n# Create an object called 'e' and assign the character string \"RStudio\"\ne &lt;- \"RStudio\"\n\n# Type the object 'e' in the console as a command to return \"RStudio\"\ne\n\n# Create an object called 'f', assign character string \"Hello world\" \nf &lt;- \"Hello world\"\n\n# Type the object 'f' in the console as a command to return \"Hello world\"\nf\n\n# Create an object called 'g' and assign \"Blade Runner is amazing\"\ng &lt;- \"Blade Runner is amazing\"\n\n# Type the object 'g' in the console to return the result\ng",
    "crumbs": [
      "Core Content",
      "Week 1: Introduction I"
    ]
  },
  {
    "objectID": "02-basic_building_blocks.html#how-to-identify-data-types-length-000829",
    "href": "02-basic_building_blocks.html#how-to-identify-data-types-length-000829",
    "title": "Week 1: Introduction I",
    "section": "",
    "text": "[Watch on YouTube]\nIn programming, variables are used to store information, and each has a type determined by the kind of data it holds (e.g., integer, character, factor, logical). Assigning an integer makes the variable type “int.” A variable is simply a reserved memory location where values are stored.\nTo use R effectively, you need a solid understanding of its basic data types, data structures, and how to work with them.\nR has a range of data types. The most common ones are:\n\nnumeric (whole, real or decimal)\nlogical\ncharacter\nfactor\n\nBelow are some basic examples of data points that are classed as character, numeric, or integer values, etc.\n\ncharacter: “a”, “swc”\nnumeric: 2.0, 15.5\nlogical: TRUE, FALSE\nfactor: 2 levels “boy”, “girl” (text categories with assigned numbers as a label)\n\nYou can use the class function i.e., class() on any object to identify its data type. For example, using the class() function on existing objects we created early on (i.e., e and f) in section 1.4 tells us that its a character object.\n\ne &lt;- \"RStudio\"\nf &lt;- \"Hello world\"\n\n\nclass(f)\n\n[1] \"character\"\n\nclass(e)\n\n[1] \"character\"\n\n\nAnother example, using the class() function on existing objects we created early on (i.e., a and b) in section 1.4 tells us that its a numeric object.\n\na &lt;- 17\nb &lt;- 10\n\n\nclass(a)\n\n[1] \"numeric\"\n\nclass(b)\n\n[1] \"numeric\"\n\n\nWe are now familiar with using the console and assigning numeric and string to objects, and identifying their data type. Let’s learn who to enter data into RStudio and create our first data frame.",
    "crumbs": [
      "Core Content",
      "Week 1: Introduction I"
    ]
  },
  {
    "objectID": "02-basic_building_blocks.html#how-to-enter-data-and-create-variables-in-rstudio-length-001627",
    "href": "02-basic_building_blocks.html#how-to-enter-data-and-create-variables-in-rstudio-length-001627",
    "title": "Week 1: Introduction I",
    "section": "",
    "text": "[Watch on YouTube]\nHere, we will learn some basics coding etiquettes creating a dataset from scratch using the two functions:\n\nc() combine function\ndata.frame(): data frame function\n\nAs you have already seen, RStudio is an object-oriented software package and so entering data is slightly different from the usual way of inputting information into a spreadsheet (e.g., Microsoft Excel). Here, you will need to enter the information as a Vector object before combining them into a Data Frame object.\nConsider this crude example of data containing the additional health information for 4 people. It contains the variable (or column) names id, name, height, weight and gender.\n\n\n\nid\nname\nheight\nweight\ngender\n\n\n\n\n1\nKofi\n1.65\n64.2\nM\n\n\n2\nHarry\n1.77\n80.3\nM\n\n\n3\nHuijun\n1.70\n58.7\nF\n\n\n4\nFatima\n1.68\n75.0\nF\n\n\n\nNow, when entering data to RStudio it is not like Microsoft Excel where we enter data into the cells of a spreadsheet. In RStudio, data is entered as a sequence of elements and listed inside an object called a vector.\nFor instance, if we have three age values of 12, 57 and 26 years, and we want to enter this in RStudio in that order, we need to use the combine function c() and combine these three elements into a vector object. Hence, the code will be c(12, 57, 26). We can assign this data by typing this code as age &lt;- c(12, 57, 26). Any time you type age into RStudio’s console it will hence return these three values as a vector unless you chose to overwrite it with different information.\nLet us look at this more closely with the id variable in the above data. Each person has a unique ID number from 1 to 4. We are going to list the numbers 1, 2, 3 and 4 as a sequence of elements into a vector using the combine function c() and then assign it as a vector object calling it id.\n\n# Create 'id' vector object \nid &lt;- c(1, 2, 3, 4)\n\n# Type the vector object 'id' in console to see output or press the 'run' button\nid\n\n[1] 1 2 3 4\n\n\nNow, let us enter the information the same way for the remaining columns for name, height, weight and gender like we did for id:\n\n# Create 'name' vector object\nname &lt;- c(\"Kofi\", \"Harry\", \"Huijun\", \"Fatima\")\nname\n\n[1] \"Kofi\"   \"Harry\"  \"Huijun\" \"Fatima\"\n\n# Create 'height' (in meters) vector object\nheight &lt;- c(1.65, 1.77, 1.70, 1.68)\nheight\n\n[1] 1.65 1.77 1.70 1.68\n\n# Create 'weight' (in kg) vector object\nweight &lt;- c(64.2, 80.3, 58.7, 75.0)\nweight\n\n[1] 64.2 80.3 58.7 75.0\n\n# Create 'gender' vector object\ngender &lt;- c(\"M\", \"M\", \"F\", \"F\")\ngender\n\n[1] \"M\" \"M\" \"F\" \"F\"\n\n\nNow, that we have the vector objects ready. Let us bring them together to create a proper dataset. This new object is called a Data frame. We need to list the vectors inside the data.frame() function. For example:\n\n# Create a dataset (data frame)\ndataset &lt;- data.frame(id, name, height, weight, gender)\n\n# Type the data frame object 'dataset' in console to print the output in console\ndataset\n\n  id   name height weight gender\n1  1   Kofi   1.65   64.2      M\n2  2  Harry   1.77   80.3      M\n3  3 Huijun   1.70   58.7      F\n4  4 Fatima   1.68   75.0      F\n\n\nYou can also see dataset in a data viewer, by using the View() function to see in a spreadsheet:\n\nView(dataset)\n\n\n\n\n\n\n\nImportant\n\n\n\nThe column ‘id’ is a numeric variable with integers. The second column ‘name’ is a text variable with strings. The third & fourth columns ‘height’ and ‘weight’ are examples of numeric variables with real numbers with continuous measures. The variable ‘gender’ is a text variable with strings – however, this type of variable is classed as a categorical variable as individuals were categorised as either ‘M’ and ‘F’.",
    "crumbs": [
      "Core Content",
      "Week 1: Introduction I"
    ]
  },
  {
    "objectID": "02-basic_building_blocks.html#how-do-we-create-a-variable-based-on-other-existing-variables-within-a-data-frame-length-002100",
    "href": "02-basic_building_blocks.html#how-do-we-create-a-variable-based-on-other-existing-variables-within-a-data-frame-length-002100",
    "title": "Week 1: Introduction I",
    "section": "",
    "text": "[Watch on YouTube]\nTo access a variable by its name within a data frame, you will need to first type the name of the data frame followed by a $ (dollar sign), and then typing the variable’s name of interest. For instance, suppose you just want to see the height values in the Console viewer - you just type:\n\n# to access height - you need to type 'dataset$height'\ndataset$height\n\n[1] 1.65 1.77 1.70 1.68\n\n\nWe can use other columns or variables within our data frame to create another variable. This technique is essentially important when cleaning and managing data. From this dataset, it is possible to derive the body mass index bmi from height and weight using the formula:\n\n\\(BMI = weight/height^2\\)\n\nTo generate bmi into our data frame, we would need to access the height (m) and weight (kg) columns using the $ from the data frame its stored to, and apply the above formula as a code to generate the new bmi column:\n\n# Create 'bmi' in the data frame i.e.,'dataset' and calculate 'bmi'\n# using the $weight and $height\ndataset$bmi &lt;- dataset$weight/((dataset$height)^2)\n# View the data frame ‘dataset’ and you will see the new bmi variable inside\nView(dataset)\n\nYou can overwrite the height (m) column to change its units into centimeters by multiplying it to 100; equally, the weight (kg) column can be overwritten and converted from units of kilograms to grams by multiplying it to 1000.\n\n# using $height and *100 \ndataset$height &lt;- dataset$height*100\n# using $weight and *100\ndataset$weight &lt;- dataset$weight*1000\n# view updated dataset\ndataset\n\n  id   name height weight gender      bmi\n1  1   Kofi    165  64200      M 23.58127\n2  2  Harry    177  80300      M 25.63120\n3  3 Huijun    170  58700      F 20.31142\n4  4 Fatima    168  75000      F 26.57313",
    "crumbs": [
      "Core Content",
      "Week 1: Introduction I"
    ]
  },
  {
    "objectID": "02-basic_building_blocks.html#saving-your-r-script",
    "href": "02-basic_building_blocks.html#saving-your-r-script",
    "title": "Week 1: Introduction I",
    "section": "",
    "text": "Well done! You have learnt the basics of coding in RStudio - next week, we will learn further coding etiquettes for managing datasets which includes:\n\nImporting & exporting spreadsheets with RStudio\nSetting up work directories\nRow and column manipulations\nMerging two data frames.\n\nYou can now save your script for future use by clicking on the save icon in the menu bar. Saving an R-script for the first will prompt a Choose Encoding window to appear:\n\n\n\n\n\n\n\n\n\nSelect the default option UTF-8 (System default) and click OK and then save.",
    "crumbs": [
      "Core Content",
      "Week 1: Introduction I"
    ]
  },
  {
    "objectID": "01-what_is_rstudio.html",
    "href": "01-what_is_rstudio.html",
    "title": "What is RStudio (or R)?",
    "section": "",
    "text": "R (or RStudio) is a statistical programming package that allows users to carry out a wide range of statistical analyses. It can also function as GIS software, enabling various types of analysis on geographical data. In the same way, it can be used for data management and geoprocessing—for example, importing different types of data, whether non-spatial or spatial, and preparing them for analysis.\nThere are two versions:\n\n\n\n\n\n\n\n\n\nThe famous icon on the left is the version for R (Base), and the one on the right is the version for RStudio. Both software packages are the same. The only difference is that RStudio is attractive, intuitive, and more importantly, it is user-friendly than Base R. So, we will be using this version (i.e., RStudio) throughout this workshop.\nLet us talk about the installation of RStudio on your personal computer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nRStudio is an open-source software, it is free. It is widely recommended in data science, scientific research, and technical communication because it is easy to access, download, and install.\nTo use RStudio, you must first install R (Base) before installing RStudio. Follow the steps below to install both programs for your operating system (Windows or Mac).\nSteps\n\nDownload the R (Base) installer from the table below, then run the file to complete the installation.\nNext, download the RStudio installer from the table below, then run the file to complete the installation.\n\n\n\n\nOS User type\nR (Base)\nRStudio Desktop\n\n\n\n\nWindows\nR-4.5.1-win.exe\nRStudio-2025.09.0-387.exe\n\n\nMAC (Intel)\nR-4.5.1-x86_64.pkg\nRStudio-2025.09.0-387.dmg\n\n\nMAC (M1, M2 or M3)\nR-4.5.1-arm64.pkg\nRStudio-2025.09.0-387.dmg\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nR (Base) is very particular about the operating system! Please be sure to use the correct installer for your computer:\n\nWindows users: use the files in the first row.\nMac (Intel) users: use the files in the second row.\nMac (M1, M2, or M3 chip) users: use the files in the third row.\n\n\n\nThis section covers how to download and install RStudio for it to be used locally on your machine. There are several ways to access RStudio:\n\nDirectly through UCL Workstation PC\nRemotely through the UCL Desktop@Anywhere remote service with your laptop.\n\nLet us quickly go through these options.\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nGo to any PC cluster in UCL and simply log on to a computer. Use the UCL Applications tool to access the desired software.\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nTo use RStudio (or any other software which UCL provides as service to students) remotely from your laptop/PC/tablet, you can:\n\nGo to https://www.ucl.ac.uk/isd/services/computers/remote-access/desktopucl-anywhere.\nClick on the blue button that says: “Log in to Desktop @ UCL Anywhere.”\nYou will be prompted to enter your UCL username (username@ucl.ac.uk) and password. Enter the correct credentials to gain access to the remote portal.\nIf you see the option “Use Web Browser”, select it to run the remote functions directly in your browser without installing the Citrix Workspace application.\nYou should now see a Desktop @ UCL Anywhere button – click this button to complete the login. At this point, it will feel as though you are using a UCL workstation in a cluster room or library, but remotely.\nOn the desktop, click the UCL Applications icon and search for RStudio.\nSelect the latest version available (check the year) and open it with a single click (not multiple clicks).\n\nVoilà! You now have remote access to RStudio.",
    "crumbs": [
      "Getting Started",
      "What is RStudio (or R)?"
    ]
  },
  {
    "objectID": "01-what_is_rstudio.html#rstudio-and-downloading-it-length-001135",
    "href": "01-what_is_rstudio.html#rstudio-and-downloading-it-length-001135",
    "title": "What is RStudio (or R)?",
    "section": "",
    "text": "[Watch on YouTube]\nRStudio is an open-source software, it is free. It is widely recommended in data science, scientific research, and technical communication because it is easy to access, download, and install.\nTo use RStudio, you must first install R (Base) before installing RStudio. Follow the steps below to install both programs for your operating system (Windows or Mac).\nSteps\n\nDownload the R (Base) installer from the table below, then run the file to complete the installation.\nNext, download the RStudio installer from the table below, then run the file to complete the installation.\n\n\n\n\nOS User type\nR (Base)\nRStudio Desktop\n\n\n\n\nWindows\nR-4.5.1-win.exe\nRStudio-2025.09.0-387.exe\n\n\nMAC (Intel)\nR-4.5.1-x86_64.pkg\nRStudio-2025.09.0-387.dmg\n\n\nMAC (M1, M2 or M3)\nR-4.5.1-arm64.pkg\nRStudio-2025.09.0-387.dmg\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nR (Base) is very particular about the operating system! Please be sure to use the correct installer for your computer:\n\nWindows users: use the files in the first row.\nMac (Intel) users: use the files in the second row.\nMac (M1, M2, or M3 chip) users: use the files in the third row.\n\n\n\nThis section covers how to download and install RStudio for it to be used locally on your machine. There are several ways to access RStudio:\n\nDirectly through UCL Workstation PC\nRemotely through the UCL Desktop@Anywhere remote service with your laptop.\n\nLet us quickly go through these options.",
    "crumbs": [
      "Getting Started",
      "What is RStudio (or R)?"
    ]
  },
  {
    "objectID": "01-what_is_rstudio.html#access-through-workstation-ucl-applications-store-length-000500",
    "href": "01-what_is_rstudio.html#access-through-workstation-ucl-applications-store-length-000500",
    "title": "What is RStudio (or R)?",
    "section": "",
    "text": "[Watch on YouTube]\nGo to any PC cluster in UCL and simply log on to a computer. Use the UCL Applications tool to access the desired software.",
    "crumbs": [
      "Getting Started",
      "What is RStudio (or R)?"
    ]
  },
  {
    "objectID": "01-what_is_rstudio.html#remotely-access-through-ucl-desktopanywhere-length-000934",
    "href": "01-what_is_rstudio.html#remotely-access-through-ucl-desktopanywhere-length-000934",
    "title": "What is RStudio (or R)?",
    "section": "",
    "text": "[Watch on YouTube]\nTo use RStudio (or any other software which UCL provides as service to students) remotely from your laptop/PC/tablet, you can:\n\nGo to https://www.ucl.ac.uk/isd/services/computers/remote-access/desktopucl-anywhere.\nClick on the blue button that says: “Log in to Desktop @ UCL Anywhere.”\nYou will be prompted to enter your UCL username (username@ucl.ac.uk) and password. Enter the correct credentials to gain access to the remote portal.\nIf you see the option “Use Web Browser”, select it to run the remote functions directly in your browser without installing the Citrix Workspace application.\nYou should now see a Desktop @ UCL Anywhere button – click this button to complete the login. At this point, it will feel as though you are using a UCL workstation in a cluster room or library, but remotely.\nOn the desktop, click the UCL Applications icon and search for RStudio.\nSelect the latest version available (check the year) and open it with a single click (not multiple clicks).\n\nVoilà! You now have remote access to RStudio.",
    "crumbs": [
      "Getting Started",
      "What is RStudio (or R)?"
    ]
  },
  {
    "objectID": "03-handling_data_structures.html",
    "href": "03-handling_data_structures.html",
    "title": "Week 2: Introduction II",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nSet your working directory on Mac and Windows\nImport a .csv file into RStudio\nUnderstand the structure of a data frame (rows and columns)\nFilter data using both numeric and categorical variables\nCombine multiple conditions with logical operators to perform further filtering of data\nExport data back into a .csv file\n\nThese objectives, combined with those from last week, serve as a gateway to learning RStudio and building a strong foundation. Let us begin!\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\n\n\n\n\nWarning\n\n\n\nBefore we do anything - make sure to have downloaded the dataset for this computer session by clicking [HERE]. It contains the file Primary Schools in Ealing.csv - this comma separated values (CSV) file contains the data needed to follow today’s tutorial.\nInstructions In your computer, do the following:\n\nCreate a new folder on your desktop and rename the folder GEOG0186\nNext, create a new sub-folder within GEOG0186 and rename it as Week 2.\nFrom the downloaded folder Dataset for Week 2, make sure to unzip and transfer ALL the datasets directly to the Week 2 folder.\n\n\n\nThis part is probably the most important section of this tutorial. We are going to learn how to set the Work Directory. This basically refers to us connecting the RStudio to the folder containing our dataset that we want to import and analyse.\nDoing this allows the user to tell RStudio to open data from a folder located somewhere in our computer using something called the Path Location.\nThe Path Location specifies the whereabouts of the data file(s) stored within a computer. Setting your directory in RStudio, in code, beforehand makes life incredibly easier (than the usual point-and-click approach) in terms of finding, importing, exporting and saving data in and out of RStudio.\nTo illustrate what a Path Location is – suppose on my desktop’s dashboard on a Mac/Windows there is a folder called GEOG0186, and within that folder, exists another folder called Week 2. Finally, suppose a comma separated value (.csv) data file called Primary Schools in Ealing.csv is store in this folder i.e., Week 2. If via RStudio you want to open this CSV data file located in within the Week 2 folder. You will need to first set the path to Week 2 in RStudio using the setwd() function.\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nFor MAC users, the path location would be written as follows, \"/Users/accountName/Desktop/GEOG0186/Week 2\". You can access this piece of information simply by:\n\nOpen the folder GEOG0186\nRight-clicking on the folder Week 2 in which the files are stored. A drop-down scroll menu will appear (see image below).\n\n\n\n\n\n\n\n\n\n\n\nHold the Option (⌥) button on your keyboard down, and click Copy “Week 2” as Pathname\nPaste the copied path name into the function setwd() and run the code\n\nFor Mac, the setwd() is as follows:\n\n# set work directory in macs\nsetwd(\"/Users/accountName/Desktop/GEOG0186/Week 2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nFor Windows user (on UCL PCs), its marginally different - the path location to this folder on a Windows machine would be written as follows, \"N:/Desktop/GEOG00186/Week 2\". You can access this piece of information simply by:\n\nOpen the GEOG0186 folder to reveal the Week 2 folder.\nOpen the Week 2 folder where your downloaded data files have been stored.\nNow, click on the browser bar at the top which shows GEOG0186 &gt; Week 2. This should highlight and show \"N:\\Desktop\\GEOG0186\\Week 2\" (see image below).\n\n\n\n\n\n\n\n\n\n\n\nNow, copy \"N:\\Desktop\\GEOG0186\\Week 2\" and paste the path name into the setwd() function in your R script.\nLastly, change all the back slashes \\ in the path name to forward slashes / and run the code. It should look like this: setwd(\"N:/Desktop/GEOG0186/Week 2\").\n\nFor Windows, the setwd() is as follows:\n\n# set work directory in windows (using UCL PC)\nsetwd(\"N:/Desktop/GEOG0186/Week 2\")\n\n# note that on usual Windows OS on a personal computer - the code looks something like:\nsetwd(\"C:/Users/accountName/Desktop/GEOG0186/Week 2\")\n\nIf you type the code getwd() and quickly run it through console - if it returns the inputted path location - then you have done this correctly.\nNow, let us learn how to import a CSV data into RStudio.\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nA CSV file (short for Comma-Separated Values) is one of the most common formats for storing data. Each row is a single record or observation. For example, it can be details about a person, household unit, a city, or even disease events; and while each column is a variable (for example, population counts, temperature readings, or region name etc.).\nFor this exercise, we will be using the Primary Schools in Ealing.csv dataset to build intuition on understanding the basics of data structure.\nSince, we have already set the work directory to folder containing this dataset. Importing it into RStudio would be a piece of cake.\nTo read a CSV file into R, we can use the read.csv() function import it as a data frame object named school_data using the assignment operator (&lt;-):\n\n# Load data into RStudio. The spreadsheet is stored in the object called 'school_data'\nschool_data &lt;- read.csv(file = \"Primary Schools in Ealing.csv\")\n\nThe loaded dataset contains up to 58 primary schools with the following variables:\n\nSchoolName: Name of school in Ealing (character)\nType: School classified as a “Primary” school (character)\nNumberBoys: Total number of boys in a primary school (integer/)\nNumberGirls: Total number of girls in a primary school (integer)\nTotalStudents: Total number of students in a primary school (integer)\nOfstedGrade: Overall school performance where 1 = “excellent”, 2 = “good”, 3 = “requires improvement”, and 4 = “inadequate” (factor/categorical)\n\n\n\n\n\n\n\nImportant\n\n\n\nWe have covered a lot of the basics in RStudio - i.e., setting the work directory and importing a spreadsheet that is CSV format. The things shown in this section will be used frequently in all future tutorials. So, get used to using setwd() and read.csv().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nOne can examine the structure of the imported data with the following basic functions.\n\nstr(): tells the user which columns in the data frame are character or numeric variables, and so on.\nnames(): prints the entire names of the columns present in the data frame\nhead(): allows the user to see the first top 10 rows of the data frame\ntail(): allows the user to see the last bottom 10 rows of the data frame\nncol(): tells the user the total number of columns present in the data frame\nnrow(): tells the user the total number of observations (or rows) present in the data frame\ndim(): prints both the total number of observations (or rows) and columns present in the data frame.\n\nUsing the following code str() will display the structure of school_data data frame object and some of its contents:\n\nstr(school_data)\n\n'data.frame':   58 obs. of  6 variables:\n $ SchoolName   : chr  \"Berrymede Junior School\" \"East Acton Primary School\" \"Oldfield Primary School\" \"North Ealing Primary School\" ...\n $ Type         : chr  \"Primary\" \"Primary\" \"Primary\" \"Primary\" ...\n $ NumberBoys   : int  180 160 225 340 265 230 155 285 185 150 ...\n $ NumberGirls  : int  200 165 240 355 205 205 180 290 190 125 ...\n $ TotalStudents: int  377 329 465 697 471 435 335 573 373 273 ...\n $ OfstedGrade  : int  2 2 2 2 2 2 2 3 2 2 ...\n\n\nUsing the following code head() or tail() will display the first or last 10 observations, respectively, in the school_data data frame object:\n\nhead(school_data)\n\n                   SchoolName    Type NumberBoys NumberGirls TotalStudents\n1     Berrymede Junior School Primary        180         200           377\n2   East Acton Primary School Primary        160         165           329\n3     Oldfield Primary School Primary        225         240           465\n4 North Ealing Primary School Primary        340         355           697\n5    St John's Primary School Primary        265         205           471\n6    St Mark's Primary School Primary        230         205           435\n  OfstedGrade\n1           2\n2           2\n3           2\n4           2\n5           2\n6           2\n\ntail(school_data)\n\n                                       SchoolName    Type NumberBoys\n53           St Vincent's Catholic Primary School Primary        240\n54 Edward Betham Church of England Primary School Primary        230\n55                    Dormers Wells Junior School Primary        205\n56                          Grange Primary School Primary        415\n57                      Petts Hill Primary School Primary        125\n58                       Khalsa VA Primary School Primary        235\n   NumberGirls TotalStudents OfstedGrade\n53         260           498           2\n54         240           471           2\n55         200           409           3\n56         385           800           2\n57         130           254           2\n58         180           412           2\n\n\nYou can inspect the data frame object in a Data viewer by using the View() syntax. This should cause the Data viewer window to open showing the full dataset:\n\nView(school_data)\n\nThe dataset has 58 rows (primary school observations) and 6 columns (variables). You can use nrow, ncol and dim() functions separately to report the size of your data frame:\n\n# total number of rows\nnrow(school_data)\n\n[1] 58\n\n# total number of columns\nncol(school_data)\n\n[1] 6\n\n# full size of data frame i.e., total rows and columns\ndim(school_data)\n\n[1] 58  6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nOne can subset or restrict the data frame by specifying which row(s) and column(s) to keep or discard using this standard square bracket syntax dataframe[Row, Column].\nBreakdown of dataframe[Row, Column]code:\n\nThe bit in the code that corresponds to dataframe from dataframe[Row, Column] represents the name data frame object.\nWhere it states Row within the square brackets specifies the subsetting, or filter action to be carried out based on rows.\nWhere it states Column within the square brackets specifies the subsetting, or filter action to be carried out based on columns.\n\nLet’s take our school_data data frame as an example - if we select the first row (row number 1) and the first column (SchoolName), we have essentially filter out only the school name Berrymede Junior School Primary:\n\ndata11 &lt;- school_data[1, 1]\ndata11\n\n[1] \"Berrymede Junior School\"\n\n\nIf you select only the first row (row number 1) and do not specific any numbers for the columns, you will be filtering out the entire row for row number 1:\n\nrow1 &lt;- school_data[1, ]\nrow1\n\n               SchoolName    Type NumberBoys NumberGirls TotalStudents\n1 Berrymede Junior School Primary        180         200           377\n  OfstedGrade\n1           2\n\n\nIn the same vein, if you select only the column and do not specific any numbers for the rows, you will be filtering out the entire column (i.e., SchoolName variable) for column number 1 which is the list of all primary school names:\n\ncolumn1 &lt;- school_data[, 1]\ncolumn1\n\n [1] \"Berrymede Junior School\"                           \n [2] \"East Acton Primary School\"                         \n [3] \"Oldfield Primary School\"                           \n [4] \"North Ealing Primary School\"                       \n [5] \"St John's Primary School\"                          \n [6] \"St Mark's Primary School\"                          \n [7] \"West Twyford Primary School\"                       \n [8] \"West Acton Primary School\"                         \n [9] \"Mayfield Primary School\"                           \n[10] \"Beaconsfield Primary and Nursery School\"           \n[11] \"Coston Primary School\"                             \n[12] \"Downe Manor Primary School\"                        \n[13] \"Drayton Green Primary School\"                      \n[14] \"North Primary School\"                              \n[15] \"Ravenor Primary School\"                            \n[16] \"Selborne Primary School\"                           \n[17] \"Hambrough Primary School\"                          \n[18] \"Hobbayne Primary School\"                           \n[19] \"John Perryn Primary School\"                        \n[20] \"Southfield Primary School\"                         \n[21] \"Allenby Primary School\"                            \n[22] \"Blair Peach Primary School\"                        \n[23] \"Clifton Primary School\"                            \n[24] \"Dairy Meadow Primary School\"                       \n[25] \"Derwentwater Primary School\"                       \n[26] \"Durdans Park Primary School\"                       \n[27] \"Fielding Primary School\"                           \n[28] \"Gifford Primary School\"                            \n[29] \"Greenwood Primary School\"                          \n[30] \"Havelock Primary School and Nursery\"               \n[31] \"Horsenden Primary School\"                          \n[32] \"Willow Tree Primary School\"                        \n[33] \"Lady Margaret Primary School\"                      \n[34] \"Little Ealing Primary School\"                      \n[35] \"Oaklands Primary School\"                           \n[36] \"Perivale Primary School\"                           \n[37] \"Stanhope Primary School\"                           \n[38] \"Viking Primary School\"                             \n[39] \"Wolf Fields Primary School\"                        \n[40] \"Featherstone Primary and Nursery School\"           \n[41] \"Three Bridges Primary School\"                      \n[42] \"Montpelier Primary School\"                         \n[43] \"Tudor Primary School\"                              \n[44] \"Hathaway Primary School\"                           \n[45] \"Vicar's Green Primary School\"                      \n[46] \"Mount Carmel Catholic Primary School\"              \n[47] \"Our Lady of the Visitation Catholic Primary School\"\n[48] \"St John Fisher Catholic Primary School\"            \n[49] \"St Anselm's Catholic Primary School\"               \n[50] \"St Gregory's Catholic Primary School\"              \n[51] \"St Joseph's Catholic Primary School\"               \n[52] \"St Raphael's Catholic Primary School\"              \n[53] \"St Vincent's Catholic Primary School\"              \n[54] \"Edward Betham Church of England Primary School\"    \n[55] \"Dormers Wells Junior School\"                       \n[56] \"Grange Primary School\"                             \n[57] \"Petts Hill Primary School\"                         \n[58] \"Khalsa VA Primary School\"                          \n\n\nWhat if we wanted to filter the following rows numbered 2, 7 and 19 from the school_data data frame object as highlighted in the image below?\n\n\n\n\n\n\n\n\n\nWe can use the combine function i.e., c() to list those numbers in the square brackets:\n\nrows_filter &lt;- school_data[c(2, 7, 19), ]\nrows_filter\n\n                    SchoolName    Type NumberBoys NumberGirls TotalStudents\n2    East Acton Primary School Primary        160         165           329\n7  West Twyford Primary School Primary        155         180           335\n19  John Perryn Primary School Primary        230         230           460\n   OfstedGrade\n2            2\n7            2\n19           2\n\n\nLikewise, what if we wanted to filter the following columns numbered 1, 5 and 6 from the school_data data frame object as highlighted in the image below?\n\n\n\n\n\n\n\n\n\nWe can also use the combine function i.e., c() to list those numbers for the columns within that square brackets:\n\ncolumns_filter &lt;- school_data[ , c(1, 5, 6)]\ncolumns_filter\n\n                                           SchoolName TotalStudents OfstedGrade\n1                             Berrymede Junior School           377           2\n2                           East Acton Primary School           329           2\n3                             Oldfield Primary School           465           2\n4                         North Ealing Primary School           697           2\n5                            St John's Primary School           471           2\n6                            St Mark's Primary School           435           2\n7                         West Twyford Primary School           335           2\n8                           West Acton Primary School           573           3\n9                             Mayfield Primary School           373           2\n10            Beaconsfield Primary and Nursery School           273           2\n11                              Coston Primary School           450           2\n12                         Downe Manor Primary School           471           2\n13                       Drayton Green Primary School           354           2\n14                               North Primary School           419           1\n15                             Ravenor Primary School           610           2\n16                            Selborne Primary School           554           2\n17                           Hambrough Primary School           520           2\n18                            Hobbayne Primary School           641           2\n19                         John Perryn Primary School           460           2\n20                          Southfield Primary School           518           2\n21                             Allenby Primary School           263           2\n22                         Blair Peach Primary School           506           2\n23                             Clifton Primary School           400           2\n24                        Dairy Meadow Primary School           469           2\n25                        Derwentwater Primary School           708           2\n26                        Durdans Park Primary School           528           2\n27                            Fielding Primary School           891           2\n28                             Gifford Primary School           866           1\n29                           Greenwood Primary School           555           2\n30                Havelock Primary School and Nursery           434           1\n31                           Horsenden Primary School           872           2\n32                         Willow Tree Primary School           759           2\n33                       Lady Margaret Primary School           671           2\n34                       Little Ealing Primary School           669           2\n35                            Oaklands Primary School           552           2\n36                            Perivale Primary School           467           2\n37                            Stanhope Primary School           598           2\n38                              Viking Primary School           275           2\n39                         Wolf Fields Primary School           421           2\n40            Featherstone Primary and Nursery School           722           2\n41                       Three Bridges Primary School           455           2\n42                          Montpelier Primary School           682           1\n43                               Tudor Primary School           453           2\n44                            Hathaway Primary School           346           4\n45                       Vicar's Green Primary School           342           1\n46               Mount Carmel Catholic Primary School           457           2\n47 Our Lady of the Visitation Catholic Primary School           471           3\n48             St John Fisher Catholic Primary School           467           1\n49                St Anselm's Catholic Primary School           239           3\n50               St Gregory's Catholic Primary School           619           2\n51                St Joseph's Catholic Primary School           558           2\n52               St Raphael's Catholic Primary School           589           2\n53               St Vincent's Catholic Primary School           498           2\n54     Edward Betham Church of England Primary School           471           2\n55                        Dormers Wells Junior School           409           3\n56                              Grange Primary School           800           2\n57                          Petts Hill Primary School           254           2\n58                           Khalsa VA Primary School           412           2\n\n\nWhat if we wanted to filter on rows numbered 2, 7 and 19, as well as columns numbered 1, 5 and 6 from that school_data data frame object as highlighted in the image below?\n\n\n\n\n\n\n\n\n\n\nfull_filter &lt;- school_data[c(2, 7, 19) , c(1, 5, 6)]\nfull_filter\n\n                    SchoolName TotalStudents OfstedGrade\n2    East Acton Primary School           329           2\n7  West Twyford Primary School           335           2\n19  John Perryn Primary School           460           2\n\n\nNot too shabby! So far, you have been shown how to do some create subset of data by filtering rows and columns. Let’s take it up a notch on filtering based on row Logical Operators to create conditions.\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nOften, we want to filter based on more than one condition — for example, “cities in England and with a population over 600000”. RStudio uses logical operators to combine these conditions:\n\n\n\n\n\n\n\n\n\nOperator\nMeaning\nExample\nDescription\n\n\n\n\n==\nEqual to\ncountry == \"England\"\nTrue if the country is England\n\n\n!=\nNot Equal to\ncountry != \"England\"\nTrue if for all except England\n\n\n&gt;\nGreater than\npopulation &gt; 600000\nTrue if population is above 600000\n\n\n&lt;\nLess than\npopulation &lt; 600000\nTrue if population is under 600000\n\n\n&gt;=\nGreater than or eqaul to\npopulation &gt;= 600000\nTrue if population is equal to 600000 or more\n\n\n&lt;=\nLess than or equal to\npopulation &lt;= 600000\nTrue if population is equal to 600000 or less\n\n\n&\nAND\ncountry == \"England\" & population &lt;= 600000\nBoth conditions are true\n\n\n|\nOR\ncountry == \"England\" | country == \"Wales\"\nTrue for either England or Wales\n\n\n\nLet’s illustrate with the school_data object. We are interested in primary schools with a total of 500 hundred or more students. This operation is always row-based. We will need to use the $ to call the column TotalStudents and the logical operator for this filter is &gt;=:\n\nschools_500plus &lt;- school_data[school_data$TotalStudents &gt;= 500, ]\nschools_500plus\n\n                                SchoolName    Type NumberBoys NumberGirls\n4              North Ealing Primary School Primary        340         355\n8                West Acton Primary School Primary        285         290\n15                  Ravenor Primary School Primary        315         295\n16                 Selborne Primary School Primary        275         280\n17                Hambrough Primary School Primary        275         245\n18                 Hobbayne Primary School Primary        340         300\n20               Southfield Primary School Primary        275         245\n22              Blair Peach Primary School Primary        265         245\n25             Derwentwater Primary School Primary        335         370\n26             Durdans Park Primary School Primary        275         250\n27                 Fielding Primary School Primary        480         410\n28                  Gifford Primary School Primary        435         435\n29                Greenwood Primary School Primary        290         265\n31                Horsenden Primary School Primary        440         435\n32              Willow Tree Primary School Primary        385         370\n33            Lady Margaret Primary School Primary        360         310\n34            Little Ealing Primary School Primary        345         325\n35                 Oaklands Primary School Primary        290         260\n37                 Stanhope Primary School Primary        305         295\n40 Featherstone Primary and Nursery School Primary        365         360\n42               Montpelier Primary School Primary        350         330\n50    St Gregory's Catholic Primary School Primary        305         310\n51     St Joseph's Catholic Primary School Primary        295         265\n52    St Raphael's Catholic Primary School Primary        325         265\n56                   Grange Primary School Primary        415         385\n   TotalStudents OfstedGrade\n4            697           2\n8            573           3\n15           610           2\n16           554           2\n17           520           2\n18           641           2\n20           518           2\n22           506           2\n25           708           2\n26           528           2\n27           891           2\n28           866           1\n29           555           2\n31           872           2\n32           759           2\n33           671           2\n34           669           2\n35           552           2\n37           598           2\n40           722           2\n42           682           1\n50           619           2\n51           558           2\n52           589           2\n56           800           2\n\n\nWhat if were interested in primary schools with an OFSTED score of 1 (Excellent). Again, this operation is row-based and so we will need to use the $ to call the column OfstedGrade and the logical operator for this filter is ==:\n\nschools_ofsted1 &lt;- school_data[school_data$OfstedGrade == 1, ]\nschools_ofsted1\n\n                               SchoolName    Type NumberBoys NumberGirls\n14                   North Primary School Primary        215         205\n28                 Gifford Primary School Primary        435         435\n30    Havelock Primary School and Nursery Primary        245         190\n42              Montpelier Primary School Primary        350         330\n45           Vicar's Green Primary School Primary        165         180\n48 St John Fisher Catholic Primary School Primary        225         240\n   TotalStudents OfstedGrade\n14           419           1\n28           866           1\n30           434           1\n42           682           1\n45           342           1\n48           467           1\n\n\nHere is an example of combining such logical operators - select schools that have more than 650 students and with an OFSTED score of 2.\n\n# logical operators is &gt;, &, ==\nschools_650_ofsted2 &lt;- school_data[school_data$TotalStudents &gt; 650 & school_data$OfstedGrade == 2, ]\nschools_650_ofsted2\n\n                                SchoolName    Type NumberBoys NumberGirls\n4              North Ealing Primary School Primary        340         355\n25             Derwentwater Primary School Primary        335         370\n27                 Fielding Primary School Primary        480         410\n31                Horsenden Primary School Primary        440         435\n32              Willow Tree Primary School Primary        385         370\n33            Lady Margaret Primary School Primary        360         310\n34            Little Ealing Primary School Primary        345         325\n40 Featherstone Primary and Nursery School Primary        365         360\n56                   Grange Primary School Primary        415         385\n   TotalStudents OfstedGrade\n4            697           2\n25           708           2\n27           891           2\n31           872           2\n32           759           2\n33           671           2\n34           669           2\n40           722           2\n56           800           2\n\n\nI am sure you now get the gist of what is happening here with these logical statements for subsetting, or filtering data accordingly. We are now in the final stretch - let us learn how to save and export a dataset as a CSV spreadsheet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nThis operation is very easy to perform. Once you have created your filtered data, you can use the following function write.csv() to save it as a new CSV file:\n\nwrite.csv(schools_650_ofsted2, file = \"Filtered_example_data.csv\", row.names = FALSE)\n\nThe above syntax should save a new CSV file named as Filtered_example_data.csv. Do not forget to save your R-script!\n\n\n\nYou have learned the following:\n\nSet your working directory on Mac and Windows\nImport a .csv file into RStudio\nUnderstand the structure of a data frame (rows and columns)\nFilter data using both numeric and categorical variables\nCombine multiple conditions with logical operators to perform further filtering of data\nExport data back into a .csv file",
    "crumbs": [
      "Core Content",
      "Week 2: Introduction II"
    ]
  },
  {
    "objectID": "03-handling_data_structures.html#learning-objectives",
    "href": "03-handling_data_structures.html#learning-objectives",
    "title": "Week 2: Introduction II",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nSet your working directory on Mac and Windows\nImport a .csv file into RStudio\nUnderstand the structure of a data frame (rows and columns)\nFilter data using both numeric and categorical variables\nCombine multiple conditions with logical operators to perform further filtering of data\nExport data back into a .csv file\n\nThese objectives, combined with those from last week, serve as a gateway to learning RStudio and building a strong foundation. Let us begin!",
    "crumbs": [
      "Core Content",
      "Week 2: Introduction II"
    ]
  },
  {
    "objectID": "03-handling_data_structures.html#setting-the-working-directory-to-datasets-length-000524",
    "href": "03-handling_data_structures.html#setting-the-working-directory-to-datasets-length-000524",
    "title": "Week 2: Introduction II",
    "section": "",
    "text": "[Watch on YouTube]\n\n\n\n\n\n\nWarning\n\n\n\nBefore we do anything - make sure to have downloaded the dataset for this computer session by clicking [HERE]. It contains the file Primary Schools in Ealing.csv - this comma separated values (CSV) file contains the data needed to follow today’s tutorial.\nInstructions In your computer, do the following:\n\nCreate a new folder on your desktop and rename the folder GEOG0186\nNext, create a new sub-folder within GEOG0186 and rename it as Week 2.\nFrom the downloaded folder Dataset for Week 2, make sure to unzip and transfer ALL the datasets directly to the Week 2 folder.\n\n\n\nThis part is probably the most important section of this tutorial. We are going to learn how to set the Work Directory. This basically refers to us connecting the RStudio to the folder containing our dataset that we want to import and analyse.\nDoing this allows the user to tell RStudio to open data from a folder located somewhere in our computer using something called the Path Location.\nThe Path Location specifies the whereabouts of the data file(s) stored within a computer. Setting your directory in RStudio, in code, beforehand makes life incredibly easier (than the usual point-and-click approach) in terms of finding, importing, exporting and saving data in and out of RStudio.\nTo illustrate what a Path Location is – suppose on my desktop’s dashboard on a Mac/Windows there is a folder called GEOG0186, and within that folder, exists another folder called Week 2. Finally, suppose a comma separated value (.csv) data file called Primary Schools in Ealing.csv is store in this folder i.e., Week 2. If via RStudio you want to open this CSV data file located in within the Week 2 folder. You will need to first set the path to Week 2 in RStudio using the setwd() function.\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nFor MAC users, the path location would be written as follows, \"/Users/accountName/Desktop/GEOG0186/Week 2\". You can access this piece of information simply by:\n\nOpen the folder GEOG0186\nRight-clicking on the folder Week 2 in which the files are stored. A drop-down scroll menu will appear (see image below).\n\n\n\n\n\n\n\n\n\n\n\nHold the Option (⌥) button on your keyboard down, and click Copy “Week 2” as Pathname\nPaste the copied path name into the function setwd() and run the code\n\nFor Mac, the setwd() is as follows:\n\n# set work directory in macs\nsetwd(\"/Users/accountName/Desktop/GEOG0186/Week 2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nFor Windows user (on UCL PCs), its marginally different - the path location to this folder on a Windows machine would be written as follows, \"N:/Desktop/GEOG00186/Week 2\". You can access this piece of information simply by:\n\nOpen the GEOG0186 folder to reveal the Week 2 folder.\nOpen the Week 2 folder where your downloaded data files have been stored.\nNow, click on the browser bar at the top which shows GEOG0186 &gt; Week 2. This should highlight and show \"N:\\Desktop\\GEOG0186\\Week 2\" (see image below).\n\n\n\n\n\n\n\n\n\n\n\nNow, copy \"N:\\Desktop\\GEOG0186\\Week 2\" and paste the path name into the setwd() function in your R script.\nLastly, change all the back slashes \\ in the path name to forward slashes / and run the code. It should look like this: setwd(\"N:/Desktop/GEOG0186/Week 2\").\n\nFor Windows, the setwd() is as follows:\n\n# set work directory in windows (using UCL PC)\nsetwd(\"N:/Desktop/GEOG0186/Week 2\")\n\n# note that on usual Windows OS on a personal computer - the code looks something like:\nsetwd(\"C:/Users/accountName/Desktop/GEOG0186/Week 2\")\n\nIf you type the code getwd() and quickly run it through console - if it returns the inputted path location - then you have done this correctly.\nNow, let us learn how to import a CSV data into RStudio.",
    "crumbs": [
      "Core Content",
      "Week 2: Introduction II"
    ]
  },
  {
    "objectID": "03-handling_data_structures.html#how-to-import-a-dataset-into-rstudio-length-000621",
    "href": "03-handling_data_structures.html#how-to-import-a-dataset-into-rstudio-length-000621",
    "title": "Week 2: Introduction II",
    "section": "",
    "text": "[Watch on YouTube]\nA CSV file (short for Comma-Separated Values) is one of the most common formats for storing data. Each row is a single record or observation. For example, it can be details about a person, household unit, a city, or even disease events; and while each column is a variable (for example, population counts, temperature readings, or region name etc.).\nFor this exercise, we will be using the Primary Schools in Ealing.csv dataset to build intuition on understanding the basics of data structure.\nSince, we have already set the work directory to folder containing this dataset. Importing it into RStudio would be a piece of cake.\nTo read a CSV file into R, we can use the read.csv() function import it as a data frame object named school_data using the assignment operator (&lt;-):\n\n# Load data into RStudio. The spreadsheet is stored in the object called 'school_data'\nschool_data &lt;- read.csv(file = \"Primary Schools in Ealing.csv\")\n\nThe loaded dataset contains up to 58 primary schools with the following variables:\n\nSchoolName: Name of school in Ealing (character)\nType: School classified as a “Primary” school (character)\nNumberBoys: Total number of boys in a primary school (integer/)\nNumberGirls: Total number of girls in a primary school (integer)\nTotalStudents: Total number of students in a primary school (integer)\nOfstedGrade: Overall school performance where 1 = “excellent”, 2 = “good”, 3 = “requires improvement”, and 4 = “inadequate” (factor/categorical)\n\n\n\n\n\n\n\nImportant\n\n\n\nWe have covered a lot of the basics in RStudio - i.e., setting the work directory and importing a spreadsheet that is CSV format. The things shown in this section will be used frequently in all future tutorials. So, get used to using setwd() and read.csv().",
    "crumbs": [
      "Core Content",
      "Week 2: Introduction II"
    ]
  },
  {
    "objectID": "03-handling_data_structures.html#data-structure",
    "href": "03-handling_data_structures.html#data-structure",
    "title": "Week 2: Introduction II",
    "section": "",
    "text": "[Watch on YouTube]\nOne can examine the structure of the imported data with the following basic functions.\n\nstr(): tells the user which columns in the data frame are character or numeric variables, and so on.\nnames(): prints the entire names of the columns present in the data frame\nhead(): allows the user to see the first top 10 rows of the data frame\ntail(): allows the user to see the last bottom 10 rows of the data frame\nncol(): tells the user the total number of columns present in the data frame\nnrow(): tells the user the total number of observations (or rows) present in the data frame\ndim(): prints both the total number of observations (or rows) and columns present in the data frame.\n\nUsing the following code str() will display the structure of school_data data frame object and some of its contents:\n\nstr(school_data)\n\n'data.frame':   58 obs. of  6 variables:\n $ SchoolName   : chr  \"Berrymede Junior School\" \"East Acton Primary School\" \"Oldfield Primary School\" \"North Ealing Primary School\" ...\n $ Type         : chr  \"Primary\" \"Primary\" \"Primary\" \"Primary\" ...\n $ NumberBoys   : int  180 160 225 340 265 230 155 285 185 150 ...\n $ NumberGirls  : int  200 165 240 355 205 205 180 290 190 125 ...\n $ TotalStudents: int  377 329 465 697 471 435 335 573 373 273 ...\n $ OfstedGrade  : int  2 2 2 2 2 2 2 3 2 2 ...\n\n\nUsing the following code head() or tail() will display the first or last 10 observations, respectively, in the school_data data frame object:\n\nhead(school_data)\n\n                   SchoolName    Type NumberBoys NumberGirls TotalStudents\n1     Berrymede Junior School Primary        180         200           377\n2   East Acton Primary School Primary        160         165           329\n3     Oldfield Primary School Primary        225         240           465\n4 North Ealing Primary School Primary        340         355           697\n5    St John's Primary School Primary        265         205           471\n6    St Mark's Primary School Primary        230         205           435\n  OfstedGrade\n1           2\n2           2\n3           2\n4           2\n5           2\n6           2\n\ntail(school_data)\n\n                                       SchoolName    Type NumberBoys\n53           St Vincent's Catholic Primary School Primary        240\n54 Edward Betham Church of England Primary School Primary        230\n55                    Dormers Wells Junior School Primary        205\n56                          Grange Primary School Primary        415\n57                      Petts Hill Primary School Primary        125\n58                       Khalsa VA Primary School Primary        235\n   NumberGirls TotalStudents OfstedGrade\n53         260           498           2\n54         240           471           2\n55         200           409           3\n56         385           800           2\n57         130           254           2\n58         180           412           2\n\n\nYou can inspect the data frame object in a Data viewer by using the View() syntax. This should cause the Data viewer window to open showing the full dataset:\n\nView(school_data)\n\nThe dataset has 58 rows (primary school observations) and 6 columns (variables). You can use nrow, ncol and dim() functions separately to report the size of your data frame:\n\n# total number of rows\nnrow(school_data)\n\n[1] 58\n\n# total number of columns\nncol(school_data)\n\n[1] 6\n\n# full size of data frame i.e., total rows and columns\ndim(school_data)\n\n[1] 58  6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nOne can subset or restrict the data frame by specifying which row(s) and column(s) to keep or discard using this standard square bracket syntax dataframe[Row, Column].\nBreakdown of dataframe[Row, Column]code:\n\nThe bit in the code that corresponds to dataframe from dataframe[Row, Column] represents the name data frame object.\nWhere it states Row within the square brackets specifies the subsetting, or filter action to be carried out based on rows.\nWhere it states Column within the square brackets specifies the subsetting, or filter action to be carried out based on columns.\n\nLet’s take our school_data data frame as an example - if we select the first row (row number 1) and the first column (SchoolName), we have essentially filter out only the school name Berrymede Junior School Primary:\n\ndata11 &lt;- school_data[1, 1]\ndata11\n\n[1] \"Berrymede Junior School\"\n\n\nIf you select only the first row (row number 1) and do not specific any numbers for the columns, you will be filtering out the entire row for row number 1:\n\nrow1 &lt;- school_data[1, ]\nrow1\n\n               SchoolName    Type NumberBoys NumberGirls TotalStudents\n1 Berrymede Junior School Primary        180         200           377\n  OfstedGrade\n1           2\n\n\nIn the same vein, if you select only the column and do not specific any numbers for the rows, you will be filtering out the entire column (i.e., SchoolName variable) for column number 1 which is the list of all primary school names:\n\ncolumn1 &lt;- school_data[, 1]\ncolumn1\n\n [1] \"Berrymede Junior School\"                           \n [2] \"East Acton Primary School\"                         \n [3] \"Oldfield Primary School\"                           \n [4] \"North Ealing Primary School\"                       \n [5] \"St John's Primary School\"                          \n [6] \"St Mark's Primary School\"                          \n [7] \"West Twyford Primary School\"                       \n [8] \"West Acton Primary School\"                         \n [9] \"Mayfield Primary School\"                           \n[10] \"Beaconsfield Primary and Nursery School\"           \n[11] \"Coston Primary School\"                             \n[12] \"Downe Manor Primary School\"                        \n[13] \"Drayton Green Primary School\"                      \n[14] \"North Primary School\"                              \n[15] \"Ravenor Primary School\"                            \n[16] \"Selborne Primary School\"                           \n[17] \"Hambrough Primary School\"                          \n[18] \"Hobbayne Primary School\"                           \n[19] \"John Perryn Primary School\"                        \n[20] \"Southfield Primary School\"                         \n[21] \"Allenby Primary School\"                            \n[22] \"Blair Peach Primary School\"                        \n[23] \"Clifton Primary School\"                            \n[24] \"Dairy Meadow Primary School\"                       \n[25] \"Derwentwater Primary School\"                       \n[26] \"Durdans Park Primary School\"                       \n[27] \"Fielding Primary School\"                           \n[28] \"Gifford Primary School\"                            \n[29] \"Greenwood Primary School\"                          \n[30] \"Havelock Primary School and Nursery\"               \n[31] \"Horsenden Primary School\"                          \n[32] \"Willow Tree Primary School\"                        \n[33] \"Lady Margaret Primary School\"                      \n[34] \"Little Ealing Primary School\"                      \n[35] \"Oaklands Primary School\"                           \n[36] \"Perivale Primary School\"                           \n[37] \"Stanhope Primary School\"                           \n[38] \"Viking Primary School\"                             \n[39] \"Wolf Fields Primary School\"                        \n[40] \"Featherstone Primary and Nursery School\"           \n[41] \"Three Bridges Primary School\"                      \n[42] \"Montpelier Primary School\"                         \n[43] \"Tudor Primary School\"                              \n[44] \"Hathaway Primary School\"                           \n[45] \"Vicar's Green Primary School\"                      \n[46] \"Mount Carmel Catholic Primary School\"              \n[47] \"Our Lady of the Visitation Catholic Primary School\"\n[48] \"St John Fisher Catholic Primary School\"            \n[49] \"St Anselm's Catholic Primary School\"               \n[50] \"St Gregory's Catholic Primary School\"              \n[51] \"St Joseph's Catholic Primary School\"               \n[52] \"St Raphael's Catholic Primary School\"              \n[53] \"St Vincent's Catholic Primary School\"              \n[54] \"Edward Betham Church of England Primary School\"    \n[55] \"Dormers Wells Junior School\"                       \n[56] \"Grange Primary School\"                             \n[57] \"Petts Hill Primary School\"                         \n[58] \"Khalsa VA Primary School\"                          \n\n\nWhat if we wanted to filter the following rows numbered 2, 7 and 19 from the school_data data frame object as highlighted in the image below?\n\n\n\n\n\n\n\n\n\nWe can use the combine function i.e., c() to list those numbers in the square brackets:\n\nrows_filter &lt;- school_data[c(2, 7, 19), ]\nrows_filter\n\n                    SchoolName    Type NumberBoys NumberGirls TotalStudents\n2    East Acton Primary School Primary        160         165           329\n7  West Twyford Primary School Primary        155         180           335\n19  John Perryn Primary School Primary        230         230           460\n   OfstedGrade\n2            2\n7            2\n19           2\n\n\nLikewise, what if we wanted to filter the following columns numbered 1, 5 and 6 from the school_data data frame object as highlighted in the image below?\n\n\n\n\n\n\n\n\n\nWe can also use the combine function i.e., c() to list those numbers for the columns within that square brackets:\n\ncolumns_filter &lt;- school_data[ , c(1, 5, 6)]\ncolumns_filter\n\n                                           SchoolName TotalStudents OfstedGrade\n1                             Berrymede Junior School           377           2\n2                           East Acton Primary School           329           2\n3                             Oldfield Primary School           465           2\n4                         North Ealing Primary School           697           2\n5                            St John's Primary School           471           2\n6                            St Mark's Primary School           435           2\n7                         West Twyford Primary School           335           2\n8                           West Acton Primary School           573           3\n9                             Mayfield Primary School           373           2\n10            Beaconsfield Primary and Nursery School           273           2\n11                              Coston Primary School           450           2\n12                         Downe Manor Primary School           471           2\n13                       Drayton Green Primary School           354           2\n14                               North Primary School           419           1\n15                             Ravenor Primary School           610           2\n16                            Selborne Primary School           554           2\n17                           Hambrough Primary School           520           2\n18                            Hobbayne Primary School           641           2\n19                         John Perryn Primary School           460           2\n20                          Southfield Primary School           518           2\n21                             Allenby Primary School           263           2\n22                         Blair Peach Primary School           506           2\n23                             Clifton Primary School           400           2\n24                        Dairy Meadow Primary School           469           2\n25                        Derwentwater Primary School           708           2\n26                        Durdans Park Primary School           528           2\n27                            Fielding Primary School           891           2\n28                             Gifford Primary School           866           1\n29                           Greenwood Primary School           555           2\n30                Havelock Primary School and Nursery           434           1\n31                           Horsenden Primary School           872           2\n32                         Willow Tree Primary School           759           2\n33                       Lady Margaret Primary School           671           2\n34                       Little Ealing Primary School           669           2\n35                            Oaklands Primary School           552           2\n36                            Perivale Primary School           467           2\n37                            Stanhope Primary School           598           2\n38                              Viking Primary School           275           2\n39                         Wolf Fields Primary School           421           2\n40            Featherstone Primary and Nursery School           722           2\n41                       Three Bridges Primary School           455           2\n42                          Montpelier Primary School           682           1\n43                               Tudor Primary School           453           2\n44                            Hathaway Primary School           346           4\n45                       Vicar's Green Primary School           342           1\n46               Mount Carmel Catholic Primary School           457           2\n47 Our Lady of the Visitation Catholic Primary School           471           3\n48             St John Fisher Catholic Primary School           467           1\n49                St Anselm's Catholic Primary School           239           3\n50               St Gregory's Catholic Primary School           619           2\n51                St Joseph's Catholic Primary School           558           2\n52               St Raphael's Catholic Primary School           589           2\n53               St Vincent's Catholic Primary School           498           2\n54     Edward Betham Church of England Primary School           471           2\n55                        Dormers Wells Junior School           409           3\n56                              Grange Primary School           800           2\n57                          Petts Hill Primary School           254           2\n58                           Khalsa VA Primary School           412           2\n\n\nWhat if we wanted to filter on rows numbered 2, 7 and 19, as well as columns numbered 1, 5 and 6 from that school_data data frame object as highlighted in the image below?\n\n\n\n\n\n\n\n\n\n\nfull_filter &lt;- school_data[c(2, 7, 19) , c(1, 5, 6)]\nfull_filter\n\n                    SchoolName TotalStudents OfstedGrade\n2    East Acton Primary School           329           2\n7  West Twyford Primary School           335           2\n19  John Perryn Primary School           460           2\n\n\nNot too shabby! So far, you have been shown how to do some create subset of data by filtering rows and columns. Let’s take it up a notch on filtering based on row Logical Operators to create conditions.\n\n\n\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nOften, we want to filter based on more than one condition — for example, “cities in England and with a population over 600000”. RStudio uses logical operators to combine these conditions:\n\n\n\n\n\n\n\n\n\nOperator\nMeaning\nExample\nDescription\n\n\n\n\n==\nEqual to\ncountry == \"England\"\nTrue if the country is England\n\n\n!=\nNot Equal to\ncountry != \"England\"\nTrue if for all except England\n\n\n&gt;\nGreater than\npopulation &gt; 600000\nTrue if population is above 600000\n\n\n&lt;\nLess than\npopulation &lt; 600000\nTrue if population is under 600000\n\n\n&gt;=\nGreater than or eqaul to\npopulation &gt;= 600000\nTrue if population is equal to 600000 or more\n\n\n&lt;=\nLess than or equal to\npopulation &lt;= 600000\nTrue if population is equal to 600000 or less\n\n\n&\nAND\ncountry == \"England\" & population &lt;= 600000\nBoth conditions are true\n\n\n|\nOR\ncountry == \"England\" | country == \"Wales\"\nTrue for either England or Wales\n\n\n\nLet’s illustrate with the school_data object. We are interested in primary schools with a total of 500 hundred or more students. This operation is always row-based. We will need to use the $ to call the column TotalStudents and the logical operator for this filter is &gt;=:\n\nschools_500plus &lt;- school_data[school_data$TotalStudents &gt;= 500, ]\nschools_500plus\n\n                                SchoolName    Type NumberBoys NumberGirls\n4              North Ealing Primary School Primary        340         355\n8                West Acton Primary School Primary        285         290\n15                  Ravenor Primary School Primary        315         295\n16                 Selborne Primary School Primary        275         280\n17                Hambrough Primary School Primary        275         245\n18                 Hobbayne Primary School Primary        340         300\n20               Southfield Primary School Primary        275         245\n22              Blair Peach Primary School Primary        265         245\n25             Derwentwater Primary School Primary        335         370\n26             Durdans Park Primary School Primary        275         250\n27                 Fielding Primary School Primary        480         410\n28                  Gifford Primary School Primary        435         435\n29                Greenwood Primary School Primary        290         265\n31                Horsenden Primary School Primary        440         435\n32              Willow Tree Primary School Primary        385         370\n33            Lady Margaret Primary School Primary        360         310\n34            Little Ealing Primary School Primary        345         325\n35                 Oaklands Primary School Primary        290         260\n37                 Stanhope Primary School Primary        305         295\n40 Featherstone Primary and Nursery School Primary        365         360\n42               Montpelier Primary School Primary        350         330\n50    St Gregory's Catholic Primary School Primary        305         310\n51     St Joseph's Catholic Primary School Primary        295         265\n52    St Raphael's Catholic Primary School Primary        325         265\n56                   Grange Primary School Primary        415         385\n   TotalStudents OfstedGrade\n4            697           2\n8            573           3\n15           610           2\n16           554           2\n17           520           2\n18           641           2\n20           518           2\n22           506           2\n25           708           2\n26           528           2\n27           891           2\n28           866           1\n29           555           2\n31           872           2\n32           759           2\n33           671           2\n34           669           2\n35           552           2\n37           598           2\n40           722           2\n42           682           1\n50           619           2\n51           558           2\n52           589           2\n56           800           2\n\n\nWhat if were interested in primary schools with an OFSTED score of 1 (Excellent). Again, this operation is row-based and so we will need to use the $ to call the column OfstedGrade and the logical operator for this filter is ==:\n\nschools_ofsted1 &lt;- school_data[school_data$OfstedGrade == 1, ]\nschools_ofsted1\n\n                               SchoolName    Type NumberBoys NumberGirls\n14                   North Primary School Primary        215         205\n28                 Gifford Primary School Primary        435         435\n30    Havelock Primary School and Nursery Primary        245         190\n42              Montpelier Primary School Primary        350         330\n45           Vicar's Green Primary School Primary        165         180\n48 St John Fisher Catholic Primary School Primary        225         240\n   TotalStudents OfstedGrade\n14           419           1\n28           866           1\n30           434           1\n42           682           1\n45           342           1\n48           467           1\n\n\nHere is an example of combining such logical operators - select schools that have more than 650 students and with an OFSTED score of 2.\n\n# logical operators is &gt;, &, ==\nschools_650_ofsted2 &lt;- school_data[school_data$TotalStudents &gt; 650 & school_data$OfstedGrade == 2, ]\nschools_650_ofsted2\n\n                                SchoolName    Type NumberBoys NumberGirls\n4              North Ealing Primary School Primary        340         355\n25             Derwentwater Primary School Primary        335         370\n27                 Fielding Primary School Primary        480         410\n31                Horsenden Primary School Primary        440         435\n32              Willow Tree Primary School Primary        385         370\n33            Lady Margaret Primary School Primary        360         310\n34            Little Ealing Primary School Primary        345         325\n40 Featherstone Primary and Nursery School Primary        365         360\n56                   Grange Primary School Primary        415         385\n   TotalStudents OfstedGrade\n4            697           2\n25           708           2\n27           891           2\n31           872           2\n32           759           2\n33           671           2\n34           669           2\n40           722           2\n56           800           2\n\n\nI am sure you now get the gist of what is happening here with these logical statements for subsetting, or filtering data accordingly. We are now in the final stretch - let us learn how to save and export a dataset as a CSV spreadsheet.",
    "crumbs": [
      "Core Content",
      "Week 2: Introduction II"
    ]
  },
  {
    "objectID": "03-handling_data_structures.html#saving-your-dataset-length-001113",
    "href": "03-handling_data_structures.html#saving-your-dataset-length-001113",
    "title": "Week 2: Introduction II",
    "section": "",
    "text": "[Watch on YouTube]\nThis operation is very easy to perform. Once you have created your filtered data, you can use the following function write.csv() to save it as a new CSV file:\n\nwrite.csv(schools_650_ofsted2, file = \"Filtered_example_data.csv\", row.names = FALSE)\n\nThe above syntax should save a new CSV file named as Filtered_example_data.csv. Do not forget to save your R-script!",
    "crumbs": [
      "Core Content",
      "Week 2: Introduction II"
    ]
  },
  {
    "objectID": "03-handling_data_structures.html#summary",
    "href": "03-handling_data_structures.html#summary",
    "title": "Week 2: Introduction II",
    "section": "",
    "text": "You have learned the following:\n\nSet your working directory on Mac and Windows\nImport a .csv file into RStudio\nUnderstand the structure of a data frame (rows and columns)\nFilter data using both numeric and categorical variables\nCombine multiple conditions with logical operators to perform further filtering of data\nExport data back into a .csv file",
    "crumbs": [
      "Core Content",
      "Week 2: Introduction II"
    ]
  },
  {
    "objectID": "10-london_airquality_ttests.html",
    "href": "10-london_airquality_ttests.html",
    "title": "Week 9: London Air Quality II",
    "section": "",
    "text": "Last week, we used open source air pollution secondary data to perform a descriptive time-series analysis, correlation and a linear regression model.\nIn this practical, you will step into the role of an urban air-quality investigator. Over the past week, most of you have taken handheld air-pollution monitors into the streets of London to collect real-time data on the air they breathe. These monitors capture small-scale variations in pollutants—variations that can shift dramatically over just a few metres. In dense urban environments, these subtle changes matter: they shape not only the spatial patterns we observe in datasets, but also the personal exposure risks faced by people moving through the city each day.\nWith that said, you will work with the dataset collected during these field surveys. Your task is to explore how air quality changes around your assigned field location and to test a hypothesis of your choosing. The nature of your hypothesis should reflect the features of your street or the environmental processes you think might be at play. For example, you might investigate whether:\n\nAir quality is worse in areas with slow or congested traffic\nPollution decreases as you move further from the main road\nOutdoor smoking areas outside pubs or cafés influence local air quality\n\nThese are just starting points but you are free to formulate any hypothesis grounded in plausible urban processes. As you proceed, remember that your analysis in the computer practical will require comparing two groups (e.g., near vs. far from the road, high traffic vs. low traffic zones, or street number 1 versus street number 2 etc.). Keep this in mind when designing your hypothesis and exploring the dataset. Also note that meaningful statistical testing (i.e., unpaired t-test) often requires more data than expected, so be prepared for sample size considerations as you evaluate significance.\n\n\n\n\n\n\nWarning\n\n\n\n\n\n\nBy the end of this activity, you should have a deeper appreciation for how dynamic and localised air pollution can be—and how thoughtful data collection and analysis can help us make sense of complex urban environments.\nLet’s begin!\n\n\nA t-test is a statistical method used to determine whether the mean (average) of a numerical variable differs significantly between two groups. It tests whether any observed difference in means is likely due to real differences between the groups or simply random variation.\nIn the context of the air pollution problem - an example would be comparing two monitoring AURN stations. For instance, suppose you have PM2.5 data from:\n\nStation A (e.g., London North Kensington (Code: KC1))\nStation B (e.g., London Farringdon Street (Code: LOFS))\n\nWe want to compare if there’s any differences in the levels of PM2.5 between these locations, an unpaired two-sample t-test can be used to answer this question.\nLet show you how by using open source data as a motivating example:\n\nlibrary(\"openair\")\n\nair_data_ttest &lt;- importAURN(site = c(\"KC1\", \"LOFS\"), year = 2025, data_type = \"daily\", pollutant = \"pm2.5\")\n\nFirst thing, since we are comparing groups i.e., KC1 and LOFS. Explore the mean PM2.5 by stations using tapply() function:\n\ntapply(air_data_ttest$pm2.5, air_data_ttest$code, mean, na.rm = TRUE)\n\n     KC1     LOFS \n8.786574 7.605366 \n\ntapply(air_data_ttest$pm2.5, air_data_ttest$code, sd, na.rm = TRUE)\n\n     KC1     LOFS \n7.450127 3.405972 \n\nboxplot(air_data_ttest$pm2.5 ~ air_data_ttest$code, ylab = \"Daily Average PM2.5 (μg/m³)\", xlab = \"Location\", outline = FALSE)\n\n\n\n\n\n\n\n\nWe can see, at face-value, the average PM2.5 at KC1 (i.e., Kensington) is 8.786, which is a slightly higher than LOFS (i.e., Farringdon) which has an average of 7.605. Visually, however, it is difficult based on medians, but KC1 has a wide distribution with more extreme values.\nNow, we want to know if this difference are indeed statistical significant. We can do this by using the t.test() and declaring paired = FALSE.\n\nt.test(air_data_ttest$pm2.5[air_data_ttest$code == \"KC1\"], air_data_ttest$pm2.5[air_data_ttest$code == \"LOFS\"], paired = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  air_data_ttest$pm2.5[air_data_ttest$code == \"KC1\"] and air_data_ttest$pm2.5[air_data_ttest$code == \"LOFS\"]\nt = 2.48, df = 490.6, p-value = 0.01347\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.2453977 2.1170178\nsample estimates:\nmean of x mean of y \n 8.786574  7.605366 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: This t-test examines whether the average PM2.5 concentration differs between two air-quality monitoring stations: KC1 and LOFS. It shows the following:\n\nDifference in means i.e., PM2.5 at KC1: 8.79 µg/m³, and LOFS is 7.61 µg/m³. So KC1 has a higher average PM2.5 than LOFS by about 1.18 µg/m³.\nStatistical significance with p = 0.01347 which less than less than 0.05. Therefore, the difference in mean PM2.5 between the two stations is statistically significant. This means that at KC1, it genuinely tends to experience higher PM2.5 levels than LOFS.\n\n\n\nThat is how you execute a t-test. Now, you should be able to the do final worksheet.",
    "crumbs": [
      "Core Content",
      "Week 9: London Air Quality II"
    ]
  },
  {
    "objectID": "10-london_airquality_ttests.html#what-is-a-t-test",
    "href": "10-london_airquality_ttests.html#what-is-a-t-test",
    "title": "Week 9: London Air Quality II",
    "section": "",
    "text": "A t-test is a statistical method used to determine whether the mean (average) of a numerical variable differs significantly between two groups. It tests whether any observed difference in means is likely due to real differences between the groups or simply random variation.\nIn the context of the air pollution problem - an example would be comparing two monitoring AURN stations. For instance, suppose you have PM2.5 data from:\n\nStation A (e.g., London North Kensington (Code: KC1))\nStation B (e.g., London Farringdon Street (Code: LOFS))\n\nWe want to compare if there’s any differences in the levels of PM2.5 between these locations, an unpaired two-sample t-test can be used to answer this question.\nLet show you how by using open source data as a motivating example:\n\nlibrary(\"openair\")\n\nair_data_ttest &lt;- importAURN(site = c(\"KC1\", \"LOFS\"), year = 2025, data_type = \"daily\", pollutant = \"pm2.5\")\n\nFirst thing, since we are comparing groups i.e., KC1 and LOFS. Explore the mean PM2.5 by stations using tapply() function:\n\ntapply(air_data_ttest$pm2.5, air_data_ttest$code, mean, na.rm = TRUE)\n\n     KC1     LOFS \n8.786574 7.605366 \n\ntapply(air_data_ttest$pm2.5, air_data_ttest$code, sd, na.rm = TRUE)\n\n     KC1     LOFS \n7.450127 3.405972 \n\nboxplot(air_data_ttest$pm2.5 ~ air_data_ttest$code, ylab = \"Daily Average PM2.5 (μg/m³)\", xlab = \"Location\", outline = FALSE)\n\n\n\n\n\n\n\n\nWe can see, at face-value, the average PM2.5 at KC1 (i.e., Kensington) is 8.786, which is a slightly higher than LOFS (i.e., Farringdon) which has an average of 7.605. Visually, however, it is difficult based on medians, but KC1 has a wide distribution with more extreme values.\nNow, we want to know if this difference are indeed statistical significant. We can do this by using the t.test() and declaring paired = FALSE.\n\nt.test(air_data_ttest$pm2.5[air_data_ttest$code == \"KC1\"], air_data_ttest$pm2.5[air_data_ttest$code == \"LOFS\"], paired = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  air_data_ttest$pm2.5[air_data_ttest$code == \"KC1\"] and air_data_ttest$pm2.5[air_data_ttest$code == \"LOFS\"]\nt = 2.48, df = 490.6, p-value = 0.01347\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.2453977 2.1170178\nsample estimates:\nmean of x mean of y \n 8.786574  7.605366 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: This t-test examines whether the average PM2.5 concentration differs between two air-quality monitoring stations: KC1 and LOFS. It shows the following:\n\nDifference in means i.e., PM2.5 at KC1: 8.79 µg/m³, and LOFS is 7.61 µg/m³. So KC1 has a higher average PM2.5 than LOFS by about 1.18 µg/m³.\nStatistical significance with p = 0.01347 which less than less than 0.05. Therefore, the difference in mean PM2.5 between the two stations is statistically significant. This means that at KC1, it genuinely tends to experience higher PM2.5 levels than LOFS.\n\n\n\nThat is how you execute a t-test. Now, you should be able to the do final worksheet.",
    "crumbs": [
      "Core Content",
      "Week 9: London Air Quality II"
    ]
  },
  {
    "objectID": "datasets/All Data/Jonathan_rmdfiles/01-mainbody2.html",
    "href": "datasets/All Data/Jonathan_rmdfiles/01-mainbody2.html",
    "title": "1 Tasks",
    "section": "",
    "text": "The tasks are described below. You should present the results of your calculations and plots, as indicated, together with any comments on the data. Make sure that your comments adhere to the length limits specified. This is an independent piece of work. Although you are welcome to discuss the exercise with your colleagues, all of the analyses and plots should be your own work.\n\n\nDownload data for the calendar year 2020 from the Defra data selector for both of the Camden monitoring stations by clicking here First, select ‘Search hourly networks’ and then, for the various options, choose as follows: data type = daily mean; date range = the year in question, monitoring sites = local authority and then London Borough of Camden (choose Camden Kerbside and London Bloomsbury from the list) select pollutants = by monitoring network and AURN and then select the following two variables from the drop-down list: PM10 particulate matter (hourly measured) and PM2.5 Particulate matter (hourly measured). You can either chose to view the data on screen and then copy and paste them into excel OR have the data emailed to you as a csv file. Note that the email option has not been working so copying and pasting from the on-screen option into a csv file may be best. If you do chose this option however, make sure that the data from the two sites appear in separate columns in your csv file. Once the file has been received or created, save it onto your N:/ drive (or your desktop if using your own computer). Before reading the dataset into R, you will need to perform a basic clean-up to replace the entry in those cells for which there are no values (marked ‘No data’) with empty cells, using the search and replace function in excel.\n\n\n\nRead the cleaned data file into R using the read.csv command, skipping the first four lines (which contain neither the column headings nor values). Then create a new dataset comprising only the date column and the columns containing values (i.e. omit the columns labelled ‘status’). This dataset is used for all subsequent analyses described below.\n\n\n\n\n\nCalculate the mean and standard deviation for PM10 and PM2.5 for both sites: calculate the mean and standard deviation for the entire year first of all, and then for each quarter. Present these values in a simple table.\n\n\n\nProduce two time-series plots for Camden Kerbside, one for PM10 and a second for PM2.5. Make sure that you label the x-axis with the date, and the y axis with the name of the variable and the units of measurement.\nComment briefly on the variations in the variable. How similar are the time series for the two variables? [5 lines]\n\n\n\nYou will now investigate further any differences or similarities between the two Camden sites, using either PM10 or PM2.5 (you only need to make one plot).\nFirst, plot the data for the two sites on a bivariate scatter plot, clearly noting which axis represents which site by appropriate labelling. Then fit a linear regression line through the dataset. Give the regression equation for the regression line and calculate the correlation coefficient between the two variables.\n\n\n\nYou will now investigate the relationship between the two variables (PM10 and PM2.5) for one of the sites. For either Camden Kerbside or London Bloomsbury, plot PM10 and PM2.5 on a bivariate scatter plot, clearly noting which axis represents which variable by appropriate labelling and indicating which site you have chosen in the plot title. Then fit a linear regression line through the dataset. Give the regression equation for the regression line and calculate the correlation coefficient between the two variables."
  },
  {
    "objectID": "datasets/All Data/Jonathan_rmdfiles/01-mainbody2.html#data-download",
    "href": "datasets/All Data/Jonathan_rmdfiles/01-mainbody2.html#data-download",
    "title": "1 Tasks",
    "section": "",
    "text": "Download data for the calendar year 2020 from the Defra data selector for both of the Camden monitoring stations by clicking here First, select ‘Search hourly networks’ and then, for the various options, choose as follows: data type = daily mean; date range = the year in question, monitoring sites = local authority and then London Borough of Camden (choose Camden Kerbside and London Bloomsbury from the list) select pollutants = by monitoring network and AURN and then select the following two variables from the drop-down list: PM10 particulate matter (hourly measured) and PM2.5 Particulate matter (hourly measured). You can either chose to view the data on screen and then copy and paste them into excel OR have the data emailed to you as a csv file. Note that the email option has not been working so copying and pasting from the on-screen option into a csv file may be best. If you do chose this option however, make sure that the data from the two sites appear in separate columns in your csv file. Once the file has been received or created, save it onto your N:/ drive (or your desktop if using your own computer). Before reading the dataset into R, you will need to perform a basic clean-up to replace the entry in those cells for which there are no values (marked ‘No data’) with empty cells, using the search and replace function in excel."
  },
  {
    "objectID": "datasets/All Data/Jonathan_rmdfiles/01-mainbody2.html#reading-data-into-r",
    "href": "datasets/All Data/Jonathan_rmdfiles/01-mainbody2.html#reading-data-into-r",
    "title": "1 Tasks",
    "section": "",
    "text": "Read the cleaned data file into R using the read.csv command, skipping the first four lines (which contain neither the column headings nor values). Then create a new dataset comprising only the date column and the columns containing values (i.e. omit the columns labelled ‘status’). This dataset is used for all subsequent analyses described below."
  },
  {
    "objectID": "datasets/All Data/Jonathan_rmdfiles/01-mainbody2.html#plotting-and-data-analysis",
    "href": "datasets/All Data/Jonathan_rmdfiles/01-mainbody2.html#plotting-and-data-analysis",
    "title": "1 Tasks",
    "section": "",
    "text": "Calculate the mean and standard deviation for PM10 and PM2.5 for both sites: calculate the mean and standard deviation for the entire year first of all, and then for each quarter. Present these values in a simple table.\n\n\n\nProduce two time-series plots for Camden Kerbside, one for PM10 and a second for PM2.5. Make sure that you label the x-axis with the date, and the y axis with the name of the variable and the units of measurement.\nComment briefly on the variations in the variable. How similar are the time series for the two variables? [5 lines]\n\n\n\nYou will now investigate further any differences or similarities between the two Camden sites, using either PM10 or PM2.5 (you only need to make one plot).\nFirst, plot the data for the two sites on a bivariate scatter plot, clearly noting which axis represents which site by appropriate labelling. Then fit a linear regression line through the dataset. Give the regression equation for the regression line and calculate the correlation coefficient between the two variables.\n\n\n\nYou will now investigate the relationship between the two variables (PM10 and PM2.5) for one of the sites. For either Camden Kerbside or London Bloomsbury, plot PM10 and PM2.5 on a bivariate scatter plot, clearly noting which axis represents which variable by appropriate labelling and indicating which site you have chosen in the plot title. Then fit a linear regression line through the dataset. Give the regression equation for the regression line and calculate the correlation coefficient between the two variables."
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/03-What_to_hand_in.html",
    "href": "datasets/All Data/Lucy_rmdfiles/03-What_to_hand_in.html",
    "title": "What to hand in",
    "section": "",
    "text": "What to hand in\nAs a group you must produce and hand-in a short report based on the measurements you have taken. You should hand in your group report, in a single document maximum of 3 sides of A4 (including references, Figures and Tables) that has 2 cm margins all round. Graphs should be appropriately titled, labelled and scaled: they should be large enough to be legible but not excessively large. Text should be in 11pt Times or Arial and adhere to the report length stated above. Any text that exceeds this limit will be disregarded and not marked. Your group’s report must contain the following elements:\n\n\n\n\n\n\n\nTitle\n\n\n\nHypothesis\n\n\n\nLocation\nBriefly describe where you undertook the research and the rationale behind the site selection.\n\n\nMethods\nBriefly describe how you undertook the research and the equipment used.\n\n\nResults\nThe raw data you collect should be in your individual field notebooks but briefly summarise your results. Use Graphs and Tables as appropriate.\n\n\nInterpretation\nDescribe how you have used your data to determine whether they support your hypothesis.\n\n\nConclusions\nSummarise your key findings as 1 or 2 bullet points.\n\n\nReflection\nWhat worked well or badly? If you were doing it again, what would you change? Were there other unforeseen factors that influenced your results?\n\n\nReferences\nInclude any references that you have used\n\n\n\n In your report, you must also address the following questions:\n\nWhich t-test did you undertake and why?\nWhat, if any, assumptions have you made about your data?\n\nYour file should be submitted anonymously by 12:00 (noon) on Friday 14th February 2025 using the Turnitin link on the course Moodle page entitled Worksheet Number 1. This worksheet is a group exercise and only one file should be submitted per group. Follow the guidelines for group submissions, which can be found on the module Moodle pages. Make sure that you adhere strictly to the file name instructions: if you do not, there is a chance that we will not be able to link the submission to you and that you will not receive the marks for it as a result."
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/02-Tasks.html",
    "href": "datasets/All Data/Lucy_rmdfiles/02-Tasks.html",
    "title": "Tasks",
    "section": "",
    "text": "The tasks are described below. You will explore the differences between two groups in order to test your chosen hypothesis.\n\n\nYou should have recorded your data in the field in your field notebook. You will need to type this up in Excel and save it as .csv file. Save this in a sensible location. Set your working directory in R to that location and load the .csv file using the read.csv() command. You can use the view() command to see the full data viewer.\n\nTip Look back at GEOG0013 and think how you should organise your data in Excel. If you have undertaken your measurements in the same location but under different categories/scenarios, consider if a heading other than location is most appropriate.\n\n\n\n\nCalculate summary statistics (e.g the mean, standard deviation etc.) of your dataset(s) using the mean(), sd(), min()functions etc.\n\nTip Think about how your data is organised. If you have ‘grouped’ data by locations or categories, you will need to define the rows that your categories are in e.g rows 1 to 10 would be [1:10] and is inserted as follows: summary statistic of choice(data$variable[rowstart:rowendofcategory], na.rm=TRUE)\n\n Now, visualise the distribution of your data using the boxplot() function. Unlike your previous practical, you may want to plot more than one boxplot in the same figure frame. To do this you will need to define the following aspects in your code boxplot(yaxis~xaxis,data=your data......)\n What does this plot tell you? Think about what you need to visualise this way in order to test your hypothesis.\n\n\n\nNow you will test how statistically significant the differences between your groups are. The below video explains why and how we test for statistical significance. It includes some important information that may help later so I recommend watching this before proceeding.\n\n\n\n\n\n\n\n\n\nFor our data, we will undertake a t-test. The t-test compares sample mean and standard deviations while considering sample size and the degree of variability of the data to determine if there is a significant difference between the means of the two groups. There are one-sample and two-sample t-tests; paired and unpaired t-tests. Using the information in the video and the below, consider which is most appropriate for your data.\n\n\n\n\n\n\n\nTest\n\n\n\n\n\nOne sample\nCalculates if there is a significant difference between the sample mean and hypothesis or assumed population mean. For example, the PM2.5 concentration within a metre of someone smoking is &gt;250.5 µg m-3.\n\n\nTwo sample paired sample t-test\nCompares two population means in the case of two samples that are correlated. For instance, air quality before and after rain or before and after rush hour in the same location.\n\n\nTwo sample unpaired t-test (or independent t-test)\nCompares the means of two independent or unrelated groups to determine if there is a significant difference between the two. For instance, comparing air quality during rush hour in Barcelona and London i.e. between two locations.\n\n\n\n We can use the t.test() function in R.\n\nTo define the groups for a Two Sample t-test, we can use the same approach to the code the boxplot. i.e. variable~category, data =\nTo perform Two Sample t-tests with equal variance, we have to set var.equal=TRUE\nTo perform a Paired t-test in R, we have to set paired=TRUE\n\n\nYou might want to think about what assumptions you might have made about your datasets when undertaking these analyses.\n\nThe higher the t-score, the larger the difference between the two sample sets and it is less likely that this was due to chance.\n\nA minus sign of a t-value is not a lower value, the ± indicates the direction.\n\n\n\n\nThe ‘p’ in p-value represents ‘probability’ and it is a measure of how likely any observed difference between groups is due to chance. A p-value is provided in the output of the t.test() function, but what does it mean?\nSimply, the lower the p-value, the greater the statistical significance of the observed difference. P-values are calculated as a percentage (from 0-100%), but the output is a decimal. For example, a p-value of 5% is 0.05 and means there was a 5% chance that these results occurred by chance. A p-value of 0.05 or lower is considered statistically significant."
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/02-Tasks.html#organising-your-data",
    "href": "datasets/All Data/Lucy_rmdfiles/02-Tasks.html#organising-your-data",
    "title": "Tasks",
    "section": "",
    "text": "You should have recorded your data in the field in your field notebook. You will need to type this up in Excel and save it as .csv file. Save this in a sensible location. Set your working directory in R to that location and load the .csv file using the read.csv() command. You can use the view() command to see the full data viewer.\n\nTip Look back at GEOG0013 and think how you should organise your data in Excel. If you have undertaken your measurements in the same location but under different categories/scenarios, consider if a heading other than location is most appropriate."
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/02-Tasks.html#summary-statistics",
    "href": "datasets/All Data/Lucy_rmdfiles/02-Tasks.html#summary-statistics",
    "title": "Tasks",
    "section": "",
    "text": "Calculate summary statistics (e.g the mean, standard deviation etc.) of your dataset(s) using the mean(), sd(), min()functions etc.\n\nTip Think about how your data is organised. If you have ‘grouped’ data by locations or categories, you will need to define the rows that your categories are in e.g rows 1 to 10 would be [1:10] and is inserted as follows: summary statistic of choice(data$variable[rowstart:rowendofcategory], na.rm=TRUE)\n\n Now, visualise the distribution of your data using the boxplot() function. Unlike your previous practical, you may want to plot more than one boxplot in the same figure frame. To do this you will need to define the following aspects in your code boxplot(yaxis~xaxis,data=your data......)\n What does this plot tell you? Think about what you need to visualise this way in order to test your hypothesis."
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/02-Tasks.html#comparing-two-or-more-groups",
    "href": "datasets/All Data/Lucy_rmdfiles/02-Tasks.html#comparing-two-or-more-groups",
    "title": "Tasks",
    "section": "",
    "text": "Now you will test how statistically significant the differences between your groups are. The below video explains why and how we test for statistical significance. It includes some important information that may help later so I recommend watching this before proceeding.\n\n\n\n\n\n\n\n\n\nFor our data, we will undertake a t-test. The t-test compares sample mean and standard deviations while considering sample size and the degree of variability of the data to determine if there is a significant difference between the means of the two groups. There are one-sample and two-sample t-tests; paired and unpaired t-tests. Using the information in the video and the below, consider which is most appropriate for your data.\n\n\n\n\n\n\n\nTest\n\n\n\n\n\nOne sample\nCalculates if there is a significant difference between the sample mean and hypothesis or assumed population mean. For example, the PM2.5 concentration within a metre of someone smoking is &gt;250.5 µg m-3.\n\n\nTwo sample paired sample t-test\nCompares two population means in the case of two samples that are correlated. For instance, air quality before and after rain or before and after rush hour in the same location.\n\n\nTwo sample unpaired t-test (or independent t-test)\nCompares the means of two independent or unrelated groups to determine if there is a significant difference between the two. For instance, comparing air quality during rush hour in Barcelona and London i.e. between two locations.\n\n\n\n We can use the t.test() function in R.\n\nTo define the groups for a Two Sample t-test, we can use the same approach to the code the boxplot. i.e. variable~category, data =\nTo perform Two Sample t-tests with equal variance, we have to set var.equal=TRUE\nTo perform a Paired t-test in R, we have to set paired=TRUE\n\n\nYou might want to think about what assumptions you might have made about your datasets when undertaking these analyses.\n\nThe higher the t-score, the larger the difference between the two sample sets and it is less likely that this was due to chance.\n\nA minus sign of a t-value is not a lower value, the ± indicates the direction."
  },
  {
    "objectID": "datasets/All Data/Lucy_rmdfiles/02-Tasks.html#p-value",
    "href": "datasets/All Data/Lucy_rmdfiles/02-Tasks.html#p-value",
    "title": "Tasks",
    "section": "",
    "text": "The ‘p’ in p-value represents ‘probability’ and it is a measure of how likely any observed difference between groups is due to chance. A p-value is provided in the output of the t.test() function, but what does it mean?\nSimply, the lower the p-value, the greater the statistical significance of the observed difference. P-values are calculated as a percentage (from 0-100%), but the output is a decimal. For example, a p-value of 5% is 0.05 and means there was a 5% chance that these results occurred by chance. A p-value of 0.05 or lower is considered statistically significant."
  },
  {
    "objectID": "09-london_airquality_relationships.html",
    "href": "09-london_airquality_relationships.html",
    "title": "Week 8: London Air Quality I",
    "section": "",
    "text": "UK AIR Information Resource https://uk-air.defra.gov.uk is a central UK resource for air-quality data and information. It provides a lot of information on air pollution in the UK: current measurements, forecasts, monitoring network details, statistics, etc. There are over 1,500 sites across the UK that monitor air quality, which are extraordinarily organised into automatic and non-automatic networks i.e., with automatic networks being many pollution sites collecting hourly data, and non-automatic networks sites measure aggregated daily/weekly/monthly samples. This is extremely helpful open data source for geography students to be familiar with if you someday want to incorporate these pollution records and monitoring sites into a GIS workflow - you could map pollutant concentrations, link with demographic/health outcome layers, or design spatial models of pollution exposure.\nLet’s dig into this dataset - you will see how everything from Week 1 to 7 comes full circle!\n\n\nBy the end of this practical, you will be able to:\n\nImport UK air quality data from Automatic Urban and Rural Netorks (AURN) remotely into RStudio via OpenAir Project.\nProduce basic descriptive statistics and time-series plots of any air pollutant.\nQuantify and visualise the correlation between selected air pollutants.\nFit a simple linear regression model for predicting a restricted set of air pollutants.\nInterpret basic output in the context of air quality in London.\n\n\n\n\nOpenair is an R package primarily developed for the analysis of air pollution measurement data for general use in the atmospheric sciences. The package consists of many tools for importing and manipulating data, and undertaking a wide range of analyses to enhance understanding of air pollution data. It gives as remote access, freely, to all UK air pollution networks to have the data at our disposal.\nTo install a package in R/RStudio, we can use the install.packages() function. Let us install the following packages - openair and openairmaps. We will also install the package lubridate to get access to additional function to deal with dates for time series.\nLet us install the 3 packages:\n\ninstall.packages(\"openair\")\ninstall.packages(\"openairmaps\")\ninstall.packages(\"lubridate\")\n\nAfter installing both packages, we will need to active them in order to use their functions. This can be done using the library() function. Let us process to activating these packages:\n\nlibrary(\"openair\")\nlibrary(\"openairmaps\")\nlibrary(\"lubridate\")\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n\n\n\n\n\nThe UK has a surprisingly large amount of air quality data that is publicly accessible. The first order of things is to understand exactly what air pollution monitoring sites are availabile (also called Automatic Urban and Rural Networks (AURN)), and what air pollution contaminants do they measure. We can access the details of air pollution monitoring sites using the importMeta() function.\nLet us pull these details and store in a data frame:\n\n# view list of all points\nall_airpollution_aurn &lt;- importMeta(source = \"aurn\", all = TRUE)\n# view data frame\nView(all_airpollution_aurn)\n\nThis shows details of what each air pollution monitor station in the UK is measuring - let us examine the image closely:\n\n\n\n\n\n\n\n\n\n\nThe following columns code, site and latitude & longitude are key information that tells as the unique identifier, name and geographical position of these air pollution monitoring stations in the UK, respectively.\nThe columns variable and Parameter_name contain the shorthand notation and actual name, respectively, of the air pollution contaminant being measure.\nThe columns start_date and end_date refer to when that monitoring network started, whether if its ongoing or if its retired. The end_date shows if its still ongoing, or if its retired.\n\nYou can visualise the spatial distribution of these sites that are currently active using the networkMap() function\n\n# map of all air pollution monitors\nnetworkMap(source = \"aurn\", control = \"site_type\", year = 2025)\n\n\n\n\n\n\n\n\nWe explored details about the networks. Now, let us extract the actual pollution measures from the networks themselves - daily averages. The network of focus are at the following locations:\n\nLondon North Kensington (Code: KC1)\n\nWe will extract the following parameters as an example:\n\nNitrogen Dioxide (no2)\nNitrogen Oxide (nox)\nParticulate matter (pm2.5)\nParticulate matter (pm10)\n\nWe can extract information into a data frame by using the importAURN() function. Let us pull the record available records in 2025:\n\nair_KC1 &lt;- importAURN(\n    site = \"KC1\", \n    year = 2025, \n    data_type = \"daily\", \n    pollutant = c(\"no2\", \"nox\", \"pm2.5\", \"pm10\"))\n\nShow the first 10 rows:\n\nhead(air_KC1, n = 10)\n\n# A tibble: 10 × 8\n   source date                site                code    no2    nox pm2.5  pm10\n   &lt;chr&gt;  &lt;dttm&gt;              &lt;chr&gt;               &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 aurn   2025-01-01 00:00:00 London N. Kensingt… KC1    8.14   8.49  3.25  5.68\n 2 aurn   2025-01-02 00:00:00 London N. Kensingt… KC1   25.6   29.0   5.43  9.3 \n 3 aurn   2025-01-03 00:00:00 London N. Kensingt… KC1   41.0   51.1   9.67 13.0 \n 4 aurn   2025-01-04 00:00:00 London N. Kensingt… KC1   38.9   52.8  11.1  13.9 \n 5 aurn   2025-01-05 00:00:00 London N. Kensingt… KC1   16.4   18.8   2.66  3.48\n 6 aurn   2025-01-06 00:00:00 London N. Kensingt… KC1   15.4   16.5   2.47  4.45\n 7 aurn   2025-01-07 00:00:00 London N. Kensingt… KC1   28.3   32.2   4.48  7.41\n 8 aurn   2025-01-08 00:00:00 London N. Kensingt… KC1   45.3   66.0   9.36 14.4 \n 9 aurn   2025-01-09 00:00:00 London N. Kensingt… KC1   37.8   45.4  11.8  19.6 \n10 aurn   2025-01-10 00:00:00 London N. Kensingt… KC1   62.3  148.   20.3  27.0 \n\n\nData is never perfect! You will see that there is a lot of missing records across the period, and the fact that a network (i.e., KC1) has no data on PM10 throughout 2025. Anyways, we now have our dataset - let us analysis it!\n\n\n\n\n\n\nA time series plot is a graph that visualizes data collected in a sequence over time, with time on the horizontal (x-axis) and the measured variable on the vertical (y-axis). It is best used to identify trends, cycles, and patterns in data, such as economic fluctuations, weather changes, website traffic, or, in our case, changes in air pollution. These plots allow for analysis and forecasting by showing how a variable changes over specific time intervals like hours, days, months, or years.\nAll the basic building blocks for building a graph from scratch was covered in Week 5 (see here) - all of this comes full circle in this tutorial.\nFirst, we need to make sure the date column is a proper calendar/date format by using the as.Date(), and by stating within that function, that the format of the date is in YYYY-MM-DD as “%Y-%m-%d”:\n\nair_KC1$date_formatted &lt;- as.Date(air_KC1$date, format = \"%Y-%m-%d\")\n# class() confirms if the column is a date formate\nclass(air_KC1$date_formatted)\n\n[1] \"Date\"\n\n\nNow that its confirmed as a date format - we can proceed to create a time series plot specifically for NO2 at that location.\nWe are going to use the plot() to create the graphical output, and axis.Date() to apply cosmetic changes to the x-axis which measures the time:\n\nplot(air_KC1$no2 ~ air_KC1$date_formatted, \n    type=\"l\", \n    lwd=1.5, \n    main=\"AURN: London North Kensington [Code: KC1]\", \n    ylab = \"NO2 [PPB, Daily Average]\", \n    xlab = \"Date [2025-01-01 to 2025-11-22]\",\n    ylim = c(0, 70),\n    bty = \"L\",\n    xaxt = \"n\")\n\naxis.Date(\n    side=1, \n    at=seq(min(air_KC1$date_formatted), max(air_KC1$date_formatted), by = \"months\"), \n    format=\"%Y-%m-%d\", \n    cex.axis = .9, \n    labels = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation: This is a face-value interpretation, and as you can see, it is quite difficult to discern clear patterns in NO₂ between 01/01/2025 and 22/11/2025. But broadly speaking—and ignoring the jagged spikes—the overall pattern suggests a gradual decline in NO₂ levels from the start of the year until early September. From the beginning of September onwards, NO₂ levels begin to increase gradually.\n\n\n\n\n\nWe have covered descriptive statistics quite extensively at this point - so the programming aspect of such analysis should be second nature to you now! All we are doing is using the summary() and, or tapply() to compute the mean and standard deviation. Please refer to Week 4 because it was covered quite extensively!\nIt is possible to calculate the overall mean and standard deviation of NO2 across the time period by simply using the summary() function along with the standard deviation using sd() function:\n\nsummary(air_KC1$no2, na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  3.435   8.214  13.085  16.369  20.581  64.969       3 \n\nsd(air_KC1$no2, na.rm = TRUE)\n\n[1] 10.82045\n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: The overall mean air pollution levels for NO2 (in 2025) in London North Kensington was 16.399ppb (with one SD of ±10.82ppb). The overall range in the distribution is 61.534ppb where the lowest observed value is 3.435ppb (minimum) and the highest observed value is 64.949ppb (maximum)\n\n\nWhile, this is basically a summary of the overall description of the distribution of NO2 - sometimes, its inappropriate or bad practice to summarise time series data in that fashion. It is best practice to report such summaries for time series data in a quarterly, or monthly manner, to see if their a distinct pattern that is temporally apparent.\nFor instance, we can produce quarterly summaries by grouping the records accordingly into quarters by using the quarters() function on the formatted date column to generate a quarterly indicator:\n\nair_KC1$quarters &lt;- quarters(air_KC1$date_formatted)\n\nNotice that new column was generated with each date defined as Q1, Q2, Q3 or Q4. Now, we can use tapply() function to compute the mean and standard across the quarters:\n\ntapply(air_KC1$no2, air_KC1$quarters, mean, na.rm = TRUE)\n\n      Q1       Q2       Q3       Q4 \n26.77046 13.28455 10.37344 14.77160 \n\ntapply(air_KC1$no2, air_KC1$quarters, sd, na.rm = TRUE)\n\n       Q1        Q2        Q3        Q4 \n13.091599  6.787649  5.578397  6.372888 \n\n\nIf you want these results to be further broken down by month - it can be done but the coding is little bit involved. Unlike the quarters() function, RStudio does not have a function for months. So we will need to be a bit creative and pull the month value from the actual date column to group the dataset accordingly.\nIn the date column (i.e., YYYY-MM-DD) the month string is contained as the 6th and 7th character in that string. We are going to extract the 6th and 7th character to create the needed month column for this analysis to get the monthly means of NO2.\nHere, we can use the substr() function, and tell it to select the 6th and 7th string only in the date column.\n\nair_KC1$months &lt;- substr(air_KC1$date, 6, 7)\n\nWe got what we want - now, we can use tapply() function to compute mean and standard deviation across those months:\n\ntapply(air_KC1$no2, air_KC1$months, mean, na.rm = TRUE)\n\n       01        02        03        04        05        06        07        08 \n29.177950 23.140038 27.407827 18.379460 11.217195 10.256994  9.935942 10.246371 \n       09        10        11 \n10.956836 14.084862 15.697202 \n\ntapply(air_KC1$no2, air_KC1$months, sd, na.rm = TRUE)\n\n       01        02        03        04        05        06        07        08 \n15.146743  9.073397 13.459940  6.740908  5.638085  4.838321  4.283866  4.707091 \n       09        10        11 \n 7.427567  5.419553  7.498650 \n\n\nNot too shabby!\n\n\n\nSo far, we have been analysing a single continuous measure, univariably. Here, we going to analysis two variables together to show a basic correlation.\n\nA correlation is a measure that describes the strength and direction of a relationship between two numerical variables. A correlation can have a value that ranges anywhere from: -1 to 0 to +1, where a positive value is a positive correlation which means that as one variable increases, the other tends to increase, whereas a negative value is a negative correlation means that as one variable increases, the other tends to decrease.\n\nWithout going too deep into the subject of correlations - its basic understanding in the context of air pollution helps identify:\n\nShared pollution sources\nHow pollutants interact\nWhether one pollutant can help predict another\n\nFor example, let us look at the correlation between NO2 and PM2.5 - perhaps, there could be some relationship? The first thing is to create a scatterplot showing the bivariable visually:\n\nplot(air_KC1$no2, air_KC1$pm2.5,\n    xlab = \"Daily Average NO2 (ppb)\", \n    ylab = \"Daily Average PM2.5 (μg/m³)\",\n    main = \"Scatterplot: NO2 versus  PM2.5 [Code: KC1]\",\n    bty = \"L\",\n    pch = 16,\n    las = 1,\n)\n\n\n\n\n\n\n\n\nGo to Week 5 as we covered the coding on creating graphs extensively!\nWhat this image illustrates - if there’s an increase in the NO2 levels, it shows as positive increase in PM2.5 as well. This relationship is only shown visually. We can quantify the strength by performing a correlation test between NO2 versus PM2.5 using the cor.test() function:\n\n# use diff() to remove trends, seasonality \ncor.test(diff(air_KC1$no2), diff(air_KC1$pm2.5))\n\n\n    Pearson's product-moment correlation\n\ndata:  diff(air_KC1$no2) and diff(air_KC1$pm2.5)\nt = 8.2481, df = 317, p-value = 4.347e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3255604 0.5067652\nsample estimates:\n      cor \n0.4203449 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: The correlation between the changes in NO₂ and the changes in PM2.5 is 0.4206. This indicates a moderate positive relationship i.e., When NO2 increases from one time point to the next, PM2.5 also tends to increase, and when NO2 decreases, PM2.5 tends to decrease. The p-value is extremely small (way below 0.05), meaning that the probability of observing a correlation this strong (or stronger) is not by chance, and therefore the correlation is statistically significant.\n\n\n\n\n\nA linear regression is a statistical method used to describe how one variable (the outcome) changes in relation to another variable (the predictor).\nIt fits a straight line to the data: Outcome = Intercept + Slope × Predictor + Error\nThe slope tells you how much the outcome changes for every one-unit increase in the predictor. Suppose you want to understand how NO2 and PM2.5 relate to each other at a monitoring station.\n\nThe correlation tells you whether they move together (e.g., correlation = 0.4206).\nBut, a linear regression goes further and quantifies how much PM2.5 changes when NO2 changes.\n\nWithout going into too much details about linear regression - let us use a simple example, and fit a linear model regression model using lm() function:\n\nmodelfit &lt;- lm(diff(air_KC1$pm2.5) ~ diff(air_KC1$no2))\nsummary(modelfit)\n\n\nCall:\nlm(formula = diff(air_KC1$pm2.5) ~ diff(air_KC1$no2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.552  -1.695   0.074   1.921  30.339 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.005887   0.310406   0.019    0.985    \ndiff(air_KC1$no2) 0.324091   0.039293   8.248 4.35e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.544 on 317 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.1767,    Adjusted R-squared:  0.1741 \nF-statistic: 68.03 on 1 and 317 DF,  p-value: 4.347e-15\n\n\nThe above code lm(diff(pm25) ~ diff(no2)) means that we are modelling the change in PM2.5 as a function of the change in NO2. From the printed output, the explore the following results:\n\nIntercept (0.005887): The intercept is essentially zero and is not statistically significant (p = 0.985 which is bigger than 0.05). This means that when there is no change in NO2, the expected change in PM2.5 is 0.005887 which is effectively zero.\nSlope for NO2 [0.324 (p &lt; 0.001)]: The slope is the key result - here, for every unit increase in NO2 will result in PM2.5 to increases on average by 0.324 µg/m³. This increase is significant because its p-value is below 0.05. A small note - this confirms a moderate, positive, and statistically significant short-term association between the two pollutants, and it aligns with the correlation result we previously obtained (0.4206).\nModel fit (0.1741): The R² of 0.1741, which means that 17.41% of the variation in changes in PM2.5 is explained by changes in NO2. Generally, its a poor model if this is below 20%. 20-40% is moderate; 40-70% is good; and anything above 70% is an excellent model.\n\nThat is how you perform a regression model - the main resul of interest is the slope and its p-value.",
    "crumbs": [
      "Core Content",
      "Week 8: London Air Quality I"
    ]
  },
  {
    "objectID": "09-london_airquality_relationships.html#learning-outcomes",
    "href": "09-london_airquality_relationships.html#learning-outcomes",
    "title": "Week 8: London Air Quality I",
    "section": "",
    "text": "By the end of this practical, you will be able to:\n\nImport UK air quality data from Automatic Urban and Rural Netorks (AURN) remotely into RStudio via OpenAir Project.\nProduce basic descriptive statistics and time-series plots of any air pollutant.\nQuantify and visualise the correlation between selected air pollutants.\nFit a simple linear regression model for predicting a restricted set of air pollutants.\nInterpret basic output in the context of air quality in London.",
    "crumbs": [
      "Core Content",
      "Week 8: London Air Quality I"
    ]
  },
  {
    "objectID": "09-london_airquality_relationships.html#installation-of-openair-and-openairmaps",
    "href": "09-london_airquality_relationships.html#installation-of-openair-and-openairmaps",
    "title": "Week 8: London Air Quality I",
    "section": "",
    "text": "Openair is an R package primarily developed for the analysis of air pollution measurement data for general use in the atmospheric sciences. The package consists of many tools for importing and manipulating data, and undertaking a wide range of analyses to enhance understanding of air pollution data. It gives as remote access, freely, to all UK air pollution networks to have the data at our disposal.\nTo install a package in R/RStudio, we can use the install.packages() function. Let us install the following packages - openair and openairmaps. We will also install the package lubridate to get access to additional function to deal with dates for time series.\nLet us install the 3 packages:\n\ninstall.packages(\"openair\")\ninstall.packages(\"openairmaps\")\ninstall.packages(\"lubridate\")\n\nAfter installing both packages, we will need to active them in order to use their functions. This can be done using the library() function. Let us process to activating these packages:\n\nlibrary(\"openair\")\nlibrary(\"openairmaps\")\nlibrary(\"lubridate\")\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union",
    "crumbs": [
      "Core Content",
      "Week 8: London Air Quality I"
    ]
  },
  {
    "objectID": "09-london_airquality_relationships.html#accessing-uk-air-pollution-data",
    "href": "09-london_airquality_relationships.html#accessing-uk-air-pollution-data",
    "title": "Week 8: London Air Quality I",
    "section": "",
    "text": "The UK has a surprisingly large amount of air quality data that is publicly accessible. The first order of things is to understand exactly what air pollution monitoring sites are availabile (also called Automatic Urban and Rural Networks (AURN)), and what air pollution contaminants do they measure. We can access the details of air pollution monitoring sites using the importMeta() function.\nLet us pull these details and store in a data frame:\n\n# view list of all points\nall_airpollution_aurn &lt;- importMeta(source = \"aurn\", all = TRUE)\n# view data frame\nView(all_airpollution_aurn)\n\nThis shows details of what each air pollution monitor station in the UK is measuring - let us examine the image closely:\n\n\n\n\n\n\n\n\n\n\nThe following columns code, site and latitude & longitude are key information that tells as the unique identifier, name and geographical position of these air pollution monitoring stations in the UK, respectively.\nThe columns variable and Parameter_name contain the shorthand notation and actual name, respectively, of the air pollution contaminant being measure.\nThe columns start_date and end_date refer to when that monitoring network started, whether if its ongoing or if its retired. The end_date shows if its still ongoing, or if its retired.\n\nYou can visualise the spatial distribution of these sites that are currently active using the networkMap() function\n\n# map of all air pollution monitors\nnetworkMap(source = \"aurn\", control = \"site_type\", year = 2025)\n\n\n\n\n\n\n\n\nWe explored details about the networks. Now, let us extract the actual pollution measures from the networks themselves - daily averages. The network of focus are at the following locations:\n\nLondon North Kensington (Code: KC1)\n\nWe will extract the following parameters as an example:\n\nNitrogen Dioxide (no2)\nNitrogen Oxide (nox)\nParticulate matter (pm2.5)\nParticulate matter (pm10)\n\nWe can extract information into a data frame by using the importAURN() function. Let us pull the record available records in 2025:\n\nair_KC1 &lt;- importAURN(\n    site = \"KC1\", \n    year = 2025, \n    data_type = \"daily\", \n    pollutant = c(\"no2\", \"nox\", \"pm2.5\", \"pm10\"))\n\nShow the first 10 rows:\n\nhead(air_KC1, n = 10)\n\n# A tibble: 10 × 8\n   source date                site                code    no2    nox pm2.5  pm10\n   &lt;chr&gt;  &lt;dttm&gt;              &lt;chr&gt;               &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 aurn   2025-01-01 00:00:00 London N. Kensingt… KC1    8.14   8.49  3.25  5.68\n 2 aurn   2025-01-02 00:00:00 London N. Kensingt… KC1   25.6   29.0   5.43  9.3 \n 3 aurn   2025-01-03 00:00:00 London N. Kensingt… KC1   41.0   51.1   9.67 13.0 \n 4 aurn   2025-01-04 00:00:00 London N. Kensingt… KC1   38.9   52.8  11.1  13.9 \n 5 aurn   2025-01-05 00:00:00 London N. Kensingt… KC1   16.4   18.8   2.66  3.48\n 6 aurn   2025-01-06 00:00:00 London N. Kensingt… KC1   15.4   16.5   2.47  4.45\n 7 aurn   2025-01-07 00:00:00 London N. Kensingt… KC1   28.3   32.2   4.48  7.41\n 8 aurn   2025-01-08 00:00:00 London N. Kensingt… KC1   45.3   66.0   9.36 14.4 \n 9 aurn   2025-01-09 00:00:00 London N. Kensingt… KC1   37.8   45.4  11.8  19.6 \n10 aurn   2025-01-10 00:00:00 London N. Kensingt… KC1   62.3  148.   20.3  27.0 \n\n\nData is never perfect! You will see that there is a lot of missing records across the period, and the fact that a network (i.e., KC1) has no data on PM10 throughout 2025. Anyways, we now have our dataset - let us analysis it!",
    "crumbs": [
      "Core Content",
      "Week 8: London Air Quality I"
    ]
  },
  {
    "objectID": "09-london_airquality_relationships.html#analysing-uk-air-pollution-data",
    "href": "09-london_airquality_relationships.html#analysing-uk-air-pollution-data",
    "title": "Week 8: London Air Quality I",
    "section": "",
    "text": "A time series plot is a graph that visualizes data collected in a sequence over time, with time on the horizontal (x-axis) and the measured variable on the vertical (y-axis). It is best used to identify trends, cycles, and patterns in data, such as economic fluctuations, weather changes, website traffic, or, in our case, changes in air pollution. These plots allow for analysis and forecasting by showing how a variable changes over specific time intervals like hours, days, months, or years.\nAll the basic building blocks for building a graph from scratch was covered in Week 5 (see here) - all of this comes full circle in this tutorial.\nFirst, we need to make sure the date column is a proper calendar/date format by using the as.Date(), and by stating within that function, that the format of the date is in YYYY-MM-DD as “%Y-%m-%d”:\n\nair_KC1$date_formatted &lt;- as.Date(air_KC1$date, format = \"%Y-%m-%d\")\n# class() confirms if the column is a date formate\nclass(air_KC1$date_formatted)\n\n[1] \"Date\"\n\n\nNow that its confirmed as a date format - we can proceed to create a time series plot specifically for NO2 at that location.\nWe are going to use the plot() to create the graphical output, and axis.Date() to apply cosmetic changes to the x-axis which measures the time:\n\nplot(air_KC1$no2 ~ air_KC1$date_formatted, \n    type=\"l\", \n    lwd=1.5, \n    main=\"AURN: London North Kensington [Code: KC1]\", \n    ylab = \"NO2 [PPB, Daily Average]\", \n    xlab = \"Date [2025-01-01 to 2025-11-22]\",\n    ylim = c(0, 70),\n    bty = \"L\",\n    xaxt = \"n\")\n\naxis.Date(\n    side=1, \n    at=seq(min(air_KC1$date_formatted), max(air_KC1$date_formatted), by = \"months\"), \n    format=\"%Y-%m-%d\", \n    cex.axis = .9, \n    labels = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation: This is a face-value interpretation, and as you can see, it is quite difficult to discern clear patterns in NO₂ between 01/01/2025 and 22/11/2025. But broadly speaking—and ignoring the jagged spikes—the overall pattern suggests a gradual decline in NO₂ levels from the start of the year until early September. From the beginning of September onwards, NO₂ levels begin to increase gradually.\n\n\n\n\n\nWe have covered descriptive statistics quite extensively at this point - so the programming aspect of such analysis should be second nature to you now! All we are doing is using the summary() and, or tapply() to compute the mean and standard deviation. Please refer to Week 4 because it was covered quite extensively!\nIt is possible to calculate the overall mean and standard deviation of NO2 across the time period by simply using the summary() function along with the standard deviation using sd() function:\n\nsummary(air_KC1$no2, na.rm = TRUE)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  3.435   8.214  13.085  16.369  20.581  64.969       3 \n\nsd(air_KC1$no2, na.rm = TRUE)\n\n[1] 10.82045\n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: The overall mean air pollution levels for NO2 (in 2025) in London North Kensington was 16.399ppb (with one SD of ±10.82ppb). The overall range in the distribution is 61.534ppb where the lowest observed value is 3.435ppb (minimum) and the highest observed value is 64.949ppb (maximum)\n\n\nWhile, this is basically a summary of the overall description of the distribution of NO2 - sometimes, its inappropriate or bad practice to summarise time series data in that fashion. It is best practice to report such summaries for time series data in a quarterly, or monthly manner, to see if their a distinct pattern that is temporally apparent.\nFor instance, we can produce quarterly summaries by grouping the records accordingly into quarters by using the quarters() function on the formatted date column to generate a quarterly indicator:\n\nair_KC1$quarters &lt;- quarters(air_KC1$date_formatted)\n\nNotice that new column was generated with each date defined as Q1, Q2, Q3 or Q4. Now, we can use tapply() function to compute the mean and standard across the quarters:\n\ntapply(air_KC1$no2, air_KC1$quarters, mean, na.rm = TRUE)\n\n      Q1       Q2       Q3       Q4 \n26.77046 13.28455 10.37344 14.77160 \n\ntapply(air_KC1$no2, air_KC1$quarters, sd, na.rm = TRUE)\n\n       Q1        Q2        Q3        Q4 \n13.091599  6.787649  5.578397  6.372888 \n\n\nIf you want these results to be further broken down by month - it can be done but the coding is little bit involved. Unlike the quarters() function, RStudio does not have a function for months. So we will need to be a bit creative and pull the month value from the actual date column to group the dataset accordingly.\nIn the date column (i.e., YYYY-MM-DD) the month string is contained as the 6th and 7th character in that string. We are going to extract the 6th and 7th character to create the needed month column for this analysis to get the monthly means of NO2.\nHere, we can use the substr() function, and tell it to select the 6th and 7th string only in the date column.\n\nair_KC1$months &lt;- substr(air_KC1$date, 6, 7)\n\nWe got what we want - now, we can use tapply() function to compute mean and standard deviation across those months:\n\ntapply(air_KC1$no2, air_KC1$months, mean, na.rm = TRUE)\n\n       01        02        03        04        05        06        07        08 \n29.177950 23.140038 27.407827 18.379460 11.217195 10.256994  9.935942 10.246371 \n       09        10        11 \n10.956836 14.084862 15.697202 \n\ntapply(air_KC1$no2, air_KC1$months, sd, na.rm = TRUE)\n\n       01        02        03        04        05        06        07        08 \n15.146743  9.073397 13.459940  6.740908  5.638085  4.838321  4.283866  4.707091 \n       09        10        11 \n 7.427567  5.419553  7.498650 \n\n\nNot too shabby!\n\n\n\nSo far, we have been analysing a single continuous measure, univariably. Here, we going to analysis two variables together to show a basic correlation.\n\nA correlation is a measure that describes the strength and direction of a relationship between two numerical variables. A correlation can have a value that ranges anywhere from: -1 to 0 to +1, where a positive value is a positive correlation which means that as one variable increases, the other tends to increase, whereas a negative value is a negative correlation means that as one variable increases, the other tends to decrease.\n\nWithout going too deep into the subject of correlations - its basic understanding in the context of air pollution helps identify:\n\nShared pollution sources\nHow pollutants interact\nWhether one pollutant can help predict another\n\nFor example, let us look at the correlation between NO2 and PM2.5 - perhaps, there could be some relationship? The first thing is to create a scatterplot showing the bivariable visually:\n\nplot(air_KC1$no2, air_KC1$pm2.5,\n    xlab = \"Daily Average NO2 (ppb)\", \n    ylab = \"Daily Average PM2.5 (μg/m³)\",\n    main = \"Scatterplot: NO2 versus  PM2.5 [Code: KC1]\",\n    bty = \"L\",\n    pch = 16,\n    las = 1,\n)\n\n\n\n\n\n\n\n\nGo to Week 5 as we covered the coding on creating graphs extensively!\nWhat this image illustrates - if there’s an increase in the NO2 levels, it shows as positive increase in PM2.5 as well. This relationship is only shown visually. We can quantify the strength by performing a correlation test between NO2 versus PM2.5 using the cor.test() function:\n\n# use diff() to remove trends, seasonality \ncor.test(diff(air_KC1$no2), diff(air_KC1$pm2.5))\n\n\n    Pearson's product-moment correlation\n\ndata:  diff(air_KC1$no2) and diff(air_KC1$pm2.5)\nt = 8.2481, df = 317, p-value = 4.347e-15\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3255604 0.5067652\nsample estimates:\n      cor \n0.4203449 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: The correlation between the changes in NO₂ and the changes in PM2.5 is 0.4206. This indicates a moderate positive relationship i.e., When NO2 increases from one time point to the next, PM2.5 also tends to increase, and when NO2 decreases, PM2.5 tends to decrease. The p-value is extremely small (way below 0.05), meaning that the probability of observing a correlation this strong (or stronger) is not by chance, and therefore the correlation is statistically significant.\n\n\n\n\n\nA linear regression is a statistical method used to describe how one variable (the outcome) changes in relation to another variable (the predictor).\nIt fits a straight line to the data: Outcome = Intercept + Slope × Predictor + Error\nThe slope tells you how much the outcome changes for every one-unit increase in the predictor. Suppose you want to understand how NO2 and PM2.5 relate to each other at a monitoring station.\n\nThe correlation tells you whether they move together (e.g., correlation = 0.4206).\nBut, a linear regression goes further and quantifies how much PM2.5 changes when NO2 changes.\n\nWithout going into too much details about linear regression - let us use a simple example, and fit a linear model regression model using lm() function:\n\nmodelfit &lt;- lm(diff(air_KC1$pm2.5) ~ diff(air_KC1$no2))\nsummary(modelfit)\n\n\nCall:\nlm(formula = diff(air_KC1$pm2.5) ~ diff(air_KC1$no2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.552  -1.695   0.074   1.921  30.339 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.005887   0.310406   0.019    0.985    \ndiff(air_KC1$no2) 0.324091   0.039293   8.248 4.35e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.544 on 317 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.1767,    Adjusted R-squared:  0.1741 \nF-statistic: 68.03 on 1 and 317 DF,  p-value: 4.347e-15\n\n\nThe above code lm(diff(pm25) ~ diff(no2)) means that we are modelling the change in PM2.5 as a function of the change in NO2. From the printed output, the explore the following results:\n\nIntercept (0.005887): The intercept is essentially zero and is not statistically significant (p = 0.985 which is bigger than 0.05). This means that when there is no change in NO2, the expected change in PM2.5 is 0.005887 which is effectively zero.\nSlope for NO2 [0.324 (p &lt; 0.001)]: The slope is the key result - here, for every unit increase in NO2 will result in PM2.5 to increases on average by 0.324 µg/m³. This increase is significant because its p-value is below 0.05. A small note - this confirms a moderate, positive, and statistically significant short-term association between the two pollutants, and it aligns with the correlation result we previously obtained (0.4206).\nModel fit (0.1741): The R² of 0.1741, which means that 17.41% of the variation in changes in PM2.5 is explained by changes in NO2. Generally, its a poor model if this is below 20%. 20-40% is moderate; 40-70% is good; and anything above 70% is an excellent model.\n\nThat is how you perform a regression model - the main resul of interest is the slope and its p-value.",
    "crumbs": [
      "Core Content",
      "Week 8: London Air Quality I"
    ]
  },
  {
    "objectID": "08-probability_distribution.html",
    "href": "08-probability_distribution.html",
    "title": "Week 7: Sourcing Data II",
    "section": "",
    "text": "In Week 3 & 4’s Examining Data II, we learnt how to perform some basic descriptive analysis of air pollution data (specifically 718 observations of Nitrogen Dioxide (NO\\(_2\\)) in Eixample, Barcelona. We derived the following outputs:\n\nFrequency distribution and table for ambient NO\\(_2\\) levels in Barcelona, which told us how often a particular measure for NO\\(_2\\) was observed. For instance, we found that in Barcelona, the common measure for ambient NO\\(_2\\) were values that fell in the range of 50-60ppb that accounted for 19.08% of the data.\nCumulative frequency distribution of ambient NO\\(_2\\) levels in Barcelona, which - based on a stated value/threshold of interest - indicated how often a particular measure for N0\\(_2\\) was observed. For example, based on this threshold &lt;50ppb, we found that it corresponds to 246 measurements for NO\\(_2\\) below 50ppb which composed of 0.3426 (34.26%) of the data.\nHistogram, which was simply a graphically representation that reflected our Frequency distribution. For instance, the image we produced last week:\n\n\n\n\n\n\n\n\n\n\n\nCentral Tendency Measures, Variance & Standard Deviation: These are a set of statistical estimates we used to provide a concise summary of the data. There more important measures were the mean (or average) and standard deviation. Recall in our air pollution data, we found the overall mean air pollutions level for NO\\(_2\\) in Eixample, Barcelona was 59.69ppb with a standard deviation (i.e., error) of ±20.63pbb.\n\nThese four recap points were brought up because they are all connected to Probability Distributions. Probability distributions are indeed the basis of data sciences, we actually use to these help model our world, enabling us to estimate or predict the probability that an event (e.g., disease outbreak, mosquito infestation, crime outcome, wildfire hazard etc.,) may occur or not. More importantly, we use these specifically in Hypothesis Testing when conducting Inferential Statistics.\nThere are a tonne of different probability distributions; but, for this tutorial, you will only be introduced to the Normal Distribution, a famous one which has its place in statistical analysis.\n\n\n\n\n\n\n\n\n\n\n\n\nWatch on [YouTube]\n\n\n\n\n\n\nNote\n\n\n\nI want to acknowledge that this video is a re-used asset from the old GEOG0013: Geography in the Field I module which no longer exist.\n\n\nDo remember a time in our GSCEs when we were asked to compute the probability of an event such as - let’s say - what the probability of throwing a die just once and it showing the value “six” as an outcome? The obvious answer to that is 1/6. Here, we computed the probability of just a single outcome. Probability Distributions on the other hand allows us to compute probabilities for ALL possible outcomes. There are several different types of probability distributions; however, the most common types you will come across in data science are:\n\nNormal Distribution for continuous data that are measurable (e.g., height, age, air pollution levels etc.)\nPoisson Distribution for discrete data that are countable (e.g., number of COVID-19 cases, reported number of phone snatched victims etc.,)\nUniform Distribution for continuous or discrete data are measurable or countable, respectively, where every data point is assumed to have an equal chance of being observed.\nBernoulli Distribution for binary data that contain two categories that are of a Yes/No, Success/Failure, In/Out or Win/Lose format (e.g., a disease outcome of a patient (Yes, No), A victim of theft (Yes, No) and so on).\n\nProbability distributions are always represented in a graphical format as a curve. As stated, the Normal Distribution, which is a bell-shaped curve, is the most “famous” and widely used probability distribution because - surprisingly - a lot of measurable phenomena that are continuous data tend to be approximately bell-shaped.\nFor instance, let’s take our previous example of the histogram graph we created from the NO\\(_2\\) data in Barcelona. While the graph shows the frequency of NO\\(_2\\) data using rectangles. The height of a rectangle (the vertical axis) basically represents the distribution frequency for the NO\\(_2\\) measure, as in, the amount, or how often that value was measured.\nNow, notice how the above figure output takes on shape that is somewhat akin to that of a bell-curve? The histogram indeed describes the shape for the ambient NO\\(_2\\) measures as a distribution that’s bell-shaped which was centered on the mean value of 59.69ppb and error (i.e., standard deviation away from the mean) was plus or minus 20.63pbb. This output reproduced in image [A] was generated from the data that follows Normal Distribution.\n\n\n\n\n\n\n\n\n\n\n\nSometimes, we can be in a situation where we are dealing with a data science problem but do not have any data at our disposal. But with probability distributions, we can do a work around if have prior knowledge or prior information of the problem. For instance, it is possible for us to generate a theoretical distribution for these NO\\(_2\\) values based on our knowledge of its mean and standard deviation with the assumption that its bell-shaped in the first place as shown in image [B]. This curve will show us the probability of observing every single possible value for the NO\\(_2\\) in Barcelona, as well as simulate a similar dataset.\nLet us learn how to generate this curve shown in image B as a step-by-step demonstration.\n\n\n\n\n\n\n\n\n\n\n\n\nWatch on [YouTube]\n\n\n\n\n\n\nNote\n\n\n\nI want to acknowledge that this video is a re-used asset from the old GEOG0013: Geography in the Field I module which no longer exist.\n\n\nWe will need to re-import the Barcelona_Air_Pollution_data.csv into RStudio and call this object air_quality_data. Recall, we used this data in Week 3 - you can get the dataset from here and view instructions accordingly [HERE]).\n\nair_quality_data &lt;- read.csv(\"Barcelona_Air_Pollution_data.csv\")\n\nLet’s use the mean(), sd(), min() and max() function on the NO2_est variable to compute some important values which will use as a basis to create the normal distribution plot.\n\n# calculates the mean of NO2\nmean(air_quality_data$NO2_est)\n\n[1] 59.6922\n\n# calculates the standard deviation of NO2\nsd(air_quality_data$NO2_est)\n\n[1] 20.60876\n\n# finds the lowest value for NO2\nmin(air_quality_data$NO2_est)\n\n[1] 2\n\n# finds the maximum value for NO2\nmax(air_quality_data$NO2_est)\n\n[1] 130\n\n\nFor the graph, we will need to create a template for the x-axis in a way that contains clean intervals of 10’s starting from 0, and ending up to 130 so as to capture the minimum and maximum values for NO\\(_2\\).\n\n# create the intervals for the x-axis\nintervals &lt;- seq(0, 130, 10)\nintervals\n\n [1]   0  10  20  30  40  50  60  70  80  90 100 110 120 130\n\n\nNow, we are going to create the a vector for the x-axis which will contain a sequence of every possible value for the NO\\(_2\\) within the range of 2ppb (minimum) to 130ppb (maximum). Here, we will create a series of clean values of equal intervals of 0.01 starting from 2 (lowest possible value), and up to 130 (highest possible value) that we know from our dataset.\n\n# create actual x-axis for every single possible value between 2 to 130\nx_axis &lt;- seq(2, 130, by = 0.01)\n\n\n\n\n\n\n\nNote\n\n\n\nIMPORTANT NOTE: The above code will create a sequence of values starting from 2 and ending at 130. These values will be 2.00, 2.01, 2.02, 2.03 … and so on and so forth.\n\n\nNow, we are going to use a new function called dnorm() which allows us to compute the probability density for each of the values along the x_axis object we created. In this function, we will need to specify our prior information which is our knowledge about the pollution levels in Barcelona. Hence, we will specify the mean and standard deviation.\n\n# we use to x_axis and our known mean and SD \nprobability_estimates &lt;- dnorm(x_axis, mean = 59.69, sd = 20.63)\n\n\n\n\n\n\n\nNote\n\n\n\nThe above code is estimating the exact probability of each value contained in that x_axis object. Hence, each value i.e., 2.00, 2.01, 2.02, 2.03 … and so on and so forth, will have its own probability estimate which is called a probability density\n\n\nNow that we have the key ingredients to visualise our probability distribution for the NO\\(_2\\) values in Barcelona. We can proceed to create our graph from scratch. We start with using the plot() and applying the labels.\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability [Normal] Distribution for NO2 in Barcelona\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is full-on custom graph we are making in RStudio. As there is no conventional way of producing such a graph. This is where the programming and hacks come to play, and brute forcing it [John Wick style] until we get the desired result!\nAbout the code:\n\nplot(x_axis, probability_estimates...), we are saying apply the values from x_axis on to the x-axis and those from probability_estimates on to the y-axis.\nThe type = \"l\", we are saying that this is a line/curve plot.\nlwd = 3, this controls the thickness of the line width. Here, we have set this to 3.\naxes = FALSE, we have deliberately set this to not appear. We want to fully customise our graph as you will see in the next set of codes.\nmain = \"...\", set the title of plot\nxlab = \"...\", set the title for the x-axis\nylab = \"...\", set the title for the y-axis\n\n\n\nLet’s add our custom made x-axis.\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability [Normal] Distribution for NO2 in Barcelona\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the interval breaks we created earlier on\naxis(1, at = intervals)\n\n\n\n\n\n\n\n\nLet see the maximum probability density estimate, this information will help us to customise the y-axis appropriately\n\nmax(probability_estimates)\n\n[1] 0.01933797\n\n\nIts 0.01933797. Okay then, so let’s add y-axis which starts from 0 and end at 0.020 spaced at intervals of 0.005.\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability [Normal] Distribution for NO2 in Barcelona\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the interval breaks we created earlier on\naxis(1, at = intervals)\n# add y-axis and start from 0 and end at 0.020 at intervals of 0.005\naxis(2, at = c(0, 0.005, 0.010, 0.015, 0.020))\n\n\n\n\n\n\n\n\nWe created the normal distribution curve. Now, we add the reference lines for the mean as well as the lower and upper bounds for the standard deviation.\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability [Normal] Distribution for NO2 in Barcelona\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the classes breaks from the histogram\naxis(1, at = intervals)\n# add y-axis and start from 0 and end at 0.020 at intervals of 0.005\naxis(2, at = c(0, 0.005, 0.010, 0.015, 0.020))\n# add vertical line for mean = 59.69\nabline(v=59.69, lwd = 1.5, col=\"black\")\n# add two vertical lines for the plus and minus values for our sd = 20.63\n# lower = 59.69 - 20.63\n# upper = 59.69 + 20.63\nabline(v=c(39.16, 80.22), lty=\"dashed\", lwd = 1.5, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe centre black line represents the position of the mean on the x-axis. Whilst the red dashed lines are the plus and minus limits from the standard deviations.\nAbout the code:\n\nabline(...), we are saying apply the reference line on main plot.\nv = ..., we are adding a vertical line to a specified value on the x-axis\nlty = \"dashed\", controls the style format of line. Here, its “dashed”\n\n\n\nThis is the expected output. Let’s demonstrate how to use it for making a prediction about the air pollution levels for NO\\(_2\\).\n\n\n\n\n\n\n\n\n\n\n\n\nWatch on [YouTube]\n\n\n\n\n\n\nNote\n\n\n\nI want to acknowledge that this video is a re-used asset from the old GEOG0013: Geography in the Field I module which no longer exist.\n\n\nFrom the above graph - the image shows that for each value of NO\\(_2\\) along the x-axis corresponds to some probability density estimate on the y-axis. For instance, the probability density for NO\\(_2\\) levels being exactly equal to 63ppb is 0.01911 (1.911%); and for an example, the probability density for NO\\(_2\\) levels being exactly equal to 80ppb is 0.01191 (1.1191%) etc.\nNow, traditionally, we are not interested in computing these probability densities for these exact values because these estimates for a single value are always negligible! But rather - what we are interested in estimating the cumulative probability density i.e., finding the probability that such an outcome is either “up to” (or at most) or alternatively “at least” some threshold, or even being “within a range” of values. The questions we usually ask to predict something are typically framed like:\n\nWhat is the probability that the air pollution levels of NO\\(_2\\) in Barcelona reaches up to (or is at most) 45ppb?\nWhat is the probability that the air pollution levels of NO\\(_2\\) in Barcelona exceeds (or is at least) 80ppb?\nWhat is the probability that the air pollution levels of NO\\(_2\\) in Barcelona falls within the range of (or is between) 45-80ppb?\n\nLet see how to answer each scenario.\n\n\nSuppose that we want to compute the probability that air pollution levels for NO\\(_2\\) in Barcelona reaches up to 45ppb. First of all, note that this is an “at most” scenario. The way in which this is represented graphically under our Normal Distribution where our mean is 59.69 and a standard deviation of ±20.63 is:\n\n\n\n\n\n\n\n\n\nThe red shaded area on the left-side under this normal curve represents the cumulative probabilities for the NO\\(_2\\) levels being at most 45ppb. Here, we can calculate this cumulative probability by using the pnorm() function. All we need to do is insert our value of interest along with the mean and standard deviation to obtain our result.\n\n# compute the cumulative probability for 45\npnorm(45, mean=59.69, sd=20.63)\n\n[1] 0.2382108\n\n\nWe get 0.2397141 or 23.97%. So we have therefore predicted that there’s a 23.97% chance that NO\\(_2\\) levels in Barcelona reach up to 45ppb. Please note that the code for generating the above plot for this scenario is shown below. It builds on that custom plot code we made earlier:\n\n# create a plot from with shaded areas\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability of NO2 being at most (or reaching) 45ppb\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the classes breaks from the histogram\naxis(1, at = intervals)\n# add y-axis and start from 0 and end at 0.020 at intervals of 0.005\naxis(2, at = c(0, 0.005, 0.010, 0.015, 0.020))\n# add mean reference line\nabline(v=59.69, lwd = 1.5, lty = \"dashed\", col=\"black\")\n\n# add shaded area under the curve (left-side)\npolygon(\n    c(min(x_axis), x_axis[x_axis &lt; 45], 45), \n    c(0, probability_estimates[x_axis &lt; 45], 0),  \n    col=\"red\")\n\n\nIn the above custom code for creating the plot, we have just added the polgyon() function to shade that portion of interest in red. You can replace the 45 with a different value if you want to reproduce this “left-sided” plot.\n\n\n\n\nSuppose that we want to compute the probability that air pollution levels for NO\\(_2\\) in Barcelona exceeds 80ppb. Please, note that this problem is an “at least” scenario. The way in which this is represented graphically under our Normal Distribution where we use our mean of 59.69 and standard deviation of ±20.63 is as follows:\n\n\n\n\n\n\n\n\n\nThe red shaded area on the right-side under this normal curve represents the cumulative probabilities for the NO\\(_2\\) levels being at least 80ppb. Note that when calculating probabilities of this scenario, we need to do 1 - pnorm() to calculate the red shaded area on the right side of this graph.\n\n# compute the cumulative probability for 80\n1 - pnorm(80, mean=59.69, sd=20.63)\n\n[1] 0.1624377\n\n\nWe get 0.1612494 or 16.12494%. So we have therefore predicted that there’s a 16.12% chance that air concentrations of NO\\(_2\\) levels in Barcelona exceeds 80ppb. Again, note that the code for generating the above plot for this scenario is shown below. It builds on that custom plot code we made earlier:\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability of NO2 being at least (or exceeding) 80ppb\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the classes breaks from the histogram\naxis(1, at = intervals)\n# add y-axis and start from 0 and end at 0.020 at intervals of 0.005\naxis(2, at = c(0, 0.005, 0.010, 0.015, 0.020))\n# add mean reference line\nabline(v=59.69, lwd = 1.5, lty = \"dashed\", col=\"black\")\n\n# add shaded area under the curve (right-side)\npolygon(\n    c(x_axis[x_axis&gt;=80], 80), \n    c(probability_estimates[x_axis&gt;=80], probability_estimates[x_axis==max(x_axis)]), \n    col=\"red\")\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above custom code for the plot, we have only added the polgyon() function to shade that portion of interest in red. You can replace the 80 with a different value if you want to reproduce this “right-sided” plot.\n\n\n\n\n\nSuppose that we want to compute the probability that air pollution levels for NO\\(_2\\) in Barcelona falls between 45ppb and 80ppb. Please, note that this problem is an “within a range” scenario. The way in which this problem is represented graphically under our Normal Distribution with mean of 59.69 and standard deviation of ±20.63 is:\n\n\n\n\n\n\n\n\n\nThe red shaded area in the middle portion under this normal curve represents the cumulative probabilities for the NO\\(_2\\) levels being between 45ppb and 80ppb. Note that when calculating probabilities of this scenario, we only need to do pnorm(80...) - pnorm(45..) to calculate the red shaded area in middle portion of this graph.\n\n# compute the cumulative probability for 80\npnorm(80, mean=59.69, sd=20.63) - pnorm(45, mean=59.69, sd=20.63)\n\n[1] 0.5993516\n\n\nWe get 0.5993516 or 59.93516%. So we have therefore predicted that there’s a 59.93% chance that air concentrations of NO\\(_2\\) levels in Barcelona fall between 45ppb and 80ppb. Again, note that the code for generating the above plot for this scenario is shown below. It builds on that custom plot code we made earlier:\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability of NO2 being between 45ppb and 80ppb\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the classes breaks from the histogram\naxis(1, at = intervals)\n# add y-axis and start from 0 and end at 0.020 at intervals of 0.005\naxis(2, at = c(0, 0.005, 0.010, 0.015, 0.020))\n\n# add shaded area under the curve (middle)\npolygon(\n    c(45, x_axis[x_axis&gt;=45 & x_axis&lt;=80], 80), \n    c(0, probability_estimates[x_axis&gt;=45 & x_axis&lt;=80] ,0),\n    col=\"red\")\n# add mean reference line\nabline(v=59.69, lwd = 1.5, lty = \"dashed\", col=\"black\")\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above custom code for the plot, we have only added the polgyon() function to shade that portion of interest in red. You can replace the lower (i.e., 45) and upper (i.e., 80) values with a different numbers to shade out the middle portion under this curve plot.",
    "crumbs": [
      "Core Content",
      "Week 7: Sourcing Data II"
    ]
  },
  {
    "objectID": "08-probability_distribution.html#quick-recap",
    "href": "08-probability_distribution.html#quick-recap",
    "title": "Week 7: Sourcing Data II",
    "section": "",
    "text": "In Week 3 & 4’s Examining Data II, we learnt how to perform some basic descriptive analysis of air pollution data (specifically 718 observations of Nitrogen Dioxide (NO\\(_2\\)) in Eixample, Barcelona. We derived the following outputs:\n\nFrequency distribution and table for ambient NO\\(_2\\) levels in Barcelona, which told us how often a particular measure for NO\\(_2\\) was observed. For instance, we found that in Barcelona, the common measure for ambient NO\\(_2\\) were values that fell in the range of 50-60ppb that accounted for 19.08% of the data.\nCumulative frequency distribution of ambient NO\\(_2\\) levels in Barcelona, which - based on a stated value/threshold of interest - indicated how often a particular measure for N0\\(_2\\) was observed. For example, based on this threshold &lt;50ppb, we found that it corresponds to 246 measurements for NO\\(_2\\) below 50ppb which composed of 0.3426 (34.26%) of the data.\nHistogram, which was simply a graphically representation that reflected our Frequency distribution. For instance, the image we produced last week:\n\n\n\n\n\n\n\n\n\n\n\nCentral Tendency Measures, Variance & Standard Deviation: These are a set of statistical estimates we used to provide a concise summary of the data. There more important measures were the mean (or average) and standard deviation. Recall in our air pollution data, we found the overall mean air pollutions level for NO\\(_2\\) in Eixample, Barcelona was 59.69ppb with a standard deviation (i.e., error) of ±20.63pbb.\n\nThese four recap points were brought up because they are all connected to Probability Distributions. Probability distributions are indeed the basis of data sciences, we actually use to these help model our world, enabling us to estimate or predict the probability that an event (e.g., disease outbreak, mosquito infestation, crime outcome, wildfire hazard etc.,) may occur or not. More importantly, we use these specifically in Hypothesis Testing when conducting Inferential Statistics.\nThere are a tonne of different probability distributions; but, for this tutorial, you will only be introduced to the Normal Distribution, a famous one which has its place in statistical analysis.",
    "crumbs": [
      "Core Content",
      "Week 7: Sourcing Data II"
    ]
  },
  {
    "objectID": "08-probability_distribution.html#what-are-probability-distributions-length-0434-minutes",
    "href": "08-probability_distribution.html#what-are-probability-distributions-length-0434-minutes",
    "title": "Week 7: Sourcing Data II",
    "section": "",
    "text": "Watch on [YouTube]\n\n\n\n\n\n\nNote\n\n\n\nI want to acknowledge that this video is a re-used asset from the old GEOG0013: Geography in the Field I module which no longer exist.\n\n\nDo remember a time in our GSCEs when we were asked to compute the probability of an event such as - let’s say - what the probability of throwing a die just once and it showing the value “six” as an outcome? The obvious answer to that is 1/6. Here, we computed the probability of just a single outcome. Probability Distributions on the other hand allows us to compute probabilities for ALL possible outcomes. There are several different types of probability distributions; however, the most common types you will come across in data science are:\n\nNormal Distribution for continuous data that are measurable (e.g., height, age, air pollution levels etc.)\nPoisson Distribution for discrete data that are countable (e.g., number of COVID-19 cases, reported number of phone snatched victims etc.,)\nUniform Distribution for continuous or discrete data are measurable or countable, respectively, where every data point is assumed to have an equal chance of being observed.\nBernoulli Distribution for binary data that contain two categories that are of a Yes/No, Success/Failure, In/Out or Win/Lose format (e.g., a disease outcome of a patient (Yes, No), A victim of theft (Yes, No) and so on).\n\nProbability distributions are always represented in a graphical format as a curve. As stated, the Normal Distribution, which is a bell-shaped curve, is the most “famous” and widely used probability distribution because - surprisingly - a lot of measurable phenomena that are continuous data tend to be approximately bell-shaped.\nFor instance, let’s take our previous example of the histogram graph we created from the NO\\(_2\\) data in Barcelona. While the graph shows the frequency of NO\\(_2\\) data using rectangles. The height of a rectangle (the vertical axis) basically represents the distribution frequency for the NO\\(_2\\) measure, as in, the amount, or how often that value was measured.\nNow, notice how the above figure output takes on shape that is somewhat akin to that of a bell-curve? The histogram indeed describes the shape for the ambient NO\\(_2\\) measures as a distribution that’s bell-shaped which was centered on the mean value of 59.69ppb and error (i.e., standard deviation away from the mean) was plus or minus 20.63pbb. This output reproduced in image [A] was generated from the data that follows Normal Distribution.\n\n\n\n\n\n\n\n\n\n\n\nSometimes, we can be in a situation where we are dealing with a data science problem but do not have any data at our disposal. But with probability distributions, we can do a work around if have prior knowledge or prior information of the problem. For instance, it is possible for us to generate a theoretical distribution for these NO\\(_2\\) values based on our knowledge of its mean and standard deviation with the assumption that its bell-shaped in the first place as shown in image [B]. This curve will show us the probability of observing every single possible value for the NO\\(_2\\) in Barcelona, as well as simulate a similar dataset.\nLet us learn how to generate this curve shown in image B as a step-by-step demonstration.",
    "crumbs": [
      "Core Content",
      "Week 7: Sourcing Data II"
    ]
  },
  {
    "objectID": "08-probability_distribution.html#how-to-generate-a-normal-distribution-curve-length-3253-minutes",
    "href": "08-probability_distribution.html#how-to-generate-a-normal-distribution-curve-length-3253-minutes",
    "title": "Week 7: Sourcing Data II",
    "section": "",
    "text": "Watch on [YouTube]\n\n\n\n\n\n\nNote\n\n\n\nI want to acknowledge that this video is a re-used asset from the old GEOG0013: Geography in the Field I module which no longer exist.\n\n\nWe will need to re-import the Barcelona_Air_Pollution_data.csv into RStudio and call this object air_quality_data. Recall, we used this data in Week 3 - you can get the dataset from here and view instructions accordingly [HERE]).\n\nair_quality_data &lt;- read.csv(\"Barcelona_Air_Pollution_data.csv\")\n\nLet’s use the mean(), sd(), min() and max() function on the NO2_est variable to compute some important values which will use as a basis to create the normal distribution plot.\n\n# calculates the mean of NO2\nmean(air_quality_data$NO2_est)\n\n[1] 59.6922\n\n# calculates the standard deviation of NO2\nsd(air_quality_data$NO2_est)\n\n[1] 20.60876\n\n# finds the lowest value for NO2\nmin(air_quality_data$NO2_est)\n\n[1] 2\n\n# finds the maximum value for NO2\nmax(air_quality_data$NO2_est)\n\n[1] 130\n\n\nFor the graph, we will need to create a template for the x-axis in a way that contains clean intervals of 10’s starting from 0, and ending up to 130 so as to capture the minimum and maximum values for NO\\(_2\\).\n\n# create the intervals for the x-axis\nintervals &lt;- seq(0, 130, 10)\nintervals\n\n [1]   0  10  20  30  40  50  60  70  80  90 100 110 120 130\n\n\nNow, we are going to create the a vector for the x-axis which will contain a sequence of every possible value for the NO\\(_2\\) within the range of 2ppb (minimum) to 130ppb (maximum). Here, we will create a series of clean values of equal intervals of 0.01 starting from 2 (lowest possible value), and up to 130 (highest possible value) that we know from our dataset.\n\n# create actual x-axis for every single possible value between 2 to 130\nx_axis &lt;- seq(2, 130, by = 0.01)\n\n\n\n\n\n\n\nNote\n\n\n\nIMPORTANT NOTE: The above code will create a sequence of values starting from 2 and ending at 130. These values will be 2.00, 2.01, 2.02, 2.03 … and so on and so forth.\n\n\nNow, we are going to use a new function called dnorm() which allows us to compute the probability density for each of the values along the x_axis object we created. In this function, we will need to specify our prior information which is our knowledge about the pollution levels in Barcelona. Hence, we will specify the mean and standard deviation.\n\n# we use to x_axis and our known mean and SD \nprobability_estimates &lt;- dnorm(x_axis, mean = 59.69, sd = 20.63)\n\n\n\n\n\n\n\nNote\n\n\n\nThe above code is estimating the exact probability of each value contained in that x_axis object. Hence, each value i.e., 2.00, 2.01, 2.02, 2.03 … and so on and so forth, will have its own probability estimate which is called a probability density\n\n\nNow that we have the key ingredients to visualise our probability distribution for the NO\\(_2\\) values in Barcelona. We can proceed to create our graph from scratch. We start with using the plot() and applying the labels.\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability [Normal] Distribution for NO2 in Barcelona\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is full-on custom graph we are making in RStudio. As there is no conventional way of producing such a graph. This is where the programming and hacks come to play, and brute forcing it [John Wick style] until we get the desired result!\nAbout the code:\n\nplot(x_axis, probability_estimates...), we are saying apply the values from x_axis on to the x-axis and those from probability_estimates on to the y-axis.\nThe type = \"l\", we are saying that this is a line/curve plot.\nlwd = 3, this controls the thickness of the line width. Here, we have set this to 3.\naxes = FALSE, we have deliberately set this to not appear. We want to fully customise our graph as you will see in the next set of codes.\nmain = \"...\", set the title of plot\nxlab = \"...\", set the title for the x-axis\nylab = \"...\", set the title for the y-axis\n\n\n\nLet’s add our custom made x-axis.\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability [Normal] Distribution for NO2 in Barcelona\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the interval breaks we created earlier on\naxis(1, at = intervals)\n\n\n\n\n\n\n\n\nLet see the maximum probability density estimate, this information will help us to customise the y-axis appropriately\n\nmax(probability_estimates)\n\n[1] 0.01933797\n\n\nIts 0.01933797. Okay then, so let’s add y-axis which starts from 0 and end at 0.020 spaced at intervals of 0.005.\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability [Normal] Distribution for NO2 in Barcelona\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the interval breaks we created earlier on\naxis(1, at = intervals)\n# add y-axis and start from 0 and end at 0.020 at intervals of 0.005\naxis(2, at = c(0, 0.005, 0.010, 0.015, 0.020))\n\n\n\n\n\n\n\n\nWe created the normal distribution curve. Now, we add the reference lines for the mean as well as the lower and upper bounds for the standard deviation.\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability [Normal] Distribution for NO2 in Barcelona\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the classes breaks from the histogram\naxis(1, at = intervals)\n# add y-axis and start from 0 and end at 0.020 at intervals of 0.005\naxis(2, at = c(0, 0.005, 0.010, 0.015, 0.020))\n# add vertical line for mean = 59.69\nabline(v=59.69, lwd = 1.5, col=\"black\")\n# add two vertical lines for the plus and minus values for our sd = 20.63\n# lower = 59.69 - 20.63\n# upper = 59.69 + 20.63\nabline(v=c(39.16, 80.22), lty=\"dashed\", lwd = 1.5, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe centre black line represents the position of the mean on the x-axis. Whilst the red dashed lines are the plus and minus limits from the standard deviations.\nAbout the code:\n\nabline(...), we are saying apply the reference line on main plot.\nv = ..., we are adding a vertical line to a specified value on the x-axis\nlty = \"dashed\", controls the style format of line. Here, its “dashed”\n\n\n\nThis is the expected output. Let’s demonstrate how to use it for making a prediction about the air pollution levels for NO\\(_2\\).",
    "crumbs": [
      "Core Content",
      "Week 7: Sourcing Data II"
    ]
  },
  {
    "objectID": "08-probability_distribution.html#using-the-normal-distribution-for-making-a-prediction-length-1750-minutes",
    "href": "08-probability_distribution.html#using-the-normal-distribution-for-making-a-prediction-length-1750-minutes",
    "title": "Week 7: Sourcing Data II",
    "section": "",
    "text": "Watch on [YouTube]\n\n\n\n\n\n\nNote\n\n\n\nI want to acknowledge that this video is a re-used asset from the old GEOG0013: Geography in the Field I module which no longer exist.\n\n\nFrom the above graph - the image shows that for each value of NO\\(_2\\) along the x-axis corresponds to some probability density estimate on the y-axis. For instance, the probability density for NO\\(_2\\) levels being exactly equal to 63ppb is 0.01911 (1.911%); and for an example, the probability density for NO\\(_2\\) levels being exactly equal to 80ppb is 0.01191 (1.1191%) etc.\nNow, traditionally, we are not interested in computing these probability densities for these exact values because these estimates for a single value are always negligible! But rather - what we are interested in estimating the cumulative probability density i.e., finding the probability that such an outcome is either “up to” (or at most) or alternatively “at least” some threshold, or even being “within a range” of values. The questions we usually ask to predict something are typically framed like:\n\nWhat is the probability that the air pollution levels of NO\\(_2\\) in Barcelona reaches up to (or is at most) 45ppb?\nWhat is the probability that the air pollution levels of NO\\(_2\\) in Barcelona exceeds (or is at least) 80ppb?\nWhat is the probability that the air pollution levels of NO\\(_2\\) in Barcelona falls within the range of (or is between) 45-80ppb?\n\nLet see how to answer each scenario.\n\n\nSuppose that we want to compute the probability that air pollution levels for NO\\(_2\\) in Barcelona reaches up to 45ppb. First of all, note that this is an “at most” scenario. The way in which this is represented graphically under our Normal Distribution where our mean is 59.69 and a standard deviation of ±20.63 is:\n\n\n\n\n\n\n\n\n\nThe red shaded area on the left-side under this normal curve represents the cumulative probabilities for the NO\\(_2\\) levels being at most 45ppb. Here, we can calculate this cumulative probability by using the pnorm() function. All we need to do is insert our value of interest along with the mean and standard deviation to obtain our result.\n\n# compute the cumulative probability for 45\npnorm(45, mean=59.69, sd=20.63)\n\n[1] 0.2382108\n\n\nWe get 0.2397141 or 23.97%. So we have therefore predicted that there’s a 23.97% chance that NO\\(_2\\) levels in Barcelona reach up to 45ppb. Please note that the code for generating the above plot for this scenario is shown below. It builds on that custom plot code we made earlier:\n\n# create a plot from with shaded areas\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability of NO2 being at most (or reaching) 45ppb\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the classes breaks from the histogram\naxis(1, at = intervals)\n# add y-axis and start from 0 and end at 0.020 at intervals of 0.005\naxis(2, at = c(0, 0.005, 0.010, 0.015, 0.020))\n# add mean reference line\nabline(v=59.69, lwd = 1.5, lty = \"dashed\", col=\"black\")\n\n# add shaded area under the curve (left-side)\npolygon(\n    c(min(x_axis), x_axis[x_axis &lt; 45], 45), \n    c(0, probability_estimates[x_axis &lt; 45], 0),  \n    col=\"red\")\n\n\nIn the above custom code for creating the plot, we have just added the polgyon() function to shade that portion of interest in red. You can replace the 45 with a different value if you want to reproduce this “left-sided” plot.\n\n\n\n\nSuppose that we want to compute the probability that air pollution levels for NO\\(_2\\) in Barcelona exceeds 80ppb. Please, note that this problem is an “at least” scenario. The way in which this is represented graphically under our Normal Distribution where we use our mean of 59.69 and standard deviation of ±20.63 is as follows:\n\n\n\n\n\n\n\n\n\nThe red shaded area on the right-side under this normal curve represents the cumulative probabilities for the NO\\(_2\\) levels being at least 80ppb. Note that when calculating probabilities of this scenario, we need to do 1 - pnorm() to calculate the red shaded area on the right side of this graph.\n\n# compute the cumulative probability for 80\n1 - pnorm(80, mean=59.69, sd=20.63)\n\n[1] 0.1624377\n\n\nWe get 0.1612494 or 16.12494%. So we have therefore predicted that there’s a 16.12% chance that air concentrations of NO\\(_2\\) levels in Barcelona exceeds 80ppb. Again, note that the code for generating the above plot for this scenario is shown below. It builds on that custom plot code we made earlier:\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability of NO2 being at least (or exceeding) 80ppb\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the classes breaks from the histogram\naxis(1, at = intervals)\n# add y-axis and start from 0 and end at 0.020 at intervals of 0.005\naxis(2, at = c(0, 0.005, 0.010, 0.015, 0.020))\n# add mean reference line\nabline(v=59.69, lwd = 1.5, lty = \"dashed\", col=\"black\")\n\n# add shaded area under the curve (right-side)\npolygon(\n    c(x_axis[x_axis&gt;=80], 80), \n    c(probability_estimates[x_axis&gt;=80], probability_estimates[x_axis==max(x_axis)]), \n    col=\"red\")\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above custom code for the plot, we have only added the polgyon() function to shade that portion of interest in red. You can replace the 80 with a different value if you want to reproduce this “right-sided” plot.\n\n\n\n\n\nSuppose that we want to compute the probability that air pollution levels for NO\\(_2\\) in Barcelona falls between 45ppb and 80ppb. Please, note that this problem is an “within a range” scenario. The way in which this problem is represented graphically under our Normal Distribution with mean of 59.69 and standard deviation of ±20.63 is:\n\n\n\n\n\n\n\n\n\nThe red shaded area in the middle portion under this normal curve represents the cumulative probabilities for the NO\\(_2\\) levels being between 45ppb and 80ppb. Note that when calculating probabilities of this scenario, we only need to do pnorm(80...) - pnorm(45..) to calculate the red shaded area in middle portion of this graph.\n\n# compute the cumulative probability for 80\npnorm(80, mean=59.69, sd=20.63) - pnorm(45, mean=59.69, sd=20.63)\n\n[1] 0.5993516\n\n\nWe get 0.5993516 or 59.93516%. So we have therefore predicted that there’s a 59.93% chance that air concentrations of NO\\(_2\\) levels in Barcelona fall between 45ppb and 80ppb. Again, note that the code for generating the above plot for this scenario is shown below. It builds on that custom plot code we made earlier:\n\n# create a plot from scratch\nplot(x_axis, probability_estimates, type = \"l\", lwd = 3, axes = FALSE,\n    main = \"Probability of NO2 being between 45ppb and 80ppb\",\n    xlab = \"NO2 values\", ylab = \"Probability Density\")\n# add x-axis and use the classes breaks from the histogram\naxis(1, at = intervals)\n# add y-axis and start from 0 and end at 0.020 at intervals of 0.005\naxis(2, at = c(0, 0.005, 0.010, 0.015, 0.020))\n\n# add shaded area under the curve (middle)\npolygon(\n    c(45, x_axis[x_axis&gt;=45 & x_axis&lt;=80], 80), \n    c(0, probability_estimates[x_axis&gt;=45 & x_axis&lt;=80] ,0),\n    col=\"red\")\n# add mean reference line\nabline(v=59.69, lwd = 1.5, lty = \"dashed\", col=\"black\")\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above custom code for the plot, we have only added the polgyon() function to shade that portion of interest in red. You can replace the lower (i.e., 45) and upper (i.e., 80) values with a different numbers to shade out the middle portion under this curve plot.",
    "crumbs": [
      "Core Content",
      "Week 7: Sourcing Data II"
    ]
  }
]